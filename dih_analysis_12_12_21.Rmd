---
title: "Analysis of DIH Prosecutions: 2000 to 2019"
author: "Kelly Kung"
date: "9/6/2021"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_knit$set(echo = TRUE, root.dir = "~/OneDrive - Boston University/Research-Lok")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Set Up
## R Code 
```{r}
#packages we need for this code file
library(ggplot2)
library(mgcv)
library(lubridate)
library(zoo)
library(tidyverse)
library(dplyr)
library(DHARMa)
library(mgcViz)
library(extrafont)
library(arm)
loadfonts()
library(stargazer)
library(ellipse)
```

```{r}
#define functions we will need for analysis
#expit function
expit<-function(x){
  return(exp(x)/(1 + exp(x)))
}

#logit function
logit<-function(x){
  return(log(x/(1 - x)))
}
```

## Data 
```{r}
#read in data
main_analysis_data<-read.csv("./Data/full_data_set_11_29_21_unintentional.csv")

################################## set up data set ################################
#add the intervention dates and time period data
main_analysis_data$Intervention_First_Date<-as.Date(main_analysis_data$Intervention_First_Date)
main_analysis_data$Time_Period_Start<-as.Date(main_analysis_data$Time_Period_Start)
names(main_analysis_data)[which(colnames(main_analysis_data) == "sum_deaths")] <- "imputed_deaths"

################################## set up the Regions ##############################
#set up the regions according to Census: https://www.census.gov/geographies/reference-maps/2010/geo/2010-census-regions-and-divisions-of-the-united-states.html
NE.name <- c("Connecticut","Maine","Massachusetts","New Hampshire",
             "Rhode Island","Vermont","New Jersey","New York",
             "Pennsylvania")

MW.name <- c("Indiana","Illinois","Michigan","Ohio","Wisconsin",
             "Iowa","Kansas","Minnesota","Missouri","Nebraska",
             "North Dakota","South Dakota")

S.name <- c("Delaware","District of Columbia","Florida","Georgia",
            "Maryland","North Carolina","South Carolina","Virginia",
            "West Virginia","Alabama","Kentucky","Mississippi",
            "Tennessee","Arkansas","Louisiana","Oklahoma","Texas")

W.name <- c("Arizona","Colorado","Idaho","New Mexico","Montana",
            "Utah","Nevada","Wyoming","Alaska","California",
            "Hawaii","Oregon","Washington")

region.list <- list(
  Northeast=NE.name,
  Midwest=MW.name,
  South=S.name,
  West=W.name)

#initialize vector with "West" and then impute the other regions for the states
main_analysis_data$Region<-rep("West", nrow(main_analysis_data))
for(state in unique(main_analysis_data$State)){
  if(state %in% region.list$Northeast){
    main_analysis_data$Region[main_analysis_data$State == state]<-"Northeast"
  }else if(state %in% region.list$Midwest){
    main_analysis_data$Region[main_analysis_data$State == state]<-"Midwest"
  }else if(state %in% region.list$South){
    main_analysis_data$Region[main_analysis_data$State == state]<-"South"
  }
}

```

# Exploratory Data Analysis
## Overdose Deaths
```{r}
############################## EDA: Plot the Outcome and Intervention Trends ###############################
#plot the time series of the number of deaths and probability of overdose death
od_data_recent <- read.csv("./Data/od_unintentional_yearly_18_and_up_11_28_21.txt", 
                           sep = "\t", stringsAsFactors = FALSE)
od_data_recent$Deaths <- as.numeric(od_data_recent$Deaths)
od_data_recent<-od_data_recent[!is.na(od_data_recent$Year),] #delete the rows that just contains data set description info
od_data_recent<- od_data_recent %>% 
  filter(Year > 1999 & Year < 2020) %>% 
  group_by(Year) %>%
  summarise(sum_deaths = sum(Deaths, na.rm = TRUE))

# pdf("./Figures/total_od_deaths_all_paper_11_29_21_2000_2019.pdf")
ggplot(data = od_data_recent, mapping = aes(x = Year, y = sum_deaths)) +
  geom_line() + 
  geom_point() +
  labs(x = "Year", y = "Yearly Number of Unintentional Drug Overdose Deaths in the 50 U.S. States") +
  theme(panel.background = element_rect("white"), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.line = element_line(colour = "black"),
        axis.title=element_text(family="Times", size=10, face="bold"),
        axis.text=element_text(family="Times",size=10)) +
  scale_x_continuous(breaks = seq(2000, 2020, by = 2)) +
  ylim(c(0, 62000))
# dev.off()

main_analysis_data_sum <- main_analysis_data %>% 
  group_by(year = year(Time_Period_Start)) %>%
  summarise(total_deaths = sum(imputed_deaths),
            sum_pop = sum(population)/2,
            total_prop = sum(imputed_deaths)/(sum(population)/2),
            total_prop_by_100000 = 100000*sum(imputed_deaths)/(sum(population)/2))
# %>%mutate(date = as.Date(as.yearmon(year)))

#compute the percentage difference between 2000 and 2019
death_2000 <- main_analysis_data_sum$total_deaths[main_analysis_data_sum$year == 2000]
death_2019 <- main_analysis_data_sum$total_deaths[main_analysis_data_sum$year == 2019]

((death_2019 - death_2000)/death_2000)*100

```

## Intervention: DIH Prosecutions
```{r}
#plot the number of states with an intervention for each time point
#first, create a data set to find the number of states with an intervention at each time point
#initialize the data set with the start date of the time period
num_states_with_intervention<-data.frame("Start_Date" =
                                  unique((main_analysis_data$Intervention_First_Date[!is.na(main_analysis_data$Intervention_First_Date)])))
numStates<-c()

#for each time period i, we first find the states where the first intervention date occurred before i
#then, we append it to numStates
for(i in unique((num_states_with_intervention$Start_Date))){
  states_w_int<-unique(main_analysis_data$State[(main_analysis_data$Intervention_First_Date)<=i])
  numStates<-append(numStates, length(states_w_int[!is.na(states_w_int)]))
}
num_states_with_intervention$numStates<-numStates
num_states_with_intervention$Start_Date <- as.Date(num_states_with_intervention$Start_Date)
num_states_with_intervention <- rbind(data.frame("Start_Date" = c(as.Date("2000-01-01"),
                                                                  as.Date("2019-12-31")),
                                                 "numStates" = c(0, max(num_states_with_intervention$numStates))),
                                      num_states_with_intervention)
num_states_with_intervention <- num_states_with_intervention %>% 
  arrange(Start_Date) %>%
  mutate(lag_numStates = lag(numStates))

num_states_with_intervention <- num_states_with_intervention %>%
  pivot_longer( c("lag_numStates", "numStates"), "numStates")

# pdf("Figures/num_states_with_intervention_11_29_21.pdf")
ggplot(num_states_with_intervention, aes(x = Start_Date, y = value, group = 1)) +
  geom_line() +
  # geom_point(num_states_with_intervention[num_states_with_intervention$numStates == "numStates",],
  #            mapping = aes(x = Start_Date, y = value, group = 1), size = 1) +
  labs(x = "Year", y = "Cumulative Number of States to have DIH Prosecutions") +
  theme(axis.text=element_text(family="Times",size=10),
        axis.title=element_text(family="Times", size=10, face="bold"),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.line = element_line(colour = "black"),
        axis.text.x = element_text(family="Times", size=10),
        panel.background = element_rect("white")) +
  scale_x_date(date_labels="%Y", breaks = seq(as.Date("2000-01-01"), as.Date("2018-01-01"), by = "2 years"))

# dev.off()
```

## Policy Dates
```{r}
#add the intervention variable as a measure of number of states with DIH prosecution
main_analysis_data <- main_analysis_data %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention = sum(Intervention_Redefined))

############################# Look at the policy dates #######################
policy_dates <- main_analysis_data %>% 
  group_by(State) %>%
  summarise(unique(format(Intervention_First_Date, "%Y-%m")),
            unique(format(as.Date(Naloxone_Pharmacy_Yes_First_Date), "%Y-%m")),
            unique(format(as.Date(Naloxone_Pharmacy_No_First_Date), "%Y-%m")),
            unique(format(as.Date(Medical_Marijuana_First_Date), "%Y-%m")),
            unique(format(as.Date(Recreational_Marijuana_First_Date), "%Y-%m")),
            unique(format(as.Date(PDMP_First_Date), "%Y-%m")),
            unique(format(as.Date(GSL_First_Date), "%Y-%m")),
            unique(format(as.Date(Medicaid_Expansion_First_Date), "%Y-%m")))
names(policy_dates) <- c("State", "DIH Prosecutions", "NAL: Pharmacists Yes",
                         "NAL: Pharmacists No", "MML", "RML", "PDMP", "GSL",
                         "Medicaid")
# write.csv(policy_dates, "./Data/policy_dates_11_29_21.csv")

```

## Create Plot of Number of DIH Prosecutions Per State
```{r}
#create a plot for each state to see how many prosecution media alerts there are per 6 month period
#read in the prosecution media alert data
prosecution_data<-read.csv("./Data/dih_prosecutions_9_6_21.csv")

#data cleaning
prosecution_data<-prosecution_data %>% 
  mutate(Date = as.Date(Date.charged, "%m/%d/%Y")) %>%
  mutate(State = ifelse(State.Filed == "pennsylvania", "Pennsylvania", State.Filed),
         State = ifelse(State.Filed == "Virginia ", "Virginia", State)) %>%
  mutate(deceased_age = ifelse(!is.na(as.numeric(Deceased.s.Age)), as.numeric(Deceased.s.Age), 9999)) %>%
  filter(!is.na(Date), 
         State.Filed != "No Info", 
         State.Filed != "No info", 
         State.Filed != "No Info ",
         deceased_age >= 18,
         State != "" ) %>%
  mutate(deceased_age = ifelse(deceased_age == 9999, NA, deceased_age))

#clean up the data by looking at the link to the article
prosecution_data$Date[prosecution_data$Date == "2026-08-01"] <- as.Date("2016-02-15", "%Y-%m-%d")

#change the states into Character instead of factor
prosecution_data$State<-as.character(prosecution_data$State)
#see how many prosecution data points there are for each state
table(prosecution_data$State)

#there are some repeated cases depending on victim so extract distinct cases
prosecution_data_unique <- prosecution_data %>%
  group_by(State) %>%
  distinct(Accused.Name, Date, .keep_all = T)
table(prosecution_data_unique$State)

#change date charged into Date object
prosecution_data_unique$Date<-mdy(prosecution_data_unique$Date.charged)

#group the data into six month periods
prosecution_data_unique<-prosecution_data_unique %>% 
  mutate(six_month_pd = lubridate::floor_date(Date , "6 months" ))

prosecution_grouped <- prosecution_data_unique %>% 
  #filter to dates after 2000 and dates before 2020
  filter(year(six_month_pd) >= 2000 & year(six_month_pd) <= 2019) %>%
  group_by(State, six_month_pd) %>% 
  #for each state, for each six month period, count the number of DIH prosecutions
  summarise(num_dih = n()) %>% 
  #ONLY IF GROUPS
  #label the groups according to zero, low, or high
  mutate(group = ifelse(num_dih == 0, "zero", ifelse(num_dih >= 5, "high", "low"))) %>%
  ungroup() %>%
  #have to add in a row for hawaii because its not in the prosecution dataset
  add_row(State = "Hawaii", six_month_pd = as.Date("2000-01-01"), num_dih = 0, group = "zero")

#look at table of ages of the victims
table(as.numeric(prosecution_data_unique$Deceased.s.Age))
table(as.numeric(prosecution_data_unique$Deceased.s.Age))/sum(!is.na(as.numeric(prosecution_data_unique$Deceased.s.Age)))

#view table of people in each age category, based on prosecution data
prosecution_data_unique <- prosecution_data_unique %>%
  mutate(Deceased.s.Age = as.numeric(Deceased.s.Age),
         age_groups = ifelse(Deceased.s.Age <= 17, "0_17", 
                             ifelse(Deceased.s.Age >= 18 & Deceased.s.Age <= 34, "18_34", 
                                    ifelse(Deceased.s.Age >= 35 & Deceased.s.Age <= 54, "35_54", 
                                           ifelse(Deceased.s.Age >= 55 & Deceased.s.Age <= 64, "55_64", "65_plus")))))

table(prosecution_data_unique$age_groups, exclude = "NA")

#we compute the final group for each state by seeing if it ever hits high or low
#if it hits a higher level, then it will remain defined in that higher level
prosecution_grouped_final <- prosecution_grouped %>%  
  group_by(State) %>% 
  summarise(final_gp = ifelse(sum(group == "high") > 0, "high", ifelse(sum(group == "low")> 0, "low", "zero"))) 

#plot of the number of states in each zero/low/high category
ggplot(prosecution_grouped_final, aes(final_gp)) + 
  geom_bar() + 
  labs(title = "Number of States by DIH prosecution Category, with Low = [1,5]") + 
  geom_text(aes(label = ..count..), stat = "count", vjust = -.75)

#number of DIH prosecutions per six month for each state
# pdf("Figures/num_dih_per_six_month_pd_by_state_11_29_21.pdf")
ggplot(prosecution_grouped, aes(x = six_month_pd, y = num_dih)) + 
  geom_bar(stat = "identity") + 
  facet_wrap(~State) + 
  labs(y = "Number of DIH Prosecutions Reported in the Media",
       x = "Date") + 
  theme(axis.text.x = element_text(hjust = 1, size = 6, family = "Times", angle = 30),
        axis.text.y = element_text(size = 6, family = "Times"),
        axis.title = element_text(size = 10, face = "bold", family = "Times"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size=8),
        panel.background = element_rect("white"),
        legend.position = "bottom")
# dev.off()

# write.csv(prosecution_grouped, "./Data/num_dih_per_six_month_pd_by_state_11_12_21.csv")


```

<!-- # Estimation of Autocorrelation Coefficients -->
<!-- ```{r} -->
<!-- #regress dependent variable on time and state effects -->
<!-- current_time_model <- gam(cbind(round(imputed_deaths), round(num_alive))~ State +  -->
<!--                             s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +  -->
<!--                             num_states_w_intervention, -->
<!--                           data = main_analysis_data, -->
<!--                           family = "binomial") -->

<!-- #obtain the residuals at time t and lagged residuals from time t - 1 -->
<!-- residuals_t <- residuals(current_time_model) -->
<!-- residuals_t_minus_1 <- lag(residuals_t) -->

<!-- #obtain the autocorrelation coefficients by regressing of residuals at time t  -->
<!-- #on residuals at time t - 1 -->
<!-- autocorrelation_model <- lm(residuals_t ~ residuals_t_minus_1 - 1) -->

<!-- summary(autocorrelation_model) -->


<!-- ``` -->

# Main Analysis: Effect of At Least One DIH Prosecution Report in Media on Unintentional Overdose Deaths

## Analysis
```{r, results = "asis"}
############################## Run Model with Spline Time Effects by Region ###############################
#model that we will be using for the main analysis
#cr is used for cubic regression spline -- we are smoothing time effects by region
#run the analysis for all the states
main_analysis_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                           s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                           Naloxone_Pharmacy_Yes_Redefined +
                           Naloxone_Pharmacy_No_Redefined +
                           Medical_Marijuana_Redefined +
                           Recreational_Marijuana_Redefined +
                           GSL_Redefined +
                           PDMP_Redefined +
                           Medicaid_Expansion_Redefined +
                           Intervention_Redefined + 
                           num_states_w_intervention,
                         data = main_analysis_data, family = "binomial")

#summary output of the model
stargazer(main_analysis_model, type = "html", dep.var.labels = c("Unintentional Overdose Deaths"))
```

## Sandwich Estimator
```{r}
#here, we estimate the variance-covariance matrix through the sandwich estimator
#we only want to estimate the variances for the policies, intervention variable, and number of states with intervention

subset_cov <- main_analysis_data %>%
  ungroup() %>%
  dplyr::select(
         Naloxone_Pharmacy_Yes_Redefined,
         Naloxone_Pharmacy_No_Redefined,
         Medical_Marijuana_Redefined,
         Recreational_Marijuana_Redefined,
         GSL_Redefined,
         PDMP_Redefined,
         Medicaid_Expansion_Redefined,
         Intervention_Redefined,
         num_states_w_intervention)

#extract the probability of overdoses and observed proportion of OD
prob_od <- predict(main_analysis_model, newdata = main_analysis_data, type = "response")
prop_obs_od <- main_analysis_data$imputed_deaths/main_analysis_data$population

#estimate the constant term: sum_{s,t} x_{s,t}*p_{s,t}*(1-p_{s,t})*x_{s,t}^T
constant_term <- matrix(0, nrow = ncol(subset_cov), ncol = ncol(subset_cov))
for(row in 1:nrow(subset_cov)){
  constant_term <- constant_term + t(subset_cov[row,])%*%prob_od[row]%*%(1-prob_od[row])%*%as.matrix(subset_cov[row,])
}

#estimate middle term: sum_{s,t} x_{s,t}*mean(prop_obs_od - prob_od)^2*x_{s,t}^T
mean_prop_minus_prob_sq <- mean((prop_obs_od - prob_od)^2)
exp_squared <- matrix(0, nrow = ncol(subset_cov), ncol = ncol(subset_cov))
for(row in 1:nrow(subset_cov)){
  exp_squared <- exp_squared + t(subset_cov[row,])%*%mean_prop_minus_prob_sq%*%as.matrix(subset_cov[row,])
}

#variance-covariance = constant_term^{-1}*exp_squared*t(constant_term^{-1})
#here, solve computes the inverse of the matrix
var_cov <- solve(constant_term)%*%exp_squared%*%t(solve(constant_term))
#we obtain the standard deviations by taking the square root of the diagonal of the variance-covariance matrix.
sd_of_coefficients <- sqrt(diag(var_cov))

#find the 95% CI for the coefficients
coefficient_values <- tail(summary(main_analysis_model)$p.coeff, length(diag(var_cov)))
lb_coef <- coefficient_values - 1.96*(sd_of_coefficients)
ub_coef <- coefficient_values + 1.96*(sd_of_coefficients)

data.frame(lb_coef, coefficient_values, ub_coef,
           exp_lb = exp(lb_coef), exp_coef = exp(coefficient_values),
           exp_ub = exp(ub_coef))
```

## Plots 
```{r}
#check diagnostics
gam.check(main_analysis_model)

main_analysis_model_object <- getViz(main_analysis_model)

midwest_plot <- plot(sm(main_analysis_model_object, 1)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + 
  theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for Midwest") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005", "2010", "2015"))  +
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

northeast_plot <- plot(sm(main_analysis_model_object,2)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + 
  theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for Northeast") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005", "2010", "2015"))+
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

south_plot <- plot(sm(main_analysis_model_object, 3)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + 
  theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for South") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005","2010", "2015"))+
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

west_plot <- plot(sm(main_analysis_model_object, 4)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for West") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005", "2010", "2015"))+
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

# pdf("./Figures/time_smoothed_effects_9_6_21.pdf")
gridPrint(midwest_plot, northeast_plot, south_plot, west_plot, ncol = 2)
# dev.off()

total_pop <- main_analysis_data %>% 
  group_by(year = year(Time_Period_Start), State) %>% 
  summarise(pop = unique(population)) %>%
  group_by(year) %>% 
  summarise(sum(pop))

main_analysis_data %>% 
  group_by(year(Time_Period_Start)) %>% 
  summarise(sum_deaths = sum(imputed_deaths)*100000) %>% 
  mutate(sum_deaths/total_pop$`sum(pop)`)

main_analysis_data %>%
  group_by(State, year(Time_Period_Start)) %>%
  summarise(death_rate = (sum(imputed_deaths)/unique(population))*100000) %>%
  group_by(State) %>%
  summarise(first_death_rate = death_rate[1],
            last_death_rate = death_rate[20]) %>%
  mutate(range_death_rate = last_death_rate - first_death_rate) %>% 
  filter(range_death_rate == min(range_death_rate) | range_death_rate == max(range_death_rate))
  

# #summarize the DIH dates
# main_analysis_data %>% 
#   group_by(Time_Period_Start) %>%
#   summarise(prop_w_intervention = mean(Intervention_Redefined > 0)) %>%
#   View()

#create a data frame to store the results and compute the confidence intervals
#initialize the columns
main_analysis_plot_table<-data.frame(State = main_analysis_data$State)
main_analysis_plot_table$Fitted<-rep(NA, nrow(main_analysis_plot_table))
main_analysis_plot_table$Observed<-rep(NA, nrow(main_analysis_plot_table))
main_analysis_plot_table$Time<-main_analysis_data$Time_Period_ID
main_analysis_plot_table$Time_Date<-main_analysis_data$Time_Period_Start
main_analysis_plot_table$Intervention_Date<-main_analysis_data$Intervention_First_Date

#we want to compare the fitted probability of overdose death and the observed values to see how the model does as a goodness of fit visual test
for(i in unique(main_analysis_plot_table$State)){
  #for each state, we first subset the main analysis data to only look at the data for that state
  index_of_state<-which(main_analysis_plot_table$State == i)
  #impute the fitted and observed probability of overdose death for the state
  main_analysis_plot_table$Fitted[index_of_state]<-fitted(main_analysis_model)[index_of_state]
  main_analysis_plot_table$Observed[index_of_state] <- (main_analysis_data$imputed_deaths[main_analysis_data$State == i]/main_analysis_data$population[main_analysis_data$State == i])
}

```

```{r, fig.height=8, fig.width=8}
#plot to compare the fitted values vs observed deaths
# pdf("./Figures/GAM_fitted_vs_actual_by_Region_9_6_21_with_int_date_full_data.pdf")
ggplot(data = main_analysis_plot_table, aes(x = Time_Date, y = Observed*100000, group = 1,
                                            color = "Observed")) +
  geom_line(aes(color = "Observed"))+ geom_point(aes(color = "Observed"), size = .5, alpha = .5) +
  geom_line(data = main_analysis_plot_table, aes(x = Time_Date, y = Fitted*100000, group = 1,
                                                 color = "Estimate")) +
  geom_point(data = main_analysis_plot_table, aes(x = Time_Date, y = Fitted*100000,
                                                  color = "Estimate"),
             size = .5, alpha = .5) +
  scale_color_manual(values = c("pink", "blue")) + 
  geom_vline(main_analysis_plot_table, mapping = aes(xintercept = Intervention_Date)) +
  facet_wrap(facets = vars(State), scales = "free_y", ncol = 5) +
  theme(axis.text.x = element_text(hjust = 1, size = 6, family = "Times"),
        axis.text.y = element_text(size = 6, family = "Times"),
        axis.title=element_text(size = 10,face = "bold", family = "Times"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size=8),
        panel.background = element_rect("white"),
        legend.position = "bottom") +
  labs(x = "Year", y = "Unintentional Drug Overdose Death Rates per 100,000 People",
       color = "")
# dev.off()
```

```{r}
#diagnostic plots to check model
residTab <- data.frame(logit_fitted_vals = logit(fitted(main_analysis_model)),
                       resids = residuals(main_analysis_model))
# pdf("./Figures/deviance_resids_9_6_21.pdf")
ggplot(residTab, aes(x = logit_fitted_vals, y = resids)) +
  geom_point() +
  theme(text = element_text(size = 10, family = "Times"),
        title = element_text(size = 10, family = "Times", face = "bold"),
        panel.background = element_rect(fill = "white", color = "black")) +
  # theme_classic() +
  labs(x = "Logistic Function of Fitted Values", y = "Deviance Residuals")
# dev.off()

pred_vals <- predict(main_analysis_model)
resids <- resid(main_analysis_model, type = "response")
# pdf("./Figures/binned_resids_plot_9_6_21.pdf")
par(font.lab = 2)
par(family = "Times")
binnedplot(pred_vals, resids, main = "", 
           xlab = "Average Logistic Function of Fitted Values",
           ylab = "Average Residuals")
# dev.off()

```

## Compile Results
```{r}
############################## Main Analysis: Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
main_analysis_full_table<-data.frame(coef(main_analysis_model))
#check to see how the table looks
head(main_analysis_full_table)
#rename the column to "Coefficient_Estimate"
colnames(main_analysis_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined", 
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(main_analysis_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(main_analysis_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(main_analysis_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(main_analysis_full_table)[i]<-substr(rownames(main_analysis_full_table)[i], start = 6,
                                                    stop = nchar(rownames(main_analysis_full_table)[i]))

    }else if(rownames(main_analysis_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(main_analysis_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(main_analysis_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(main_analysis_full_table)[i]<-paste("Smoothed Time for Region ",
                                                   substr(rownames(main_analysis_full_table)[i], start = 36,
                                                          stop = nchar(rownames(main_analysis_full_table)[i])),
                                                   sep = "")

    }
  }
}

#confidence intervals for the coefficients
main_analysis_full_table$Coefficient_Lower_Bound<-main_analysis_full_table$Coefficient_Estimate - 
  1.96*summary(main_analysis_model)$se
main_analysis_full_table$Coefficient_Upper_Bound<-main_analysis_full_table$Coefficient_Estimate + 
  1.96*summary(main_analysis_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
main_analysis_full_table$Odds_Ratio<-exp(main_analysis_full_table$Coefficient_Estimate)
main_analysis_full_table$Odds_Ratio_LB<-exp(main_analysis_full_table$Coefficient_Lower_Bound)
main_analysis_full_table$Odds_Ratio_UB<-exp(main_analysis_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
main_analysis_full_table$Standard_Error<-summary(main_analysis_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
main_analysis_full_table$p_value<-c(summary(main_analysis_model)$p.pv, 
                                    rep(NA, length(coef(main_analysis_model)) - 
                                          length(summary(main_analysis_model)$p.pv)))

head(main_analysis_full_table)
tail(main_analysis_full_table)

#save the table into a CSV
# write.csv(round(main_analysis_full_table,5), "./Data/coefficients_GAM_9_6_21_full_data_uninentional_od.csv")

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(main_analysis_full_table) %in% covariates)
main_analysis_covariate_table<-(round(main_analysis_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(main_analysis_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                           "Naloxone_Pharmacy_No",
                                           "Medical_Marijuana",
                                           "Recreational_Marijuana",
                                           "GSL", 
                                           "PDMP", 
                                           "Medicaid_Expansion",
                                           "Intervention", 
                                           "Number of States with DIH Prosecutions")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
main_analysis_covariate_table<-rbind(main_analysis_covariate_table, 
                                     main_analysis_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
main_analysis_covariate_table<-main_analysis_covariate_table[,-which(colnames(main_analysis_covariate_table) %in%
                                                                       c("Coefficient_Estimate", "Coefficient_Lower_Bound",
                                                                         "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(main_analysis_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(main_analysis_covariate_table, 10)

#save the table into a CSV
# write.csv(round(main_analysis_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_unintentional_od.csv")


```

## Attributable Deaths
````{r}
############################## Main Analysis: Number of Overdose Deaths Attributed to Intervention ###############################
#find the number of deaths attributable to the intervention
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_main_analysis<-main_analysis_data[which(main_analysis_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_main_analysis<-expit(-coef(main_analysis_model)["Intervention_Redefined"]*attr_deaths_anlys_main_analysis$Intervention_Redefined
                                    + logit(attr_deaths_anlys_main_analysis$imputed_deaths/attr_deaths_anlys_main_analysis$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(main_analysis_model) - 1.96*summary(main_analysis_model)$se
coef_ub<-coef(main_analysis_model) + 1.96*summary(main_analysis_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_main_analysis<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_main_analysis$Intervention_Redefined
                                       + logit(attr_deaths_anlys_main_analysis$imputed_deaths/attr_deaths_anlys_main_analysis$population))

prob_od_no_int_UB_main_analysis<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_main_analysis$Intervention_Redefined
                                       + logit(attr_deaths_anlys_main_analysis$imputed_deaths/attr_deaths_anlys_main_analysis$population))


#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(attr_deaths_anlys_main_analysis$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_main_analysis$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_main_analysis$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_main_analysis$imputed_deaths[time_point_index]
                          - prob_od_no_int_main_analysis[time_point_index]*attr_deaths_anlys_main_analysis$population[time_point_index])
  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_main_analysis$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_main_analysis[time_point_index]*attr_deaths_anlys_main_analysis$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_main_analysis$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_main_analysis[time_point_index]*attr_deaths_anlys_main_analysis$population[time_point_index])
  index<-index + 1
}

num_attr_od_main_analysis<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_main_analysis$Time_Period_ID)),
                                      "Time_Start" = sort(unique(attr_deaths_anlys_main_analysis$Time_Period_Start)),
                                      "Num_Attr_Deaths" = num_attr_od,
                                      "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                      "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_main_analysis$Num_Attr_Deaths)
summary(num_attr_od_main_analysis$Num_Attr_Deaths)
num_attr_od_main_analysis$Time_Start<-as.Date(num_attr_od_main_analysis$Time_Start)

#compute the 95% CI for the total
sum(num_attr_od_main_analysis$Num_Attr_Deaths_LB)
sum(num_attr_od_main_analysis$Num_Attr_Deaths_UB)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_main_analysis<-num_attr_od_main_analysis %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_main_analysis$deaths)
summary(yearly_num_Attr_Deaths_main_analysis$death_lb)
summary(yearly_num_Attr_Deaths_main_analysis$death_ub)

```

## Bootstrap Estimates
```{r}
#first create a function to run the analysis
fit_model <- function(boot_data, int_variables, other_predictors = NULL){
  #want to make a function that will work for the case when we use one intervention variable and two
  #to do this, we store the formula that will be put into the model
  #we paste the intervention variables, a vector of variable names, at the end. 
  #clean up the int_variables by putting a "+" in between the variable names
  cleaned_int_var <- paste(c(int_variables, other_predictors), collapse = "+")
  model_formula <- as.formula(paste("cbind(round(imputed_deaths), round(num_alive))~ State +
                           s(Time_Period_ID, bs = 'cr', by = as.factor(Region)) +
                           Naloxone_Pharmacy_Yes_Redefined +
                           Naloxone_Pharmacy_No_Redefined +
                           Medical_Marijuana_Redefined +
                           Recreational_Marijuana_Redefined +
                           GSL_Redefined +
                           PDMP_Redefined +
                           Medicaid_Expansion_Redefined +",
                           cleaned_int_var))
  #plug model_formula into the gam model
  boot_model <- gam(model_formula, data = boot_data, family = "binomial")
  #take the summary of the gam model
  summary_boot_model <- summary(boot_model)
  #extract the coefficients for the variables excluding intervention and state
  int_variables_coef <- summary_boot_model$p.coeff[grepl("Intercept", names(summary_boot_model$p.coeff)) == FALSE &
                                                     grepl("State", names(summary_boot_model$p.coeff)) == FALSE]
  #return the coefficients
  return(int_variables_coef)
}

bootstrap_run <- function(data_set, int_variables, other_predictors = NULL, boot_var = "State", n_iter = 1000, seed = 123, 
                          print_index = TRUE){
  boot_var_unique <- unique(data_set[,boot_var])
  stored_coef <- data.frame()
  # names(stored_coef) <- c("(Intercept)", sapply(unique(data_set$State), function(x){paste("State", x, sep = "")}), 
  #                         "Naloxone_Pharmacy_Yes_Redefined", "Naloxone_Pharmacy_No_Redefined", "Medical_Marijuana_Redefined",
  #                          "Recreational_Marijuana_Redefined", "GSL_Redefined", "PDMP_Redefined", "Medicaid_Expansion_Redefined",
  #                         int_variables, other_predictors)
  stored_attributable_deaths <- data.frame()
  
  #set seed
  set.seed(seed)
  #run n_iter times
  for(iter in 1:n_iter){
    if(print_index == TRUE & iter %% 10 == 0){print(iter)}
    #sample state with replacement
    sampled_var <- sample(boot_var_unique, length(boot_var_unique), replace = TRUE)
    #bootstrap dataset contains the sampled states
    boot_data_list <- lapply(sampled_var, function(x){data_set[data_set[,boot_var] == x,]})
    boot_data <- do.call(rbind, boot_data_list)

    #run model 
    fitted_model_coef <- fit_model(boot_data, int_variables, other_predictors)
    #store the coefficients
    stored_coef <- rbind(stored_coef, fitted_model_coef)
    names(stored_coef) <- names(fitted_model_coef)

    #estimate attributable deaths
    #first create vector to store the counts
    attr_deaths_boot <- rep(0, length(unique(year(data_set$Time_Period_Start))))
    names(attr_deaths_boot) <- unique(year(data_set$Time_Period_Start))

    #first subset data to the rows in which intervention is non-zero
    if(length(int_variables) == 2){
      #write the condition to filter the dataset to those with int > 0
      condition <- paste(paste(int_variables, collapse = "> 0 &"), "> 0")
    }else{
      condition <- paste(int_variables, "> 0")
    }
    attr_death_data <- data_set %>%
      filter(eval(parse(text = condition)))

    attr_death_data <- attr_death_data %>%
      #then we compute the probability of overdose death had intervention not occured
      mutate(p_OD_no_int = expit(logit(imputed_deaths/population) - 
                                   apply(sapply(int_variables, function(x){attr_death_data[,x]*fitted_model_coef[x]}), 
                                         1, sum)),
             #compute the number of OD had intervention not occurred
             n_OD_no_int = population*p_OD_no_int,
             #compute the number of attributable deaths
             n_attr = imputed_deaths - n_OD_no_int
             
      )
    #now group by the year to find the sum of OD deaths
    yrly_num_attr <- attr_death_data %>%
      group_by(year = year(Time_Period_Start)) %>%
      summarise(total_attr_deaths = sum(n_attr))
    
    #store into the attributable deaths
    attr_deaths_boot <- as.vector(yrly_num_attr$total_attr_deaths)
    stored_attributable_deaths <- rbind(stored_attributable_deaths, attr_deaths_boot)
  }
  
  names(stored_attributable_deaths) <- unique(year(data_set$Time_Period_Start))
  #rename the columns of the dataset so that we know which columns the coefficients correspond to
  return(list(stored_coef = stored_coef, stored_attributable_deaths = stored_attributable_deaths))
}

#run the bootstrap
main_analysis_data <- as.data.frame(main_analysis_data)
# bootstrap_results_main_anlys <- bootstrap_run(main_analysis_data, int_variables = "Intervention_Redefined",
#                                   other_predictors = "num_states_w_intervention",
#                                   n_iter = 2000)


# write.csv(bootstrap_results_main_anlys$stored_coef, "./Data/bootstrap_coef_main_anlys_11_29_21.csv", row.names = FALSE)
# write.csv(bootstrap_results_main_anlys$stored_attributable_deaths, "./Data/bootstrap_attr_deaths_main_anlys_11_29_21.csv", row.names = FALSE)

stored_coef <- read.csv("./Data/bootstrap_coef_main_anlys_11_29_21.csv")
stored_attributable_deaths <- read.csv("./Data/bootstrap_attr_deaths_main_anlys_11_29_21.csv")
bootstrap_results <- list(stored_coef = stored_coef, stored_attributable_deaths = stored_attributable_deaths)

apply(bootstrap_results_main_anlys$stored_coef, 2, quantile, probs = c(.025, .5, .975))
coef_bootstrap_tab <- data.frame(t(apply(bootstrap_results_main_anlys$stored_coef, 2, quantile, probs = c(.025, .5, .975))))
coef_bootstrap_tab$predictors <- row.names(coef_bootstrap_tab)
coef_bootstrap_tab$mean <- apply(bootstrap_results_main_anlys$stored_coef, 2, mean)
row.names(coef_bootstrap_tab) <- NULL
colnames(coef_bootstrap_tab) <- c("2.5_perc", "50_perc", "97.5_perc", "predictors", "mean" )
coef_bootstrap_tab <- coef_bootstrap_tab[,c(4,1,2,5,3)]

coef_bootstrap_tab$exp_2.5_perc <- exp(coef_bootstrap_tab$`2.5_perc`)
coef_bootstrap_tab$exp_mean <- exp(coef_bootstrap_tab$mean)
coef_bootstrap_tab$exp_97.5_perc <- exp(coef_bootstrap_tab$`97.5_perc`)
# write.csv(coef_bootstrap_tab, "./Data/bootstrap_predictors_summary_main_anlys_11_29_21.csv", row.names = FALSE)


apply(bootstrap_results_main_anlys$stored_coef, 2, mean)

#convert the dataset to long to plot
bootstrap_results_stored_coef_long <- bootstrap_results$stored_coef %>%
  pivot_longer(everything(), 
               names_to = "predictor",
               values_to = "coef") %>%
  group_by(predictor) %>%
  mutate(lb = quantile(coef, 0.025),
         ub = quantile(coef, 0.975), 
         mean = mean(coef))

#merge the real coefficients into the dataset
sensitivity_anlys_age_unemp_model_coef <- data.frame(predictor = names(summary(sensitivity_anlys_age_unemp_model)$p.coeff), 
                                                     coef_obs = summary(sensitivity_anlys_age_unemp_model)$p.coeff)
bootstrap_results_stored_coef_long <- merge(bootstrap_results_stored_coef_long, sensitivity_anlys_age_unemp_model_coef, 
                                            by = "predictor")

ggplot(bootstrap_results_stored_coef_long, aes(x = coef)) + 
  geom_histogram() + 
  facet_wrap(~predictor, scales = "free") + 
  geom_vline(aes(xintercept = lb, color = "95% CI")) + 
  geom_vline(aes(xintercept = ub, color = "95% CI")) + 
  geom_vline(aes(xintercept = mean, color = "mean")) + 
  geom_vline(aes(xintercept = coef_obs, color = "Observed")) + 
  theme(axis.text.x = element_text(hjust = 1, size = 6, family = "Times"),
        axis.text.y = element_text(size = 6, family = "Times"),
        axis.title = element_text(size = 10, face = "bold", family = "Times"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size=8),
        panel.background = element_rect("white"),
        legend.position = "bottom")

bootstrap_results_attr_deaths_long <- bootstrap_results$stored_attributable_deaths %>%
  pivot_longer(everything(),
               names_to = "year",
               values_to = "attr_death") %>%
  group_by(year) %>%
  summarise(mean_death = mean(attr_death),
            lb_death = quantile(attr_death, 0.025),
            ub_death = quantile(attr_death, 0.975)) %>%
  ungroup()

ggplot(bootstrap_results_attr_deaths_long) + 
  geom_line(aes(y = mean_death, x = year, color = "Estimate", group = 1)) +
  geom_point(aes(y = mean_death, x = year)) + 
  geom_line(aes(y = lb_death, x = year, color = "95% CI", group = 1)) + 
  geom_line(aes(y = ub_death, x = year, color = "95% CI", group = 1)) + 
  labs(y = "Number of Unintentional Overdose Deaths Attributable to DIH Prosecutions",
       x = "Year") + 
  theme(axis.text=element_text(family="Times",size=10),
        axis.title=element_text(family="Times", size=10, face="bold"),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.line = element_line(colour = "black"),
        axis.text.x = element_text(family="Times", size=10, angle = 30),
        panel.background = element_rect("white")) 
```

# Secondary Analysis: Effect of At Least One DIH Prosecution Report in Media on All Overdose Deaths
## Analysis 
```{r, results = "asis"}
############################## Secondary Analysis: All Overdose Deaths ###############################
od_all_data <- read.csv("./Data/full_data_set_9_7_21_all_od.csv")
colnames(od_all_data)[which(colnames(od_all_data) == "sum_deaths")] <- "imputed_deaths"
od_all_data$Time_Period_Start <- as.Date(od_all_data$Time_Period_Start)
od_all_data$Intervention_First_Date <- as.Date(od_all_data$Intervention_First_Date)

#initialize vector with "West" and then impute the other regions for the states
od_all_data$Region<-rep("West", nrow(od_all_data))
for(state in unique(od_all_data$State)){
  if(state %in% region.list$Northeast){
    od_all_data$Region[od_all_data$State == state]<-"Northeast"
  }else if(state %in% region.list$Midwest){
    od_all_data$Region[od_all_data$State == state]<-"Midwest"
  }else if(state %in% region.list$South){
    od_all_data$Region[od_all_data$State == state]<-"South"
  }
}

#compute the number of states with intervention by adding up the intervention variable
od_all_data <- od_all_data %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention = sum(Intervention_Redefined))

#model that we will be using for the main analysis
#cr is used for cubic regression spline -- we are smoothing time effects by region
#run the analysis for all the states
od_all_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                    s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                    Naloxone_Pharmacy_Yes_Redefined +
                    Naloxone_Pharmacy_No_Redefined +
                    Medical_Marijuana_Redefined +
                    Recreational_Marijuana_Redefined +
                    GSL_Redefined +
                    PDMP_Redefined +
                    Medicaid_Expansion_Redefined +
                    Intervention_Redefined + 
                    num_states_w_intervention,
                  data = od_all_data, family = "binomial")

#summary output of the model
stargazer(od_all_model, type = "html", dep.var.labels = "All Overdose Deaths")
# gam.check(od_all_model)

```

## Plots
```{r}
#create a data frame to store the results and compute the confidence intervals
#initialize the columns
od_all_plot_table<-data.frame(State = od_all_data$State)
od_all_plot_table$Fitted<-rep(NA, nrow(od_all_plot_table))
od_all_plot_table$Observed<-rep(NA, nrow(od_all_plot_table))
od_all_plot_table$Time<-od_all_data$Time_Period_ID
od_all_plot_table$Time_Date<-od_all_data$Time_Period_Start
od_all_plot_table$Intervention_Date<-od_all_data$Intervention_First_Date

#we want to compare the fitted probability of overdose death and the observed values to see how the model does as a goodness of fit visual test
for(i in unique(od_all_plot_table$State)){
  #for each state, we first subset the main analysis data to only look at the data for that state
  index_of_state<-which(od_all_plot_table$State == i)
  #impute the fitted and observed probability of overdose death for the state
  od_all_plot_table$Fitted[index_of_state]<-fitted(od_all_model)[index_of_state]
  od_all_plot_table$Observed[index_of_state]<-(od_all_data$imputed_deaths[od_all_data$State == i]/od_all_data$population[od_all_data$State == i])
}
```

```{r, fig.height=8, fig.width=8}
#plot to compare the fitted values vs observed deaths
ggplot(data = od_all_plot_table) + 
  geom_point(aes(x = Time_Date, y = Observed*100000, group = 1, color = "Observed"), size = 0.5, alpha = 0.5) +
  geom_line(aes(x = Time_Date, y = Observed*100000, group = 1, color = "Observed"))+
  geom_point(data = od_all_plot_table, aes(x = Time_Date, y = Fitted*100000, group = 1, col = "Estimate"), size = 0.5, alpha = 0.5) + 
  geom_line(data = od_all_plot_table, aes(x = Time_Date, y = Fitted*100000, group = 1, col = "Estimate")) +
  geom_vline(od_all_plot_table, mapping = aes(xintercept = Intervention_Date)) +
  facet_wrap(facets = vars(State)) +
  scale_color_manual(values = c("pink", "blue")) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7), axis.text.y = element_text(size = 7),
        axis.title=element_text(size = 15,face = "bold"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size=8),panel.background = element_rect("white"),
        legend.position = "bottom") +
  labs(x = "Year", y = "Drug Overdose Death Rates per 100,000 People", color = "")
```

```{r}
#plot to show the time smooth effects by region
plot(od_all_model, ylab = "Smoothed Time Effects", xlab = "Time", pages = 1)

#diagnostic plots to check model
gam.check(od_all_model)
plot(logit(fitted(od_all_model)), residuals(od_all_model),
     xlab = "Logit(Fitted Values)", ylab = "Deviance Residuals" )

```

## Compile Results
```{r}
############################## Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
od_all_full_table<-data.frame(coef(od_all_model))
#check to see how the table looks
head(od_all_full_table)
#rename the column to "Coefficient_Estimate"
colnames(od_all_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined", 
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(od_all_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(od_all_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(od_all_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(od_all_full_table)[i]<-substr(rownames(od_all_full_table)[i], start = 6,
                                             stop = nchar(rownames(od_all_full_table)[i]))

    }else if(rownames(od_all_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(od_all_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(od_all_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(od_all_full_table)[i]<-paste("Smoothed Time for Region ",
                                            substr(rownames(od_all_full_table)[i], start = 36,
                                                   stop = nchar(rownames(od_all_full_table)[i])),
                                            sep = "")

    }
  }
}

#confidence intervals for the coefficients
od_all_full_table$Coefficient_Lower_Bound<-od_all_full_table$Coefficient_Estimate - 1.96*summary(od_all_model)$se
od_all_full_table$Coefficient_Upper_Bound<-od_all_full_table$Coefficient_Estimate + 1.96*summary(od_all_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
od_all_full_table$Odds_Ratio<-exp(od_all_full_table$Coefficient_Estimate)
od_all_full_table$Odds_Ratio_LB<-exp(od_all_full_table$Coefficient_Lower_Bound)
od_all_full_table$Odds_Ratio_UB<-exp(od_all_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
od_all_full_table$Standard_Error<-summary(od_all_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
od_all_full_table$p_value<-c(summary(od_all_model)$p.pv, 
                             rep(NA, length(coef(od_all_model)) - 
                                   length(summary(od_all_model)$p.pv)))

head(od_all_full_table)
tail(od_all_full_table)

#save the table into a CSV
# write.csv(round(od_all_full_table,5), "./Data/coefficients_GAM_9_1_20_full_data_all_od.csv")

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(od_all_full_table) %in% covariates)
od_all_covariate_table<-(round(od_all_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(od_all_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                    "Naloxone_Pharmacy_No",
                                    "Medical_Marijuana",
                                    "Recreational_Marijuana",
                                    "GSL", 
                                    "PDMP", 
                                    "Medicaid_Expansion",
                                    "Intervention", 
                                    "Number of States with DIH Prosecution")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
od_all_covariate_table<-rbind(od_all_covariate_table, od_all_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
od_all_covariate_table<-od_all_covariate_table[,-which(colnames(od_all_covariate_table) %in%
                                                         c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(od_all_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(od_all_covariate_table, 11)

#save the table into a CSV
# write.csv(round(od_all_covariate_table, 5), "./Data/coefficients_covariates_9_6_21_full_data_all_od.csv")
```

## Attributable Deaths
```{r}
############################## Number of Overdose Deaths Attributed to Intervention ###############################
#find the number of deaths attributable to the intervention
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_od_all<-od_all_data[which(od_all_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_od_all<-expit(-coef(od_all_model)["Intervention_Redefined"]*attr_deaths_anlys_od_all$Intervention_Redefined
                             + logit(attr_deaths_anlys_od_all$imputed_deaths/attr_deaths_anlys_od_all$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(od_all_model) - 1.96*summary(od_all_model)$se
coef_ub<-coef(od_all_model) + 1.96*summary(od_all_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_od_all<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_od_all$Intervention_Redefined
                                + logit(attr_deaths_anlys_od_all$imputed_deaths/attr_deaths_anlys_od_all$population))

prob_od_no_int_UB_od_all<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_od_all$Intervention_Redefined
                                + logit(attr_deaths_anlys_od_all$imputed_deaths/attr_deaths_anlys_od_all$population))


#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(attr_deaths_anlys_od_all$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_od_all$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_od_all$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_od_all$imputed_deaths[time_point_index]
                          - prob_od_no_int_od_all[time_point_index]*attr_deaths_anlys_od_all$population[time_point_index])
  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_od_all$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_od_all[time_point_index]*attr_deaths_anlys_od_all$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_od_all$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_od_all[time_point_index]*attr_deaths_anlys_od_all$population[time_point_index])
  index<-index + 1
}

num_attr_od_od_all<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_od_all$Time_Period_ID)),
                               "Time_Start" = sort(unique(attr_deaths_anlys_od_all$Time_Period_Start)),
                               "Num_Attr_Deaths" = num_attr_od,
                               "Num_Attr_Deaths_LB" = num_attr_od_LB,
                               "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_od_all$Num_Attr_Deaths) #16336.4
summary(num_attr_od_od_all$Num_Attr_Deaths)
num_attr_od_od_all$Time_Start<-as.Date(num_attr_od_od_all$Time_Start)

#compute the 95% CI for the total
sum(num_attr_od_od_all$Num_Attr_Deaths_LB)
sum(num_attr_od_od_all$Num_Attr_Deaths_UB)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_od_all<-num_attr_od_od_all %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_od_all$deaths)
summary(yearly_num_Attr_Deaths_od_all$death_lb)
summary(yearly_num_Attr_Deaths_od_all$death_ub)

```

# Sensitivity Analysis 1: Confounding by Indication
## Analysis

```{r, results = "asis"}
############################## Sensitivity Analysis 1: Confounding by Indication Analysis ###############################
#create a variable that is equal to 1 just before intervention
#initialize the column to all zeros
main_analysis_data$justBeforeIntervention<-0

#for each state, we first subset the data so it only contains the state's data
for(state in unique(main_analysis_data$State)){
  state_data<-main_analysis_data[main_analysis_data$State == state,]
  #then, we find the first time point where intervention occurred
  index_first_intervention<-which(state_data$Intervention_Redefined>0)[1]
  #impute a 1 for the time point right before when intervention first occurs
  main_analysis_data$justBeforeIntervention[main_analysis_data$State == state][index_first_intervention-1]<-1
}

#subset the data so that we are only looking at the periods before the intervention occurs for each state
sensitivity_anlys_conf_by_indication_data<-data.frame()
for(state in unique(main_analysis_data$State)){
  state_data<-main_analysis_data[main_analysis_data$State == state,]
  #we don't include these states because Georgia and Ohio have intervention in 2000
  #Hawaii is in this list because it doesn't have an intervention, so we will encounter an error
  #if the state is Hawaii, we'll go to the else if condition
  if(!(state %in% c("Hawaii", "Georgia", "Ohio"))){
    #look for the index where it is just before the intervention
    index_first_intervention<-which(state_data$justBeforeIntervention>0)
    #add the rows that occur before the intervention to the sensitivity analysis data
    sensitivity_anlys_conf_by_indication_data<-rbind(sensitivity_anlys_conf_by_indication_data, state_data[1:(index_first_intervention),])

  }else if(state == "Hawaii"){
    #Hawaii doesn't have an intervention, so we want to include all the rows of Hawaii
    sensitivity_anlys_conf_by_indication_data<-rbind(sensitivity_anlys_conf_by_indication_data, state_data)
  }
}

#run the analysis for the sensitivity analysis
sensitivity_anlys_conf_by_indication<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                            s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                                            Naloxone_Pharmacy_Yes_Redefined +
                                            Naloxone_Pharmacy_No_Redefined +
                                            Medical_Marijuana_Redefined +
                                            Recreational_Marijuana_Redefined +
                                            GSL_Redefined +
                                            PDMP_Redefined +
                                            Medicaid_Expansion_Redefined +
                                            justBeforeIntervention + 
                                            num_states_w_intervention,
                                          data = sensitivity_anlys_conf_by_indication_data, family = "binomial")

stargazer(sensitivity_anlys_conf_by_indication, type = "html", dep.var.labels = "Unintentional Overdose Deaths")

```

## Compile Results
```{r}
############################## Sensitivity Analysis 1: Make Data Frame of Results and 95% CI ###############################
#store the coefficients of all the terms into a table
sensitivity_anlys_conf_by_indication_full_table<-data.frame(coef(sensitivity_anlys_conf_by_indication))
head(sensitivity_anlys_conf_by_indication_full_table)
#change the column name to "Coefficient_Estimate"
colnames(sensitivity_anlys_conf_by_indication_full_table)<-c("Coefficient_Estimate")

#store a vector of the covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "justBeforeIntervention",
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_conf_by_indication_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_conf_by_indication_full_table)[i] %in% covariates)){

    #we see if it is a state effect
    if(substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_conf_by_indication_full_table)[i]<-substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i],
                                                                           start = 6,
                                                                           stop =
                                                                        nchar(rownames(sensitivity_anlys_conf_by_indication_full_table)[i]))

    }else if(rownames(sensitivity_anlys_conf_by_indication_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_conf_by_indication_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_conf_by_indication_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                          substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i], 
                                                                                 start = 36, 
                                                                                 stop = 
                                                                      nchar(rownames(sensitivity_anlys_conf_by_indication_full_table)[i])),
                                                                          sep = "")


    }
  }
}

#store the 95% CI
sensitivity_anlys_conf_by_indication_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_conf_by_indication_full_table$Coefficient_Estimate - 1.96*summary(sensitivity_anlys_conf_by_indication)$se
sensitivity_anlys_conf_by_indication_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_conf_by_indication_full_table$Coefficient_Estimate + 1.96*summary(sensitivity_anlys_conf_by_indication)$se

#store the odds ratio estimates and 95% CI
sensitivity_anlys_conf_by_indication_full_table$Odds_Ratio<-exp(sensitivity_anlys_conf_by_indication_full_table$Coefficient_Estimate)
sensitivity_anlys_conf_by_indication_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_conf_by_indication_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_conf_by_indication_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_conf_by_indication_full_table$Coefficient_Upper_Bound)

#store the standard error and p-values
sensitivity_anlys_conf_by_indication_full_table$Standard_Error<-summary(sensitivity_anlys_conf_by_indication)$se
#since there are no p-values for the smoothed time effects, it imputes NA for those rows
sensitivity_anlys_conf_by_indication_full_table$p_value<-c(summary(sensitivity_anlys_conf_by_indication)$p.pv,
                                                           rep(NA, length(coef(sensitivity_anlys_conf_by_indication)) - length(summary(sensitivity_anlys_conf_by_indication)$p.pv)))

head(sensitivity_anlys_conf_by_indication_full_table)
tail(sensitivity_anlys_conf_by_indication_full_table)

#export the sensitivity analysis confounding by indication full table of estimates as CSV
# write.csv(round(sensitivity_anlys_conf_by_indication_full_table,3), "./Data/coefficients_JustBeforeInd_9_6_21_full_data.csv")

#export out a table with just the covariates:
#find the rows in the full table which contain estimates for the covariates and extract them
covariate_Index<-which(rownames(sensitivity_anlys_conf_by_indication_full_table) %in% covariates)
sensitivity_anlys_conf_by_indication_covariate_table<-(round(sensitivity_anlys_conf_by_indication_full_table[covariate_Index,],5))

#rename these variables so they look nicer
rownames(sensitivity_anlys_conf_by_indication_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                                  "Naloxone_Pharmacy_No",
                                                                  "Medical_Marijuana",
                                                                  "Recreational_Marijuana",
                                                                  "GSL", 
                                                                  "PDMP",
                                                                  "Medicaid_Expansion",
                                                                  "Just Before Indicator", 
                                                                  "Number of States w DIH Prosecution")

#put the covariate rows on top and the rest of the data at the bottom
sensitivity_anlys_conf_by_indication_covariate_table<-rbind(sensitivity_anlys_conf_by_indication_covariate_table,
                                                            sensitivity_anlys_conf_by_indication_full_table[-covariate_Index,])

#extract the columns that gives the odds ratio estimates
sensitivity_anlys_conf_by_indication_covariate_table<-sensitivity_anlys_conf_by_indication_covariate_table[,-which(colnames(sensitivity_anlys_conf_by_indication_covariate_table) %in% c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]
colnames(sensitivity_anlys_conf_by_indication_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_conf_by_indication_covariate_table, 10)

#export the covariate table into a CSV file
# write.csv(round(sensitivity_anlys_conf_by_indication_covariate_table,3), "./Data/covariates_just_before_intervention_9_6_21_full_data.csv")

```

# Sensitivity Analysis 2: Exclude States with At Least 75% Missing Monthly Data
## Analysis

```{r}
############################## Exclude States from Analysis Based on Missing Data ###############################
#subset the data to be between 2000 and 2019
overdose_monthly_imputed<-read.csv("./Data/od_data_interpolated_unintentional_1999_2019_age_17_up_9_6_21.csv")
od_2000_to_2019<-overdose_monthly_imputed[overdose_monthly_imputed$Year>=2000 & overdose_monthly_imputed$Year<=2019,]

#convert the date to Date objects and the number of deaths to numeric
od_2000_to_2019$Date<-as.Date(as.yearmon(od_2000_to_2019$Month.Code, format = "%Y/%m"))
od_2000_to_2019$Deaths<-as.numeric(od_2000_to_2019$Deaths)

#plot to look at the trend of the outcome and see how much data is missing
ggplot(od_2000_to_2019, aes(x = Date, y = Deaths)) + geom_line() + facet_wrap(vars(State))

#find the states where the proportion of monthly missing data for years 2000 to 2017 is greater than 75%
statesToExcludeCriteria<-sapply(unique(od_2000_to_2019$State), function(state){
  mean(is.na(od_2000_to_2019$Deaths[od_2000_to_2019$State == state]))>.75})

statesToExclude<-unique(od_2000_to_2019$State)[statesToExcludeCriteria]
#states excluded: [1] "Alaska"       "Montana"      "Nebraska"     "North Dakota" "South Dakota" "Vermont" "Wyoming"

#calculate the median (and IQR) of missingnness from resulting data
summary_missingness <- od_2000_to_2019 %>% 
  group_by(State) %>% 
  summarise(missing = mean(is.na(Deaths)))

median(summary_missingness$missing[!summary_missingness$State %in% statesToExclude])
IQR(summary_missingness$missing[!summary_missingness$State %in% statesToExclude])
summary(summary_missingness$missing[!summary_missingness$State %in% statesToExclude])

#include only the states that are not excluded
sensitivity_anlys_exclude_states_data<-main_analysis_data[!(main_analysis_data$State %in% statesToExclude), ]


sensitivity_anlys_exclude_states_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                              s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                              Naloxone_Pharmacy_Yes_Redefined +
                                              Naloxone_Pharmacy_No_Redefined +
                                              Medical_Marijuana_Redefined +
                                              Recreational_Marijuana_Redefined +
                                              GSL_Redefined +
                                              PDMP_Redefined +
                                              Medicaid_Expansion_Redefined +
                                              Intervention_Redefined + 
                                              num_states_w_intervention,
                                            data = sensitivity_anlys_exclude_states_data, family = "binomial")

```

```{r, results = "asis"}
stargazer(sensitivity_anlys_exclude_states_model, type = "html", dep.var.labels = "Unintentional Overdose Death")

```

## Compile Results
```{r}

############################## Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
sensitivity_anlys_exclude_states_full_table<-data.frame(coef(sensitivity_anlys_exclude_states_model))
#check to see how the table looks
head(sensitivity_anlys_exclude_states_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_exclude_states_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined",
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_exclude_states_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_exclude_states_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_exclude_states_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_exclude_states_full_table)[i]<-substr(rownames(sensitivity_anlys_exclude_states_full_table)[i], 
                                                                       start = 6,
                                                                       stop = nchar(rownames(sensitivity_anlys_exclude_states_full_table)[i]))

    }else if(rownames(sensitivity_anlys_exclude_states_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_exclude_states_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_exclude_states_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_exclude_states_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                      substr(rownames(sensitivity_anlys_exclude_states_full_table)[i], 
                                                                             start = 36,
                                                                             stop = nchar(rownames(sensitivity_anlys_exclude_states_full_table)[i])),
                                                                      sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_exclude_states_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_exclude_states_full_table$Coefficient_Estimate -
  1.96*summary(sensitivity_anlys_exclude_states_model)$se
sensitivity_anlys_exclude_states_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_exclude_states_full_table$Coefficient_Estimate +
  1.96*summary(sensitivity_anlys_exclude_states_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_exclude_states_full_table$Odds_Ratio<-exp(sensitivity_anlys_exclude_states_full_table$Coefficient_Estimate)
sensitivity_anlys_exclude_states_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_exclude_states_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_exclude_states_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_exclude_states_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_exclude_states_full_table$Standard_Error<-summary(sensitivity_anlys_exclude_states_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_exclude_states_full_table$p_value<-c(summary(sensitivity_anlys_exclude_states_model)$p.pv,
                                                       rep(NA, length(coef(sensitivity_anlys_exclude_states_model)) -
                                                             length(summary(sensitivity_anlys_exclude_states_model)$p.pv)))

head(sensitivity_anlys_exclude_states_full_table)
tail(sensitivity_anlys_exclude_states_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_exclude_states_full_table) %in% covariates)
sensitivity_anlys_exclude_states_covariate_table<-(round(sensitivity_anlys_exclude_states_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_exclude_states_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                              "Naloxone_Pharmacy_No",
                                                              "Medical_Marijuana",
                                                              "Recreational_Marijuana",
                                                              "GSL", 
                                                              "PDMP", 
                                                              "Medicaid_Expansion",
                                                              "Intervention_Redefined",
                                                              "Number of States w DIH Prosecution")

#now, reorganize the data so that the covariates are on top and the rest of the variables are below
sensitivity_anlys_exclude_states_covariate_table<-rbind(sensitivity_anlys_exclude_states_covariate_table, sensitivity_anlys_exclude_states_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_exclude_states_covariate_table<-sensitivity_anlys_exclude_states_covariate_table[,-which(colnames(sensitivity_anlys_exclude_states_covariate_table) %in% c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_exclude_states_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_exclude_states_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_exclude_states_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_exclude_states.csv")

```

## Attributable Deaths
```{r}
###################################### Attributable Deaths #############################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_exclude_states<-sensitivity_anlys_exclude_states_data[which(sensitivity_anlys_exclude_states_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_exclude_states<-expit(-coef(sensitivity_anlys_exclude_states_model)["Intervention_Redefined"]*attr_deaths_anlys_exclude_states$Intervention_Redefined
                                     + logit(attr_deaths_anlys_exclude_states$imputed_deaths/attr_deaths_anlys_exclude_states$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_exclude_states_model) - 1.96*summary(sensitivity_anlys_exclude_states_model)$se
coef_ub<-coef(sensitivity_anlys_exclude_states_model) + 1.96*summary(sensitivity_anlys_exclude_states_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_exclude_states<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_exclude_states$Intervention_Redefined
                                        + logit(attr_deaths_anlys_exclude_states$imputed_deaths/attr_deaths_anlys_exclude_states$population))

prob_od_no_int_UB_exclude_states<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_exclude_states$Intervention_Redefined
                                        + logit(attr_deaths_anlys_exclude_states$imputed_deaths/attr_deaths_anlys_exclude_states$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_exclude_states_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_exclude_states$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_exclude_states$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_exclude_states$imputed_deaths[time_point_index]
                          - prob_od_no_int_exclude_states[time_point_index]*attr_deaths_anlys_exclude_states$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_exclude_states$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_exclude_states[time_point_index]*attr_deaths_anlys_exclude_states$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_exclude_states$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_exclude_states[time_point_index]*attr_deaths_anlys_exclude_states$population[time_point_index])


  index<-index + 1
}

num_attr_od_exclude_states<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_exclude_states$Time_Period_ID)),
                                       "Time_Start" = sort(unique(attr_deaths_anlys_exclude_states$Time_Period_Start)),
                                       "Num_Attr_Deaths" = num_attr_od,
                                       "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                       "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_exclude_states$Num_Attr_Deaths)


#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_exclude_states<-num_attr_od_exclude_states %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_exclude_states$deaths)



```

# Sensitivity Analysis 2b: Excluding Different States
## Analysis

```{r}
############################## Exclude Different States from Analysis ###############################
# main_analysis_data %>% 
#   group_by(State) %>% 
#   summarise(sum_int = sum(Intervention_Redefined), sum_deaths = sum(imputed_deaths)) %>% 
#   arrange(desc(sum_deaths))

# list_of_states <- c("Alabama",
#                       "Alaska",
#                       "Arizona",
#                       "Arkansas",
#                       "California",
#                       "Colorado",
#                       "Connecticut",
#                       "Delaware",
#                       "Florida",
#                       "Georgia",
#                       "Hawaii",
#                       "Idaho",
#                       "Illinois",
#                       "Indiana",
#                       "Iowa",
#                       "Kansas",
#                       "Kentucky",
#                       "Louisiana",
#                       "Maine",
#                       "Maryland",
#                       "Massachusetts",
#                       "Michigan",
#                       "Minnesota",
#                       "Mississippi",
#                       "Missouri",
#                       "Montana",
#                       "Nebraska",
#                       "Nevada",
#                       "New Hampshire",
#                       "New Jersey",
#                       "New Mexico",
#                       "New York",
#                       "North Carolina",
#                       "North Dakota",
#                       "Ohio",
#                       "Oklahoma",
#                       "Oregon",
#                       "Pennsylvania",
#                       "Rhode Island",
#                       "South Carolina",
#                       "South Dakota",
#                       "Tennessee",
#                       "Texas",
#                       "Utah",
#                       "Vermont",
#                       "Virginia",
#                       "Washington",
#                       "West Virginia",
#                       "Wisconsin",
#                       "Wyoming" )

#use R's built-in list of state names
list_of_states <- c("", state.name)
sensitivity_anlys_exclude_diff_states_tab_results <- data.frame(state = c("", state.name), 
                                                                coefficient_intervention = rep(NA, 51),
                                                                std_error = rep(NA, 51),
                                                                p_val = rep(NA, 51))
for(state in list_of_states){
  #include only the states that are not excluded
  
  sensitivity_anlys_exclude_diff_states_data <- main_analysis_data %>%
    ungroup() %>%
    filter(State != state)
  
  sensitivity_anlys_exclude_diff_states_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                                     s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                                     Naloxone_Pharmacy_Yes_Redefined +
                                                     Naloxone_Pharmacy_No_Redefined +
                                                     Medical_Marijuana_Redefined +
                                                     Recreational_Marijuana_Redefined +
                                                     GSL_Redefined +
                                                     PDMP_Redefined +
                                                     Medicaid_Expansion_Redefined +
                                                     Intervention_Redefined + 
                                                     num_states_w_intervention,
                                                   data = sensitivity_anlys_exclude_diff_states_data, family = "binomial")
  #obtain the summary of model
  summary_sensitivity_analys_exclude_diff_states <- summary(sensitivity_anlys_exclude_diff_states_model)
  #obtain coefficient of intervention
  sensitivity_anlys_exclude_diff_states_tab_results$coefficient_intervention[sensitivity_anlys_exclude_diff_states_tab_results$state == state] <- summary_sensitivity_analys_exclude_diff_states$p.coeff["Intervention_Redefined"]
  #obtain standard error of intervention
  sensitivity_anlys_exclude_diff_states_tab_results$std_error[sensitivity_anlys_exclude_diff_states_tab_results$state == state] <- summary_sensitivity_analys_exclude_diff_states$se["Intervention_Redefined"]
  #obtain standard error of intervention
  sensitivity_anlys_exclude_diff_states_tab_results$p_val[sensitivity_anlys_exclude_diff_states_tab_results$state == state] <- summary_sensitivity_analys_exclude_diff_states$p.pv["Intervention_Redefined"]
}

sensitivity_anlys_exclude_diff_states_tab_results$state[sensitivity_anlys_exclude_diff_states_tab_results$state == ""] <- "all"

```

```{r, results = "asis"}
stargazer(sensitivity_anlys_exclude_diff_states_model, type = "html", dep.var.labels = "Unintentional Overdose Death")

```

## Compile Results
```{r}

############################## Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
sensitivity_anlys_exclude_diff_states_full_table<-data.frame(coef(sensitivity_anlys_exclude_diff_states_model))
#check to see how the table looks
head(sensitivity_anlys_exclude_diff_states_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_exclude_diff_states_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined",
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_exclude_diff_states_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_exclude_diff_states_full_table)[i]<-substr(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i], 
                                                                       start = 6,
                                                                       stop = nchar(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i]))

    }else if(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_exclude_diff_states_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_exclude_diff_states_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                      substr(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i], 
                                                                             start = 36,
                                                                             stop = nchar(rownames(sensitivity_anlys_exclude_diff_states_full_table)[i])),
                                                                      sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_exclude_diff_states_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_exclude_diff_states_full_table$Coefficient_Estimate -
  1.96*summary(sensitivity_anlys_exclude_diff_states_model)$se
sensitivity_anlys_exclude_diff_states_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_exclude_diff_states_full_table$Coefficient_Estimate +
  1.96*summary(sensitivity_anlys_exclude_diff_states_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_exclude_diff_states_full_table$Odds_Ratio<-exp(sensitivity_anlys_exclude_diff_states_full_table$Coefficient_Estimate)
sensitivity_anlys_exclude_diff_states_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_exclude_diff_states_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_exclude_diff_states_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_exclude_diff_states_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_exclude_diff_states_full_table$Standard_Error<-summary(sensitivity_anlys_exclude_diff_states_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_exclude_diff_states_full_table$p_value<-c(summary(sensitivity_anlys_exclude_diff_states_model)$p.pv,
                                                       rep(NA, length(coef(sensitivity_anlys_exclude_diff_states_model)) -
                                                             length(summary(sensitivity_anlys_exclude_diff_states_model)$p.pv)))

head(sensitivity_anlys_exclude_diff_states_full_table)
tail(sensitivity_anlys_exclude_diff_states_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_exclude_diff_states_full_table) %in% covariates)
sensitivity_anlys_exclude_diff_states_covariate_table<-(round(sensitivity_anlys_exclude_diff_states_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_exclude_diff_states_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                              "Naloxone_Pharmacy_No",
                                                              "Medical_Marijuana",
                                                              "Recreational_Marijuana",
                                                              "GSL", 
                                                              "PDMP", 
                                                              "Medicaid_Expansion",
                                                              "Intervention_Redefined",
                                                              "Number of States w DIH Prosecution")

#now, reorganize the data so that the covariates are on top and the rest of the variables are below
sensitivity_anlys_exclude_diff_states_covariate_table<-rbind(sensitivity_anlys_exclude_diff_states_covariate_table, sensitivity_anlys_exclude_diff_states_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_exclude_diff_states_covariate_table<-sensitivity_anlys_exclude_diff_states_covariate_table[,-which(colnames(sensitivity_anlys_exclude_diff_states_covariate_table) %in% c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_exclude_diff_states_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_exclude_diff_states_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_exclude_diff_states_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_exclude_diff_states.csv")

```

## Attributable Deaths
```{r}
###################################### Attributable Deaths #############################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_exclude_diff_states<-sensitivity_anlys_exclude_diff_states_data[which(sensitivity_anlys_exclude_diff_states_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_exclude_diff_states<-expit(-coef(sensitivity_anlys_exclude_diff_states_model)["Intervention_Redefined"]*attr_deaths_anlys_exclude_diff_states$Intervention_Redefined
                                     + logit(attr_deaths_anlys_exclude_diff_states$imputed_deaths/attr_deaths_anlys_exclude_diff_states$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_exclude_diff_states_model) - 1.96*summary(sensitivity_anlys_exclude_diff_states_model)$se
coef_ub<-coef(sensitivity_anlys_exclude_diff_states_model) + 1.96*summary(sensitivity_anlys_exclude_diff_states_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_exclude_diff_states<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_exclude_diff_states$Intervention_Redefined
                                        + logit(attr_deaths_anlys_exclude_diff_states$imputed_deaths/attr_deaths_anlys_exclude_diff_states$population))

prob_od_no_int_UB_exclude_diff_states<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_exclude_diff_states$Intervention_Redefined
                                        + logit(attr_deaths_anlys_exclude_diff_states$imputed_deaths/attr_deaths_anlys_exclude_diff_states$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_exclude_diff_states_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_exclude_diff_states$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_exclude_diff_states$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_exclude_diff_states$imputed_deaths[time_point_index]
                          - prob_od_no_int_exclude_diff_states[time_point_index]*attr_deaths_anlys_exclude_diff_states$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_exclude_diff_states$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_exclude_diff_states[time_point_index]*attr_deaths_anlys_exclude_diff_states$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_exclude_diff_states$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_exclude_diff_states[time_point_index]*attr_deaths_anlys_exclude_diff_states$population[time_point_index])


  index<-index + 1
}

num_attr_od_exclude_diff_states<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_exclude_diff_states$Time_Period_ID)),
                                       "Time_Start" = sort(unique(attr_deaths_anlys_exclude_diff_states$Time_Period_Start)),
                                       "Num_Attr_Deaths" = num_attr_od,
                                       "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                       "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_exclude_diff_states$Num_Attr_Deaths)


#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_exclude_diff_states<-num_attr_od_exclude_diff_states %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_exclude_diff_states$deaths)



```

# Sensitivity Analysis 3: Divide Unintentional Deaths Equally Amongst Missing Months
## Analysis
```{r}
### Sensitivity Analysis 3: Divide Unaccountable Deaths Equally Among Missing Months #####
od_data <- read.csv("./Data/unintentional_od_1999_2019_age_17_up.txt", sep = "\t", stringsAsFactors = FALSE)
od_data$Deaths <- as.numeric(od_data$Deaths)
od_data<-od_data[!is.na(od_data$Year),] #delete the rows that just contains data set description info
tail(od_data)
sum(is.na(od_data$Deaths))

#set up the overdose data to impute the missing data
#set up the dates
od_data$Date<-mdy(od_data$Month)
length(unique(od_data$State))
#DC is being counted in this, but we do not have data on prosecutions. Remove these rows
od_data<-od_data[od_data$State!="District of Columbia",]

#interpolate the missing values
od_year <- read.csv("./Data/unintentional_od_yearly_1999_2019_age_17_up.txt", sep = "\t", stringsAsFactors = FALSE)
od_year$Deaths <- as.numeric(od_year$Deaths)
head(od_year, 50)
#see how many states have missing yearly entries and use the totals to impute the missing yearly values.
sum_na <- od_year %>% group_by(State) %>% summarise(sum(is.na(Deaths)))
table(sum_na)

#the 3 states are: North Dakota, South Dakota, and Rhode Island
od_year$Deaths[od_year$State == "North Dakota" & is.na(od_year$Deaths)] <- (od_year$Deaths[od_year$State == "North Dakota" & 
                                                                                             od_year$Notes == "Total"] -
                                                                              sum(od_year$Deaths[od_year$State == "North Dakota" &
                                                                                                   od_year$Notes != "Total"], 
                                                                                  na.rm = TRUE))/sum(is.na(od_year$Deaths[od_year$State == "North Dakota"]))
od_year$Deaths[od_year$State == "South Dakota" & is.na(od_year$Deaths)] <- (od_year$Deaths[od_year$State == "South Dakota" &
                                                                                             od_year$Notes == "Total"] -
                                                                              sum(od_year$Deaths[od_year$State == "South Dakota" &
                                                                                                   od_year$Notes != "Total"], 
                                                                                  na.rm = TRUE))/sum(is.na(od_year$Deaths[od_year$State == "South Dakota"]))
od_year$Deaths[od_year$State == "Rhode Island" & is.na(od_year$Deaths)] <- (od_year$Deaths[od_year$State == "Rhode Island" & 
                                                                                             od_year$Notes == "Total"] -
                                                                              sum(od_year$Deaths[od_year$State == "Rhode Island" &
                                                                                                   od_year$Notes != "Total"], 
                                                                                  na.rm = TRUE))/sum(is.na(od_year$Deaths[od_year$State == "Rhode Island"]))
od_year <- od_year[!is.na(od_year$Year),]
tail(od_year)

od_data$imputed_vals<-rep(NA, nrow(od_data))

startYear<-1999

for(state in unique(od_data$State)){
  #get the values of the deaths for state
  currentDeaths<-od_data$Deaths[od_data$State == state]
  for(year in startYear:2019){
    #find the indices of the missing data -- gets the missing indices for that particular year
    indexMissing <- which(is.na(currentDeaths[(1:12) + 12*(year - startYear)]))
    if(length(indexMissing) != 0){
      #if there are missing values, we find the number of accounted deaths
      currentDeathsTotal <- sum(currentDeaths[(1:12) + 12*(year - startYear)], na.rm = TRUE)
      #and calculate the deaths that are not accounted for using the yearly deaths for the state
      numNotAccounted <- od_year$Deaths[od_year$State == state & od_year$Year == year] - currentDeathsTotal

      #we then divide number of unaccounted deaths evenly by the number of months with missing values
      currentDeaths[(1:12) + 12*(year - startYear)][indexMissing] <- numNotAccounted/length(indexMissing)
    }else{
      #otherwise, if there is no missing values, we skip to the next year
      next
    }
  }
  #store the imputed values
  od_data$imputed_vals[od_data$State == state]<-currentDeaths
}

#group into 6 month time periods now and compute the total number of deaths in each period
od_data_grouped_data <- od_data %>%
  mutate(Time_Period_Start = lubridate::floor_date(Date , "6 months" )) %>%
  group_by(State, Time_Period_Start) %>%
  summarise(sum_deaths = sum(imputed_vals, na.rm = TRUE))

#restrict the dataset to be between 2000 and 2017
od_data_grouped_data <- od_data_grouped_data[year(od_data_grouped_data$Time_Period_Start) > 1999 &
                     year(od_data_grouped_data$Time_Period_Start) < 2020,]

#create a new dataset for the analysis using columns from the main analysis data
sensitivity_anlys_imputed_od_wo_interp_data <- main_analysis_data %>%
  ungroup() %>%
  mutate(imputed_deaths_no_interp = od_data_grouped_data$sum_deaths,
         num_alive_no_interp = population - imputed_deaths_no_interp) %>%
  dplyr::select(-c(imputed_deaths, num_alive)) #want to remove the outcome from main analysis to not get confused

#compute the number of states with intervention
sensitivity_anlys_imputed_od_wo_interp_data <- sensitivity_anlys_imputed_od_wo_interp_data %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_with_intervention = sum(Intervention_Redefined))

#run the model for analysis
sensitivity_anlys_imputed_od_wo_interp <- gam(cbind(round(imputed_deaths_no_interp),
                                                    round(num_alive_no_interp))~ State +
                                                s(Time_Period_ID, bs = "cr",
                                                  by = as.factor(Region)) +
                                                Naloxone_Pharmacy_Yes_Redefined +
                                                Naloxone_Pharmacy_No_Redefined +
                                                Medical_Marijuana_Redefined +
                                                Recreational_Marijuana_Redefined +
                                                GSL_Redefined +
                                                PDMP_Redefined +
                                                Medicaid_Expansion_Redefined +
                                                Intervention_Redefined + 
                                                num_states_w_intervention,
                                              data = sensitivity_anlys_imputed_od_wo_interp_data,
                                              family = "binomial")
```

```{r, results = "asis"}
stargazer(sensitivity_anlys_imputed_od_wo_interp, type = "html", dep.var.labels = "Unintentional Overdose Death")
# exp(coef(sensitivity_anlys_imputed_od_wo_interp)["Intervention_Redefined"]) 

```

## Compile Results
```{r}
########## Sensitivity Analysis 3: Make Data Frame of Results and 95% CI #############
#store the coefficients into the table
sensitivity_anlys_wo_interp_full_table<-data.frame(coef(sensitivity_anlys_imputed_od_wo_interp))
#check to see how the table looks
head(sensitivity_anlys_wo_interp_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_wo_interp_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined",
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_wo_interp_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_wo_interp_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_wo_interp_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_wo_interp_full_table)[i]<-substr(rownames(sensitivity_anlys_wo_interp_full_table)[i], start = 6,
                                                                       stop = nchar(rownames(sensitivity_anlys_wo_interp_full_table)[i]))

    }else if(rownames(sensitivity_anlys_wo_interp_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_wo_interp_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_wo_interp_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_wo_interp_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                      substr(rownames(sensitivity_anlys_wo_interp_full_table)[i], 
                                                                             start = 36,
                                                                             stop = nchar(rownames(sensitivity_anlys_wo_interp_full_table)[i])),
                                                                      sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_wo_interp_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_wo_interp_full_table$Coefficient_Estimate - 1.96*summary(sensitivity_anlys_imputed_od_wo_interp)$se
sensitivity_anlys_wo_interp_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_wo_interp_full_table$Coefficient_Estimate + 1.96*summary(sensitivity_anlys_imputed_od_wo_interp)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_wo_interp_full_table$Odds_Ratio<-exp(sensitivity_anlys_wo_interp_full_table$Coefficient_Estimate)
sensitivity_anlys_wo_interp_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_wo_interp_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_wo_interp_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_wo_interp_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_wo_interp_full_table$Standard_Error<-summary(sensitivity_anlys_imputed_od_wo_interp)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_wo_interp_full_table$p_value<-c(summary(sensitivity_anlys_imputed_od_wo_interp)$p.pv,
                                                       rep(NA, length(coef(sensitivity_anlys_imputed_od_wo_interp)) -
                                                             length(summary(sensitivity_anlys_imputed_od_wo_interp)$p.pv)))

head(sensitivity_anlys_wo_interp_full_table)
tail(sensitivity_anlys_wo_interp_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_wo_interp_full_table) %in% covariates)
sensitivity_anlys_wo_interp_covariate_table<-(round(sensitivity_anlys_wo_interp_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_wo_interp_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                         "Naloxone_Pharmacy_No",
                                                         "Medical_Marijuana",
                                                         "Recreational_Marijuana",
                                                         "GSL", 
                                                         "PDMP", 
                                                         "Medicaid_Expansion",
                                                         "Intervention_Redefined",
                                                         "Number of States with Intervention")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_wo_interp_covariate_table<-rbind(sensitivity_anlys_wo_interp_covariate_table, sensitivity_anlys_wo_interp_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_wo_interp_covariate_table<-sensitivity_anlys_wo_interp_covariate_table[,-which(colnames(sensitivity_anlys_wo_interp_covariate_table) %in%
                                                                                                             c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_wo_interp_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_wo_interp_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_wo_interp_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_wo_interp.csv")

```

## Attributable Deaths
```{r}
################ Sensitivity Analysis 3: Number of Attributable Deaths #############
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_wo_interp<-sensitivity_anlys_imputed_od_wo_interp_data[which(sensitivity_anlys_imputed_od_wo_interp_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_wo_interpolation<-expit(-coef(sensitivity_anlys_imputed_od_wo_interp)["Intervention_Redefined"]*attr_deaths_anlys_wo_interp$Intervention_Redefined
                                     + logit(attr_deaths_anlys_wo_interp$imputed_deaths_no_interp/attr_deaths_anlys_wo_interp$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_imputed_od_wo_interp) - 1.96*summary(sensitivity_anlys_imputed_od_wo_interp)$se
coef_ub<-coef(sensitivity_anlys_imputed_od_wo_interp) + 1.96*summary(sensitivity_anlys_imputed_od_wo_interp)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_wo_interp<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_wo_interp$Intervention_Redefined
                                        + logit(attr_deaths_anlys_wo_interp$imputed_deaths_no_interp/attr_deaths_anlys_wo_interp$population))

prob_od_no_int_UB_wo_interp<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_wo_interp$Intervention_Redefined
                                        + logit(attr_deaths_anlys_wo_interp$imputed_deaths_no_interp/attr_deaths_anlys_wo_interp$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_imputed_od_wo_interp_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_wo_interp$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_wo_interp$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_wo_interp$imputed_deaths_no_interp[time_point_index]
                          - prob_od_no_int_wo_interpolation[time_point_index]*attr_deaths_anlys_wo_interp$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_wo_interp$imputed_deaths_no_interp[time_point_index]
                             - prob_od_no_int_LB_wo_interp[time_point_index]*attr_deaths_anlys_wo_interp$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_wo_interp$imputed_deaths_no_interp[time_point_index]
                             - prob_od_no_int_UB_wo_interp[time_point_index]*attr_deaths_anlys_wo_interp$population[time_point_index])


  index<-index + 1
}

num_attr_od_wo_interpolation<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_wo_interp$Time_Period_ID)),
                                       "Time_Start" = sort(unique(attr_deaths_anlys_wo_interp$Time_Period_Start)),
                                       "Num_Attr_Deaths" = num_attr_od,
                                       "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                       "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_wo_interpolation$Num_Attr_Deaths)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_wo_interpolation<-num_attr_od_wo_interpolation %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_wo_interpolation$deaths)





```

# Sensitivity Analysis 4: Effect of DIH Prosecutions on Unintentional Overdose Deaths, Assuming Effect Lasts for Two Years
## Analysis
```{r, results = "asis"}
########### Sensitivity Analysis 4: Two Year Intervention Effect ######################
#create a plot for each state to see how many prosecution media alerts there are per 6 month period
#read in the prosecution media alert data
prosecution_data<-read.csv("./Data/dih_prosecutions_9_6_21.csv")

#data cleaning
prosecution_data<-prosecution_data %>% 
  mutate(Date = as.Date(Date.charged, "%m/%d/%Y")) %>%
  mutate(State = ifelse(State.Filed == "pennsylvania", "Pennsylvania", State.Filed),
         State = ifelse(State.Filed == "Virginia ", "Virginia", State)) %>%
  filter(!is.na(Date), State.Filed != "No Info", State.Filed != "No info", State.Filed != "No Info ",
         State != "")

#clean up the data by looking at the link to the article
prosecution_data$Date[prosecution_data$Date == "2026-08-01"] <- as.Date("2016-02-15", "%Y-%m-%d")

#change the states into Character instead of factor
prosecution_data$State<-as.character(prosecution_data$State)
#see how many prosecution data points there are for each state
table(prosecution_data$State)

#there are some repeated cases depending on victim
prosecution_data_unique <- prosecution_data %>%
  group_by(State) %>%
  distinct(Accused.Name, Date, .keep_all = T)
table(prosecution_data_unique$State)

#change date charged into Date object
prosecution_data_unique$Date<-mdy(prosecution_data_unique$Date.charged)

#group the data into six month periods
prosecution_data_unique<-prosecution_data_unique %>% 
  mutate(six_month_pd = lubridate::floor_date(Date , "6 months" ))

#######ONLY IF GROUPS######
prosecution_grouped <- prosecution_data_unique %>% 
  #filter to dates after 2000 and dates before 2020
  filter(year(six_month_pd) >= 2000 & year(six_month_pd) <= 2019) %>%
  group_by(State, six_month_pd) %>% 
  #for each state, for each six month period, count the number of DIH prosecutions
  summarise(num_dih = n()) %>% 
  #label the groups according to zero, low, or high
  mutate(group = ifelse(num_dih == 0, "zero", ifelse(num_dih >= 5, "high", "low"))) %>%
  ungroup() %>%
  #have to add in a row for hawaii because its not in the prosecution dataset
  add_row(State = "Hawaii", six_month_pd = as.Date("2000-01-01"), num_dih = 0, group = "zero")

#we compute the final group for each state by seeing if it ever hits high or low
prosecution_grouped_final <- prosecution_grouped %>%  
  group_by(State) %>% 
  summarise(final_gp = ifelse(sum(group == "high") > 0, "high", ifelse(sum(group == "low")> 0, "low", "zero"))) 

ggplot(prosecution_grouped_final, aes(final_gp)) + 
  geom_bar() + 
  labs(title = "Number of States by DIH prosecution Category, with Low = [1,5]") + 
  geom_text(aes(label = ..count..), stat = "count", vjust = -.75)

#number of DIH prosecutions per six month for each state
# pdf("Figures/num_dih_per_six_month_pd_by_state_11_12_21.pdf")
ggplot(prosecution_grouped, aes(x = six_month_pd, y = num_dih)) + 
  geom_bar(stat = "identity") + 
  facet_wrap(~State) + 
  theme(axis.text.x = element_text(angle = 30, size = 5))
# dev.off()

# write.csv(prosecution_grouped, "./Data/num_dih_per_six_month_pd_by_state_11_12_21.csv")

#count the number of prosecution media alerts in each six month period
#we also get the first and last date of prosecution in time period
prosecution_data_by_six_month_pd <- prosecution_data %>%
  filter(year(six_month_pd)>1999 & year(six_month_pd)<2020) %>%
  group_by(State, six_month_pd) %>%
  summarise(first_date_in_pd = min(Date), last_date_in_pd = max(Date))

#create the data set used for this sensitivity analysis
#first, we merge the grouped prosecution data set with the main data set by state and time period
sensitivity_anlys_redefine_int_data<-merge(main_analysis_data,
                                           prosecution_data_by_six_month_pd, 
                                           by.x = c("State", "Time_Period_Start"),
                                           by.y = c("State", "six_month_pd"), all = TRUE)

#create a intervention 2 year effect variable by initializing it to be all 0
sensitivity_anlys_redefine_int_data<-sensitivity_anlys_redefine_int_data %>% 
  group_by(State) %>%
  mutate(int_2_yr_effect = 0)

#change the date into a date object
sensitivity_anlys_redefine_int_data$Time_Period_Start<-as.Date(sensitivity_anlys_redefine_int_data$Time_Period_Start)
sensitivity_anlys_redefine_int_data$Time_Period_End<-as.Date(sensitivity_anlys_redefine_int_data$Time_Period_End)

#we need to impute the newly defined intervention variable depending on the case
#by examining each row of the data set
for(state in unique(sensitivity_anlys_redefine_int_data$State)){
  #first, subset the data set into state_data which only contains the data for the state
  state_index<-which(sensitivity_anlys_redefine_int_data$State == state)
  state_data<-sensitivity_anlys_redefine_int_data[state_index,]

  #note that the first four rows of the 2 year effect intervention variable are the same as the
  #first four rows of the original intervention variable
  state_data$int_2_yr_effect[1:4]<-state_data$Intervention_Redefined[1:4]

  for(i in 5:nrow(state_data)){
    #next, we deal with the rows where there was at least one prosecution in the last 3 six month periods
    #These rows will be imputed with a 1
    if((!is.na(state_data$first_date_in_pd[i - 1]) |
        !is.na(state_data$first_date_in_pd[i - 2]) |
        !is.na(state_data$first_date_in_pd[i - 3]))){

      state_data$int_2_yr_effect[i]<-1

    }else{
      #next, we account for the rows with the fractions:
      # 1) an intervention occurs in row i without an intervention 2 years ago
      # 2) row i contains the lasting effects of an intervention that occurred 2 years ago
      # 3) row i contains effects from both a new intervention starting in row i and lasting
      # effects from 2 years ago

      #To compute the fraction, we add the number of days that are affected by an intervention
      #(from both the current prosecution and previous prosecution) and then divide by the total
      #number of days in the period:

      total_len_of_pd<-as.numeric(state_data$Time_Period_End[i] - state_data$Time_Period_Start[i])

      #If there is no prosecution two years ago, i.e. in period i-4, then the last_date is the first
      #date in period i. We subtract the last_date by the first date in the period, so we will get
      #a 0 for the number of days that are affected by a prosecution from period i-4. Otherwise,
      #the last_date is the last date of prosecution from period i-4, plus 2 years.
      len_of_past_effect <- ifelse(!is.na(state_data$first_date_in_pd[i - 4]),
                                   (state_data$last_date_in_pd[i - 4] + years(2)) - state_data$Time_Period_Start[i],
                                   0)

      #If there is no prosecution in the period i, then the start_date is the last date in the period i.
      #We subtract start_date from the last date in period i, so we will get a 0 for the number
      #of days that are affected by a prosecution in period i. Otherwise, the start_date is the
      #first date of a prosecution in period i.
      len_of_current_effect <- ifelse(!is.na(state_data$first_date_in_pd[i]),
                                      as.numeric(state_data$Time_Period_End[i] - state_data$first_date_in_pd[i]),
                                      0)

      state_data$int_2_yr_effect[i]<-(len_of_past_effect + len_of_current_effect)/total_len_of_pd
    }
  }

  #for the case where the int_2_yr_effect is greater than 1 (could result when we add the effects of
  #previous intervention and the current intervention), we just impute a 1 instead
  state_data$int_2_yr_effect[state_data$int_2_yr_effect>1]<-1

  #lastly, we store the int_2_yr_effect variable into the sensitivity analysis data set
  sensitivity_anlys_redefine_int_data$int_2_yr_effect[state_index]<-state_data$int_2_yr_effect
}

#view the data set just to make sure the imputation looks right
# View(sensitivity_anlys_redefine_int_data %>% select(State, Time_Period_Start, Time_Period_End,
#                                                     Intervention_Redefined, first_date_in_pd,
#                                                     last_date_in_pd,
#                                                     int_2_yr_effect))


sensitivity_anlys_redefine_int_data <- sensitivity_anlys_redefine_int_data %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention_2_yr_effect = sum(int_2_yr_effect))

#run the analysis on the sensitivity analysis data
sensitivity_anlys_redefine_int_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                            s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                            Naloxone_Pharmacy_Yes_Redefined +
                                            Naloxone_Pharmacy_No_Redefined +
                                            Medical_Marijuana_Redefined +
                                            Recreational_Marijuana_Redefined +
                                            GSL_Redefined +
                                            PDMP_Redefined +
                                            Medicaid_Expansion_Redefined +
                                            int_2_yr_effect + 
                                            num_states_w_intervention_2_yr_effect,
                                          data = sensitivity_anlys_redefine_int_data, family = "binomial")

stargazer(sensitivity_anlys_redefine_int_model, type = "html", dep.var.labels = "Unintentional Overdose Death")
```

## Plots
```{r}
plot(sensitivity_anlys_redefine_int_model)

```

## Compile Results
```{r}
############## Sensitivity Analysis 4: Make Data Frame of Results and 95% CI ##########
#store the coefficients into the table
sensitivity_anlys_redefine_int_full_table<-data.frame(coef(sensitivity_anlys_redefine_int_model))
#check to see how the table looks
head(sensitivity_anlys_redefine_int_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_redefine_int_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "int_2_yr_effect", 
              "num_states_w_intervention_2_yr_effect")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_redefine_int_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_redefine_int_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 6,
                                                                     stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i]))

    }else if(rownames(sensitivity_anlys_redefine_int_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                    substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 36,
                                                                           stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i])),
                                                                    sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_redefine_int_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate - 1.96*summary(sensitivity_anlys_redefine_int_model)$se
sensitivity_anlys_redefine_int_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate + 1.96*summary(sensitivity_anlys_redefine_int_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_redefine_int_full_table$Odds_Ratio<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate)
sensitivity_anlys_redefine_int_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_redefine_int_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_redefine_int_full_table$Standard_Error<-summary(sensitivity_anlys_redefine_int_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_redefine_int_full_table$p_value<-c(summary(sensitivity_anlys_redefine_int_model)$p.pv,
                                                     rep(NA, length(coef(sensitivity_anlys_redefine_int_model)) -
                                                           length(summary(sensitivity_anlys_redefine_int_model)$p.pv)))

head(sensitivity_anlys_redefine_int_full_table)
tail(sensitivity_anlys_redefine_int_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_redefine_int_full_table) %in% covariates)
sensitivity_anlys_redefine_int_covariate_table<-(round(sensitivity_anlys_redefine_int_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_redefine_int_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                            "Naloxone_Pharmacy_No",
                                                            "Medical_Marijuana",
                                                            "Recreational_Marijuana",
                                                            "GSL", 
                                                            "PDMP", 
                                                            "Medicaid_Expansion",
                                                            "Two Year Intervention Effect", 
                                                            "Number of States w Intervention")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_redefine_int_covariate_table<-rbind(sensitivity_anlys_redefine_int_covariate_table, sensitivity_anlys_redefine_int_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_redefine_int_covariate_table<-sensitivity_anlys_redefine_int_covariate_table[,-which(colnames(sensitivity_anlys_redefine_int_covariate_table) %in%
                                                                                                         c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_redefine_int_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_redefine_int_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_redefine_int_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_redefine_int.csv")

```

## Attributable Deaths
```{r}
################ Sensitivity Analysis 4: Number of Attributable Deaths ################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_redefine_int<-sensitivity_anlys_redefine_int_data[which(sensitivity_anlys_redefine_int_data$int_2_yr_effect>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_redefine_int<-expit(-coef(sensitivity_anlys_redefine_int_model)["int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                   + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_redefine_int_model) - 1.96*summary(sensitivity_anlys_redefine_int_model)$se
coef_ub<-coef(sensitivity_anlys_redefine_int_model) + 1.96*summary(sensitivity_anlys_redefine_int_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_redefine_int<-expit(-coef_lb[names(coef_lb) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

prob_od_no_int_UB_redefine_int<-expit(-coef_ub[names(coef_ub) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_redefine_int_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_redefine_int$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                          - prob_od_no_int_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])


  index<-index + 1
}

num_attr_od_redefine_int<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID)),
                                     "Time_Start" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_Start)),
                                     "Num_Attr_Deaths" = num_attr_od,
                                     "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                     "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_redefine_int$Num_Attr_Deaths)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_redefine_int<-num_attr_od_redefine_int %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths),
            death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_redefine_int$deaths)

ggplot(yearly_num_Attr_Deaths_redefine_int, aes(x = year, y = deaths)) + geom_line() + geom_point()

```

# Sensitivity Analysis 5: Effect of Lagged DIH Prosecutions on Unintentional Overdose Deaths, Assuming Effect Lasts for Two Years
## Analysis

```{r, results = "asis"}
############# Sensitivity Analysis 5: Two Year Effect Lagged by 6 months ######################
#create a plot for each state to see how many prosecution media alerts there are per 6 month period
#read in the prosecution media alert data
prosecution_data<-read.csv("./Data/dih_prosecutions_9_6_21.csv")

#data cleaning
#data cleaning
prosecution_data<-prosecution_data %>% 
  mutate(Date = as.Date(Date.charged, "%m/%d/%Y")) %>%
  mutate(State = ifelse(State.Filed == "pennsylvania", "Pennsylvania", State.Filed),
         State = ifelse(State.Filed == "Virginia ", "Virginia", State)) %>%
  filter(!is.na(Date), State.Filed != "No Info", State.Filed != "No info", State.Filed != "No Info ",
         State != "")

#clean up the data by looking at the link to the article
prosecution_data$Date[prosecution_data$Date == "2026-08-01"] <- as.Date("2016-02-15", "%Y-%m-%d")

#change the states into Character instead of factor
prosecution_data$State<-as.character(prosecution_data$State)
#see how many prosecution data points there are for each state
table(prosecution_data$State)

#change date charged into Date object
prosecution_data$Date<-mdy(prosecution_data$Date.charged)

#group the data into six month periods
prosecution_data<-prosecution_data %>% mutate(six_month_pd = lubridate::floor_date(Date , "6 months" ))

#count the number of prosecution media alerts in each six month period
#we also get the first and last date of prosecution in time period
prosecution_data_by_six_month_pd <- prosecution_data %>%
  filter(year(six_month_pd)>1999 & year(six_month_pd)<2020) %>%
  group_by(State, six_month_pd) %>%
  summarise(first_date_in_pd = min(Date), last_date_in_pd = max(Date))

#create the data set used for this sensitivity analysis
#first, we merge the grouped prosecution data set with the main data set by state and time period
sensitivity_anlys_2yr_int_lag<-merge(main_analysis_data,
                                     prosecution_data_by_six_month_pd,
                                     by.x = c("State", "Time_Period_Start"),
                                     by.y = c("State", "six_month_pd"), all = TRUE)

#create a intervention 2 year effect variable by initializing it to be all 0
sensitivity_anlys_2yr_int_lag<-sensitivity_anlys_2yr_int_lag %>% 
  group_by(State) %>%
  mutate(int_2_yr_effect_lag = 0)

#change the date into a date object
sensitivity_anlys_2yr_int_lag$Time_Period_Start<-as.Date(sensitivity_anlys_2yr_int_lag$Time_Period_Start)
sensitivity_anlys_2yr_int_lag$Time_Period_End<-as.Date(sensitivity_anlys_2yr_int_lag$Time_Period_End)

#we need to calculate the 2 year intervention variable depending on the case
#by examining each row of the data set
for(state in unique(sensitivity_anlys_2yr_int_lag$State)){
  #first, subset the data set into state_data which only contains the data for the state
  state_index<-which(sensitivity_anlys_2yr_int_lag$State == state)
  state_data<-sensitivity_anlys_2yr_int_lag[state_index,]

  for(i in 1:(nrow(state_data)-1)){
    #for the states that had at least one prosecution in the first 2 years of analysis period,
    #we impute the intervention variable based on the intervention variable of main analysis:
    #if intervention occurred in time i, then for the 6-month lagged effect, we compute the
    #proportion of days affected by intervention, taking into account the 6 month lag. Else,
    #if the intervention had occurred by time i, we impute a 1 in the next six-month interval
    #since lagged
    if(i %in% c(1:4)){
      if(state_data$Intervention_Redefined[i] > 0 & state_data$Intervention_Redefined[i] < 1){
        state_data$int_2_yr_effect_lag[i + 1] <- as.numeric(state_data$Time_Period_End[i + 1] - (state_data$first_date_in_pd[i] %m+% months(6)))/as.numeric(state_data$Time_Period_End[i + 1] - state_data$Time_Period_Start[i + 1])
      }else if(state_data$Intervention_Redefined[i] == 1){
        state_data$int_2_yr_effect_lag[i + 1] <- 1
      }

      #next, if there was at least one prosecution in the last 3 six-month periods (i.e. 1.5 years) before time i
      #These rows will be imputed with a 1 in the next six-month interval since lagged
    }else if((!is.na(state_data$first_date_in_pd[i - 1]) |
              !is.na(state_data$first_date_in_pd[i - 2]) |
              !is.na(state_data$first_date_in_pd[i - 3]))){

      state_data$int_2_yr_effect_lag[i + 1]<-1

    }else{
      #next, we account for the rows with the fractions:
      # 1) an intervention occurs in row i without an intervention 2 years ago
      # 2) row i contains the lasting effects of an intervention that occurred 2 years ago
      # 3) row i contains effects from both a new intervention starting in row i and lasting
      # effects from 2 years ago

      #To compute the fraction, we add the number of days that are affected by an intervention
      #(from both the current prosecution and previous prosecution) and then divide by the total
      #number of days in the period:

      #first, we compute the number of days in the period of time interval i + 1
      total_len_of_pd<-as.numeric(state_data$Time_Period_End[i + 1] - state_data$Time_Period_Start[i + 1])

      #If there is no prosecution two years ago, i.e. in period i-4, then the last_date is the first
      #date in period i + 1. We subtract the last_date by the first date in period i + 1, so we will get
      #a 0 for the number of days that are affected by a prosecution from period i-4. Otherwise,
      #the last_date is the last date of prosecution from period i-4, plus 2 years.
      #The start time is the first date of period i + 1

      len_of_past_effect <- ifelse(!is.na(state_data$first_date_in_pd[i - 4]),
                                   as.numeric((as.Date(state_data$last_date_in_pd[i - 4] + years(2), format = "%Y-%m-%d") %m+% months(6)) - state_data$Time_Period_Start[i + 1]),
                                   0)

      #If there is no prosecution in the period i, then the start_date is the last date in period i + 1 (because lagged effect).
      #We subtract start_date from the last date in period i + 1, so we will get a 0 for the number
      #of days that are affected by a prosecution in period i. Otherwise, the start_date is the
      #first date of a prosecution in period i + 6 months. The end date is the last date in period i + 1.

      len_of_current_effect <- ifelse(!is.na(state_data$first_date_in_pd[i]),
                                      as.numeric(state_data$Time_Period_End[i + 1] - (state_data$first_date_in_pd[i] %m+% months(6))),
                                      0)

      state_data$int_2_yr_effect_lag[i + 1]<-(len_of_past_effect + len_of_current_effect)/total_len_of_pd
    }
  }

  #for the case where the int_2_yr_effect is greater than 1 (could result when we add the effects of
  #previous intervention and the current intervention), we just impute a 1 instead
  state_data$int_2_yr_effect_lag[state_data$int_2_yr_effect_lag>1]<-1

  #lastly, we store the int_2_yr_effect variable into the sensitivity analysis data set
  sensitivity_anlys_2yr_int_lag$int_2_yr_effect_lag[state_index]<-state_data$int_2_yr_effect_lag
}

sensitivity_anlys_2yr_int_lag <- sensitivity_anlys_2yr_int_lag %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention_2yr_lag = sum(int_2_yr_effect_lag))
#view the data set just to make sure the imputation looks right
# View(sensitivity_anlys_2yr_int_lag %>% select(State, Time_Period_Start, Time_Period_End,
#                                                     Intervention_Redefined, first_date_in_pd,
#                                                     last_date_in_pd,
#                                                     int_2_yr_effect_lag))


#run the analysis for all the states
lagged_analysis_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                             s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                             Naloxone_Pharmacy_Yes_Redefined +
                             Naloxone_Pharmacy_No_Redefined +
                             Medical_Marijuana_Redefined +
                             Recreational_Marijuana_Redefined +
                             GSL_Redefined +
                             PDMP_Redefined +
                             Medicaid_Expansion_Redefined +
                             int_2_yr_effect_lag + 
                             num_states_w_intervention_2yr_lag,
                           data = sensitivity_anlys_2yr_int_lag, family = "binomial")

#summary output of the model
stargazer(lagged_analysis_model, type = "html", dep.var.labels = "Unintentional Overdose Death")
```

## Plots
```{r}
gam.check(lagged_analysis_model)
```

## Compile Results
```{r}
############## Sensitivity Analysis 5: Make Data Frame of Results and 95% CI ###########
#store the coefficients into the table
sensitivity_anlys_2yr_int_lag_full_table<-data.frame(coef(lagged_analysis_model))
#check to see how the table looks
head(sensitivity_anlys_2yr_int_lag_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_2yr_int_lag_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "int_2_yr_effect_lag", 
              "num_states_w_intervention_2yr_lag")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_2yr_int_lag_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_2yr_int_lag_full_table)[i]<-substr(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i], start = 6,
                                                                    stop = nchar(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i]))

    }else if(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_2yr_int_lag_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_2yr_int_lag_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                   substr(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i], start = 36,
                                                                          stop = nchar(rownames(sensitivity_anlys_2yr_int_lag_full_table)[i])),
                                                                   sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_2yr_int_lag_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_2yr_int_lag_full_table$Coefficient_Estimate - 1.96*summary(lagged_analysis_model)$se
sensitivity_anlys_2yr_int_lag_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_2yr_int_lag_full_table$Coefficient_Estimate + 1.96*summary(lagged_analysis_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_2yr_int_lag_full_table$Odds_Ratio<-exp(sensitivity_anlys_2yr_int_lag_full_table$Coefficient_Estimate)
sensitivity_anlys_2yr_int_lag_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_2yr_int_lag_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_2yr_int_lag_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_2yr_int_lag_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_2yr_int_lag_full_table$Standard_Error<-summary(lagged_analysis_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_2yr_int_lag_full_table$p_value<-c(summary(lagged_analysis_model)$p.pv, rep(NA, length(coef(lagged_analysis_model)) - length(summary(lagged_analysis_model)$p.pv)))

head(sensitivity_anlys_2yr_int_lag_full_table)
tail(sensitivity_anlys_2yr_int_lag_full_table)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_2yr_int_lag_full_table,5), "./Data/coefficients_GAM_9_6_21_lagged_2yr_int.csv")

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_2yr_int_lag_full_table) %in% covariates)
sens_analysis_2yr_int_lag_covariate_table<-(round(sensitivity_anlys_2yr_int_lag_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sens_analysis_2yr_int_lag_covariate_table)<-c("Naloxone_Pharmacy_Yes",
                                                       "Naloxone_Pharmacy_No",
                                                       "Medical_Marijuana",
                                                       "Recreational_Marijuana",
                                                       "GSL", 
                                                       "PDMP", 
                                                       "Medicaid_Expansion",
                                                       "Intervention", 
                                                       "Number of States w Intervention")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sens_analysis_2yr_int_lag_covariate_table<-rbind(sens_analysis_2yr_int_lag_covariate_table, sensitivity_anlys_2yr_int_lag_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sens_analysis_2yr_int_lag_covariate_table<-sens_analysis_2yr_int_lag_covariate_table[,-which(colnames(sens_analysis_2yr_int_lag_covariate_table) %in%
                                                                                               c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sens_analysis_2yr_int_lag_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sens_analysis_2yr_int_lag_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sens_analysis_2yr_int_lag_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_2_yr_int_lag.csv")

```

## Attributable Deaths
```{r}
################# Sensitivity Analysis 5: Number of Attributable Deaths #################
#find the number of deaths attributable to the intervention
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_2yr_int_lag<-sensitivity_anlys_2yr_int_lag[which(sensitivity_anlys_2yr_int_lag$int_2_yr_effect_lag>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_2yr_int_lag<-expit(-coef(lagged_analysis_model)["int_2_yr_effect_lag"]
                                  + logit(attr_deaths_anlys_2yr_int_lag$imputed_deaths/attr_deaths_anlys_2yr_int_lag$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(lagged_analysis_model) - 1.96*summary(lagged_analysis_model)$se
coef_ub<-coef(lagged_analysis_model) + 1.96*summary(lagged_analysis_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_2yr_int_lag<-expit(-coef_lb[names(coef_lb) == "int_2_yr_effect_lag"]
                                     + logit(attr_deaths_anlys_2yr_int_lag$imputed_deaths/attr_deaths_anlys_2yr_int_lag$population))

prob_od_no_int_UB_2yr_int_lag<-expit(-coef_ub[names(coef_ub) == "int_2_yr_effect_lag"]
                                     + logit(attr_deaths_anlys_2yr_int_lag$imputed_deaths/attr_deaths_anlys_2yr_int_lag$population))


#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(attr_deaths_anlys_2yr_int_lag$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_2yr_int_lag$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_2yr_int_lag$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_2yr_int_lag$imputed_deaths[time_point_index]
                          - prob_od_no_int_2yr_int_lag[time_point_index]*attr_deaths_anlys_2yr_int_lag$population[time_point_index])
  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_2yr_int_lag$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_2yr_int_lag[time_point_index]*attr_deaths_anlys_2yr_int_lag$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_2yr_int_lag$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_2yr_int_lag[time_point_index]*attr_deaths_anlys_2yr_int_lag$population[time_point_index])
  index<-index + 1
}

num_attr_od_2yr_int_lag<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_2yr_int_lag$Time_Period_ID)),
                                    "Time_Start" = sort(unique(attr_deaths_anlys_2yr_int_lag$Time_Period_Start)),
                                    "Num_Attr_Deaths" = num_attr_od,
                                    "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                    "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_2yr_int_lag$Num_Attr_Deaths)
summary(num_attr_od_2yr_int_lag$Num_Attr_Deaths)
num_attr_od_2yr_int_lag$Time_Start<-as.Date(num_attr_od_2yr_int_lag$Time_Start)

#compute the 95% CI for the total
sum(num_attr_od_2yr_int_lag$Num_Attr_Deaths_LB)
sum(num_attr_od_2yr_int_lag$Num_Attr_Deaths_UB)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_2yr_int_lag<-num_attr_od_2yr_int_lag %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_2yr_int_lag$deaths)
summary(yearly_num_Attr_Deaths_2yr_int_lag$death_lb)
summary(yearly_num_Attr_Deaths_2yr_int_lag$death_ub)

ggplot(yearly_num_Attr_Deaths_2yr_int_lag, aes(x = year, y = deaths)) + geom_line() + geom_point()



```

## Compile Attributable Deaths Results
```{r, fig.width=8, fig.height= 8}
########################## Compiled Attributable Deaths Plot###############################
#add color column to the datasets
yearly_num_Attr_Deaths_main_analysis <- yearly_num_Attr_Deaths_main_analysis %>% 
  mutate(gp_type = "a")
yearly_num_Attr_Deaths_exclude_states <- yearly_num_Attr_Deaths_exclude_states %>%
  mutate(gp_type = "c")
yearly_num_Attr_Deaths_od_all <- yearly_num_Attr_Deaths_od_all %>%
  mutate(gp_type = "b")
yearly_num_Attr_Deaths_redefine_int <- yearly_num_Attr_Deaths_redefine_int %>%
  mutate(gp_type = "d")
pdf("Figures/num_attr_deaths_yearly_for_all_anlys_9_6_21_all_od_shape_alpha.pdf")
ggplot(yearly_num_Attr_Deaths_main_analysis) +
  geom_line(aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1, color = gp_type,
                linetype = "Estimate")) +
  geom_point(aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1, color = gp_type, shape = gp_type))  +
  geom_line(yearly_num_Attr_Deaths_main_analysis,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
                          color = gp_type, linetype = "95% Confidence Interval")) +
  geom_line(yearly_num_Attr_Deaths_main_analysis,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
                          color =gp_type,linetype = "95% Confidence Interval")) +
  # geom_point(yearly_num_Attr_Deaths_main_analysis,
  #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
  #                         color = "a")) +
  # geom_point(yearly_num_Attr_Deaths_main_analysis,
  #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
  #                         color ="a")) +
  geom_line(yearly_num_Attr_Deaths_redefine_int,
            mapping = aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1,
                          color = gp_type, linetype = "Estimate")) +
  geom_point(yearly_num_Attr_Deaths_redefine_int,
             mapping = aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1,
                           color = gp_type, shape = gp_type))  +
  geom_line(yearly_num_Attr_Deaths_redefine_int,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
                          color = gp_type, linetype = "95% Confidence Interval")) +
  geom_line(yearly_num_Attr_Deaths_redefine_int,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
                          color = gp_type, linetype = "95% Confidence Interval")) +
  # geom_point(yearly_num_Attr_Deaths_redefine_int,
  #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
  #                         color = "d")) +
  # geom_point(yearly_num_Attr_Deaths_redefine_int,
  #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
  #                         color = "d")) +
  geom_line(yearly_num_Attr_Deaths_od_all, mapping = aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1,
                                                         color = gp_type, linetype = "Estimate"), alpha = 1) +
  geom_point(yearly_num_Attr_Deaths_od_all, mapping = aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1,
                                                          color = gp_type, shape = gp_type), alpha = 1)  +
  geom_line(yearly_num_Attr_Deaths_od_all,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
                          color = gp_type, linetype = "95% Confidence Interval"), alpha = 1) +
  geom_line(yearly_num_Attr_Deaths_od_all,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
                          color = gp_type, linetype = "95% Confidence Interval"),  alpha = 1) +
  # geom_point(yearly_num_Attr_Deaths_od_all,
  #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
  #                         color = "b"), alpha = 0.5) +
  # geom_point(yearly_num_Attr_Deaths_od_all,
  #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
  #                         color = "b") , alpha = 0.5) +
  geom_line(yearly_num_Attr_Deaths_exclude_states,
            mapping = aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1,
                          color = gp_type, linetype = "Estimate"), alpha = 1) +
  geom_point(yearly_num_Attr_Deaths_exclude_states, mapping = aes(x = as.Date(as.yearmon(year)), y = deaths, group = 1,
                                                                  color = gp_type, shape = gp_type), alpha = 1)  +
  geom_line(yearly_num_Attr_Deaths_exclude_states,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
                          color = gp_type, linetype = "95% Confidence Interval"),alpha = 1) +
  geom_line(yearly_num_Attr_Deaths_exclude_states,
            mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
                          color = gp_type, linetype = "95% Confidence Interval"), alpha = 1) +
    # geom_point(yearly_num_Attr_Deaths_exclude_states,
    #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_ub, group = 1,
    #                         color = "c"),alpha = 0.5) +
    # geom_point(yearly_num_Attr_Deaths_exclude_states,
    #           mapping = aes(x = as.Date(as.yearmon(year)), y = death_lb, group = 1,
    #                         color = "c"), alpha = 0.5) +
 theme(axis.text.y=element_text(size=10, family = "Times"),
        axis.title=element_text(size=10,face="bold", family = "Times"),
        panel.border = element_blank(), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),
        axis.text.x = element_text(size = 10, family = "Times"),
        panel.background = element_rect("white"),
        legend.text=element_text(size=9, family = "Times"),
        legend.position = c(.4,.85),
       # legend.position = "bottom",
        legend.box="vertical", legend.margin=margin()) +
  guides(color=guide_legend(nrow=2,byrow=TRUE)) +
  labs(x = "Year", y = "Yearly Drug Overdose Deaths Attributable to DIH Prosecutions Reported in Media", color = "",
       linetype = "", shape = "") +
  scale_color_manual(values=c('black', 'red', 'green', 'deepskyblue'),
                     labels = c("Main Analysis: Unintentional Drug Overdose Deaths", "All Drug Overdose Deaths",
                                "Excluding States with At Least 75% Missing Monthly", "2 Year Effect")) +
  scale_x_date(date_labels="%Y", breaks = seq(as.Date("2000-01-01"), as.Date("2018-01-01"), by = "2 years")) +
  scale_linetype_manual(values = c("Estimate" = "solid", "95% Confidence Interval" = "dashed"),
                        breaks = c("Estimate", "95% Confidence Interval")) +
  guides(linetype = guide_legend(nrow = 1)) + 
  scale_shape_manual(values = c(16, 4, 5, 2), 
                     labels = c("Main Analysis: Unintentional Drug Overdose Deaths", "All Drug Overdose Deaths",
                                "Excluding States with At Least 75% Missing Monthly", "2 Year Effect"))

dev.off()
```


# Sensitivity Analysis 6: Event Study
## Analysis

```{r}
sensitivity_anlys_event_study_data <- main_analysis_data %>%
  mutate(month = month(Time_Period_Start), 
         year = year(Time_Period_Start)) %>%
  group_by(State, month) %>%
  mutate(int_period = ifelse(year == year(Intervention_First_Date), 1, 0),
         neg_4_pd = ifelse(year <= year(Intervention_First_Date) - 4, 1, 0),
         neg_3_pd = lead(int_period, 3),
         neg_2_pd = lead(int_period, 2),
         neg_1_pd = lead(int_period, 1),
         pos_1_pd = lag(int_period, 1),
         pos_2_pd = lag(int_period, 2),
         pos_3_pd = lag(int_period, 3),
         pos_4_pd = ifelse(year >= year(Intervention_First_Date) + 4, 1, 0)) %>%
  mutate(neg_4_pd = ifelse(is.na(neg_4_pd), 0, neg_4_pd),
         neg_3_pd = ifelse(is.na(neg_3_pd), 0, neg_3_pd),
         neg_2_pd = ifelse(is.na(neg_2_pd), 0, neg_2_pd),
         pos_1_pd = ifelse(is.na(pos_1_pd), 0, pos_1_pd),
         pos_2_pd = ifelse(is.na(pos_2_pd), 0, pos_2_pd),
         pos_3_pd = ifelse(is.na(pos_3_pd), 0, pos_3_pd),
         pos_4_pd = ifelse(is.na(pos_4_pd), 0, pos_4_pd))


sensitivity_anlys_event_study_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                           s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                           Naloxone_Pharmacy_Yes_Redefined +
                                           Naloxone_Pharmacy_No_Redefined +
                                           Medical_Marijuana_Redefined +
                                           Recreational_Marijuana_Redefined +
                                           GSL_Redefined +
                                           PDMP_Redefined +
                                           Medicaid_Expansion_Redefined +
                                           neg_1_pd +
                                           neg_4_pd + 
                                           neg_3_pd + 
                                           neg_2_pd + 
                                           pos_1_pd + 
                                           pos_2_pd + 
                                           pos_3_pd + 
                                           pos_4_pd + 
                                           num_states_w_intervention,
                                         data = sensitivity_anlys_event_study_data, family = "binomial")

```

```{r, results = "asis"}
stargazer(sensitivity_anlys_event_study_model, type = "html", dep.var.labels = "Unintentional Overdose Death")

```

## Compile Results
```{r}

############################## Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
sensitivity_anlys_event_study_full_table<-data.frame(coef(sensitivity_anlys_event_study_model))
#check to see how the table looks
head(sensitivity_anlys_event_study_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_event_study_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined",
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_event_study_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_event_study_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_event_study_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_event_study_full_table)[i]<-substr(rownames(sensitivity_anlys_event_study_full_table)[i], 
                                                                       start = 6,
                                                                       stop = nchar(rownames(sensitivity_anlys_event_study_full_table)[i]))

    }else if(rownames(sensitivity_anlys_event_study_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_event_study_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_event_study_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_event_study_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                      substr(rownames(sensitivity_anlys_event_study_full_table)[i], 
                                                                             start = 36,
                                                                             stop = nchar(rownames(sensitivity_anlys_event_study_full_table)[i])),
                                                                      sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_event_study_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_event_study_full_table$Coefficient_Estimate -
  1.96*summary(sensitivity_anlys_event_study_model)$se
sensitivity_anlys_event_study_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_event_study_full_table$Coefficient_Estimate +
  1.96*summary(sensitivity_anlys_event_study_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_event_study_full_table$Odds_Ratio<-exp(sensitivity_anlys_event_study_full_table$Coefficient_Estimate)
sensitivity_anlys_event_study_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_event_study_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_event_study_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_event_study_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_event_study_full_table$Standard_Error<-summary(sensitivity_anlys_event_study_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_event_study_full_table$p_value<-c(summary(sensitivity_anlys_event_study_model)$p.pv,
                                                       rep(NA, length(coef(sensitivity_anlys_event_study_model)) -
                                                             length(summary(sensitivity_anlys_event_study_model)$p.pv)))

head(sensitivity_anlys_event_study_full_table)
tail(sensitivity_anlys_event_study_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_event_study_full_table) %in% covariates)
sensitivity_anlys_event_study_covariate_table<-(round(sensitivity_anlys_event_study_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_event_study_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                              "Naloxone_Pharmacy_No",
                                                              "Medical_Marijuana",
                                                              "Recreational_Marijuana",
                                                              "GSL", 
                                                              "PDMP", 
                                                              "Medicaid_Expansion",
                                                              "Intervention_Redefined",
                                                              "Number of States w DIH Prosecution")

#now, reorganize the data so that the covariates are on top and the rest of the variables are below
sensitivity_anlys_event_study_covariate_table<-rbind(sensitivity_anlys_event_study_covariate_table, sensitivity_anlys_event_study_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_event_study_covariate_table<-sensitivity_anlys_event_study_covariate_table[,-which(colnames(sensitivity_anlys_event_study_covariate_table) %in% c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_event_study_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_event_study_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_event_study_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_event_study.csv")

```

## Attributable Deaths
```{r}
###################################### Attributable Deaths #############################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_event_study<-sensitivity_anlys_event_study_data[which(sensitivity_anlys_event_study_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_event_study<-expit(-coef(sensitivity_anlys_event_study_model)["Intervention_Redefined"]*attr_deaths_anlys_event_study$Intervention_Redefined
                                     + logit(attr_deaths_anlys_event_study$imputed_deaths/attr_deaths_anlys_event_study$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_event_study_model) - 1.96*summary(sensitivity_anlys_event_study_model)$se
coef_ub<-coef(sensitivity_anlys_event_study_model) + 1.96*summary(sensitivity_anlys_event_study_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_event_study<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_event_study$Intervention_Redefined
                                        + logit(attr_deaths_anlys_event_study$imputed_deaths/attr_deaths_anlys_event_study$population))

prob_od_no_int_UB_event_study<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_event_study$Intervention_Redefined
                                        + logit(attr_deaths_anlys_event_study$imputed_deaths/attr_deaths_anlys_event_study$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_event_study_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_event_study$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_event_study$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_event_study$imputed_deaths[time_point_index]
                          - prob_od_no_int_event_study[time_point_index]*attr_deaths_anlys_event_study$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_event_study$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_event_study[time_point_index]*attr_deaths_anlys_event_study$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_event_study$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_event_study[time_point_index]*attr_deaths_anlys_event_study$population[time_point_index])


  index<-index + 1
}

num_attr_od_event_study<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_event_study$Time_Period_ID)),
                                       "Time_Start" = sort(unique(attr_deaths_anlys_event_study$Time_Period_Start)),
                                       "Num_Attr_Deaths" = num_attr_od,
                                       "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                       "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_event_study$Num_Attr_Deaths)


#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_event_study<-num_attr_od_event_study %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_event_study$deaths)



```



# Sensitivity Analysis 7: Effect of DIH Prosecutions on Unintentional Overdose Deaths, Including Two Intervention Variables
## Analysis
```{r, results = "asis"}
########### Sensitivity Analysis 4: Two Year Intervention Effect ######################
#create a plot for each state to see how many prosecution media alerts there are per 6 month period
#read in the prosecution media alert data
prosecution_data <- read.csv("./Data/dih_prosecutions_9_6_21.csv")

#data cleaning
prosecution_data<-prosecution_data %>% 
  mutate(Date = as.Date(Date.charged, "%m/%d/%Y")) %>%
  mutate(State = ifelse(State.Filed == "pennsylvania", "Pennsylvania", State.Filed),
         State = ifelse(State.Filed == "Virginia ", "Virginia", State)) %>%
  filter(!is.na(Date), State.Filed != "No Info", State.Filed != "No info", State.Filed != "No Info ",
         State != "")

#clean up the data by looking at the link to the article
prosecution_data$Date[prosecution_data$Date == "2026-08-01"] <- as.Date("2016-02-15", "%Y-%m-%d")

#change the states into Character instead of factor
prosecution_data$State<-as.character(prosecution_data$State)
#see how many prosecution data points there are for each state
table(prosecution_data$State)

#there are some repeated cases depending on victim
prosecution_data_unique <- prosecution_data %>%
  group_by(State) %>%
  distinct(Accused.Name, Date, .keep_all = T)
table(prosecution_data_unique$State)

#change date charged into Date object
prosecution_data$Date<-mdy(prosecution_data$Date.charged)

#group the data into six month periods
prosecution_data<-prosecution_data %>% mutate(six_month_pd = lubridate::floor_date(Date , "6 months" ))

#count the number of prosecution media alerts in each six month period
#we also get the first and last date of prosecution in time period
prosecution_data_by_six_month_pd <- prosecution_data %>%
  filter(year(six_month_pd)>1999 & year(six_month_pd)<2020) %>%
  group_by(State, six_month_pd) %>%
  summarise(first_date_in_pd = min(Date), last_date_in_pd = max(Date))

#create the data set used for this sensitivity analysis
#first, we merge the grouped prosecution data set with the main data set by state and time period
sensitivity_anlys_two_int_var<-merge(main_analysis_data,
                                           prosecution_data_by_six_month_pd, 
                                           by.x = c("State", "Time_Period_Start"),
                                           by.y = c("State", "six_month_pd"), all = TRUE)

#create a intervention 2 year effect variable by initializing it to be all 0
sensitivity_anlys_two_int_var<-sensitivity_anlys_two_int_var %>% 
  group_by(State) %>%
  mutate(int_2_yr_effect = 0)

#change the date into a date object
sensitivity_anlys_two_int_var$Time_Period_Start<-as.Date(sensitivity_anlys_two_int_var$Time_Period_Start)
sensitivity_anlys_two_int_var$Time_Period_End<-as.Date(sensitivity_anlys_two_int_var$Time_Period_End)

#we need to impute the newly defined intervention variable depending on the case
#by examining each row of the data set
for(state in unique(sensitivity_anlys_two_int_var$State)){
  #first, subset the data set into state_data which only contains the data for the state
  state_index<-which(sensitivity_anlys_two_int_var$State == state)
  state_data<-sensitivity_anlys_two_int_var[state_index,]

  #note that the first four rows of the 2 year effect intervention variable are the same as the
  #first four rows of the original intervention variable
  state_data$int_2_yr_effect[1:4]<-state_data$Intervention_Redefined[1:4]

  for(i in 5:nrow(state_data)){
    #next, we deal with the rows where there was at least one prosecution in the last 3 six month periods
    #These rows will be imputed with a 1
    if((!is.na(state_data$first_date_in_pd[i - 1]) |
        !is.na(state_data$first_date_in_pd[i - 2]) |
        !is.na(state_data$first_date_in_pd[i - 3]))){

      state_data$int_2_yr_effect[i]<-1

    }else{
      #next, we account for the rows with the fractions:
      # 1) an intervention occurs in row i without an intervention 2 years ago
      # 2) row i contains the lasting effects of an intervention that occurred 2 years ago
      # 3) row i contains effects from both a new intervention starting in row i and lasting
      # effects from 2 years ago

      #To compute the fraction, we add the number of days that are affected by an intervention
      #(from both the current prosecution and previous prosecution) and then divide by the total
      #number of days in the period:

      total_len_of_pd<-as.numeric(state_data$Time_Period_End[i] - state_data$Time_Period_Start[i])

      #If there is no prosecution two years ago, i.e. in period i-4, then the last_date is the first
      #date in period i. We subtract the last_date by the first date in the period, so we will get
      #a 0 for the number of days that are affected by a prosecution from period i-4. Otherwise,
      #the last_date is the last date of prosecution from period i-4, plus 2 years.
      len_of_past_effect <- ifelse(!is.na(state_data$first_date_in_pd[i - 4]),
                                   (state_data$last_date_in_pd[i - 4] + years(2)) - state_data$Time_Period_Start[i],
                                   0)

      #If there is no prosecution in the period i, then the start_date is the last date in the period i.
      #We subtract start_date from the last date in period i, so we will get a 0 for the number
      #of days that are affected by a prosecution in period i. Otherwise, the start_date is the
      #first date of a prosecution in period i.
      len_of_current_effect <- ifelse(!is.na(state_data$first_date_in_pd[i]),
                                      as.numeric(state_data$Time_Period_End[i] - state_data$first_date_in_pd[i]),
                                      0)

      state_data$int_2_yr_effect[i]<-(len_of_past_effect + len_of_current_effect)/total_len_of_pd
    }
  }

  #for the case where the int_2_yr_effect is greater than 1 (could result when we add the effects of
  #previous intervention and the current intervention), we just impute a 1 instead
  state_data$int_2_yr_effect[state_data$int_2_yr_effect>1]<-1

  #lastly, we store the int_2_yr_effect variable into the sensitivity analysis data set
  sensitivity_anlys_two_int_var$int_2_yr_effect[state_index]<-state_data$int_2_yr_effect
}

#view the data set just to make sure the imputation looks right
# View(sensitivity_anlys_two_int_var %>% select(State, Time_Period_Start, Time_Period_End,
#                                                     Intervention_Redefined, first_date_in_pd,
#                                                     last_date_in_pd,
#                                                     int_2_yr_effect))


sensitivity_anlys_two_int_var <- sensitivity_anlys_two_int_var %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention_2_yr_effect = sum(int_2_yr_effect))

#run the analysis on the sensitivity analysis data
sensitivity_anlys_two_int_var_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                            s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                            Naloxone_Pharmacy_Yes_Redefined +
                                            Naloxone_Pharmacy_No_Redefined +
                                            Medical_Marijuana_Redefined +
                                            Recreational_Marijuana_Redefined +
                                            GSL_Redefined +
                                            PDMP_Redefined +
                                            Medicaid_Expansion_Redefined +
                                            Intervention_Redefined + 
                                            num_states_w_intervention + 
                                            int_2_yr_effect, 
                                            # num_states_w_intervention_2_yr_effect,
                                          data = sensitivity_anlys_two_int_var, family = "binomial")

stargazer(sensitivity_anlys_two_int_var_model, type = "html", dep.var.labels = "Unintentional Overdose Death")
```

```{r}
#plot the confidence region between the two treatment coefficients
plot(ellipse(sensitivity_anlys_two_int_var_model, which = c(58, 60), level = 0.95))

```

## Plots
```{r}
plot(sensitivity_anlys_two_int_var_model)

```

## Compile Results
```{r}
############## Sensitivity Analysis 4: Make Data Frame of Results and 95% CI ##########
#store the coefficients into the table
sensitivity_anlys_redefine_int_full_table<-data.frame(coef(sensitivity_anlys_two_int_var_model))
#check to see how the table looks
head(sensitivity_anlys_redefine_int_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_redefine_int_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "int_2_yr_effect", 
              "num_states_w_intervention_2_yr_effect")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_redefine_int_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_redefine_int_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 6,
                                                                     stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i]))

    }else if(rownames(sensitivity_anlys_redefine_int_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                    substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 36,
                                                                           stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i])),
                                                                    sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_redefine_int_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate - 1.96*summary(sensitivity_anlys_two_int_var_model)$se
sensitivity_anlys_redefine_int_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate + 1.96*summary(sensitivity_anlys_two_int_var_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_redefine_int_full_table$Odds_Ratio<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate)
sensitivity_anlys_redefine_int_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_redefine_int_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_redefine_int_full_table$Standard_Error<-summary(sensitivity_anlys_two_int_var_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_redefine_int_full_table$p_value<-c(summary(sensitivity_anlys_two_int_var_model)$p.pv,
                                                     rep(NA, length(coef(sensitivity_anlys_two_int_var_model)) -
                                                           length(summary(sensitivity_anlys_two_int_var_model)$p.pv)))

head(sensitivity_anlys_redefine_int_full_table)
tail(sensitivity_anlys_redefine_int_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_redefine_int_full_table) %in% covariates)
sensitivity_anlys_redefine_int_covariate_table<-(round(sensitivity_anlys_redefine_int_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_redefine_int_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                            "Naloxone_Pharmacy_No",
                                                            "Medical_Marijuana",
                                                            "Recreational_Marijuana",
                                                            "GSL", 
                                                            "PDMP", 
                                                            "Medicaid_Expansion",
                                                            "Two Year Intervention Effect", 
                                                            "Number of States w Intervention")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_redefine_int_covariate_table<-rbind(sensitivity_anlys_redefine_int_covariate_table, sensitivity_anlys_redefine_int_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_redefine_int_covariate_table<-sensitivity_anlys_redefine_int_covariate_table[,-which(colnames(sensitivity_anlys_redefine_int_covariate_table) %in%
                                                                                                         c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_redefine_int_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_redefine_int_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_redefine_int_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_redefine_int.csv")

```

## Attributable Deaths
```{r}
################ Sensitivity Analysis 4: Number of Attributable Deaths ################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_redefine_int<-sensitivity_anlys_two_int_var[which(sensitivity_anlys_two_int_var$int_2_yr_effect>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_redefine_int<-expit(-coef(sensitivity_anlys_two_int_var_model)["int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                   + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_two_int_var_model) - 1.96*summary(sensitivity_anlys_two_int_var_model)$se
coef_ub<-coef(sensitivity_anlys_two_int_var_model) + 1.96*summary(sensitivity_anlys_two_int_var_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_redefine_int<-expit(-coef_lb[names(coef_lb) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

prob_od_no_int_UB_redefine_int<-expit(-coef_ub[names(coef_ub) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_two_int_var$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_redefine_int$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                          - prob_od_no_int_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])


  index<-index + 1
}

num_attr_od_redefine_int<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID)),
                                     "Time_Start" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_Start)),
                                     "Num_Attr_Deaths" = num_attr_od,
                                     "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                     "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_redefine_int$Num_Attr_Deaths)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_redefine_int<-num_attr_od_redefine_int %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths),
            death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_redefine_int$deaths)

ggplot(yearly_num_Attr_Deaths_redefine_int, aes(x = year, y = deaths)) + geom_line() + geom_point()

```


# Sensitivity Analysis 8: Using a Poisson Regression

## Analysis
```{r, results = "asis"}
############################## Run Model with Spline Time Effects by Region ###############################
#model that we will be using for the main analysis
#cr is used for cubic regression spline -- we are smoothing time effects by region
#run the analysis for all the states
sensitivity_anlys_poiss_model<-gam(log(imputed_deaths)~ State +
                                     s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                                     Naloxone_Pharmacy_Yes_Redefined +
                                     Naloxone_Pharmacy_No_Redefined +
                                     Medical_Marijuana_Redefined +
                                     Recreational_Marijuana_Redefined +
                                     GSL_Redefined +
                                     PDMP_Redefined +
                                     Medicaid_Expansion_Redefined +
                                     Intervention_Redefined + 
                                     num_states_w_intervention +
                                     log(population),
                                   data = main_analysis_data,
                                 family = "poisson")

#summary output of the model
stargazer(sensitivity_anlys_poiss_model, type = "html", dep.var.labels = c("Unintentional Overdose Deaths"))
```

## Compile Results
```{r}
############################## Main Analysis: Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
sensitivity_anlys_poiss_full_table<-data.frame(coef(sensitivity_anlys_poiss_model))
#check to see how the table looks
head(sensitivity_anlys_poiss_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_poiss_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined", 
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_poiss_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_poiss_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_poiss_full_table)[i]<-substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 6,
                                                    stop = nchar(rownames(sensitivity_anlys_poiss_full_table)[i]))

    }else if(rownames(sensitivity_anlys_poiss_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_poiss_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_poiss_full_table)[i]<-paste("Smoothed Time for Region ",
                                                   substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 36,
                                                          stop = nchar(rownames(sensitivity_anlys_poiss_full_table)[i])),
                                                   sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_poiss_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_poiss_full_table$Coefficient_Estimate - 
  1.96*summary(sensitivity_anlys_poiss_model)$se
sensitivity_anlys_poiss_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_poiss_full_table$Coefficient_Estimate + 
  1.96*summary(sensitivity_anlys_poiss_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_poiss_full_table$Odds_Ratio<-exp(sensitivity_anlys_poiss_full_table$Coefficient_Estimate)
sensitivity_anlys_poiss_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_poiss_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_poiss_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_poiss_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_poiss_full_table$Standard_Error<-summary(sensitivity_anlys_poiss_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_poiss_full_table$p_value<-c(summary(sensitivity_anlys_poiss_model)$p.pv, 
                                    rep(NA, length(coef(sensitivity_anlys_poiss_model)) - 
                                          length(summary(sensitivity_anlys_poiss_model)$p.pv)))

head(sensitivity_anlys_poiss_full_table)
tail(sensitivity_anlys_poiss_full_table)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_poiss_full_table,5), "./Data/coefficients_GAM_9_6_21_full_data_uninentional_od.csv")

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_poiss_full_table) %in% covariates)
sensitivity_anlys_poiss_covariate_table<-(round(sensitivity_anlys_poiss_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_poiss_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                           "Naloxone_Pharmacy_No",
                                           "Medical_Marijuana",
                                           "Recreational_Marijuana",
                                           "GSL", 
                                           "PDMP", 
                                           "Medicaid_Expansion",
                                           "Intervention", 
                                           "Number of States with DIH Prosecutions")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_poiss_covariate_table<-rbind(sensitivity_anlys_poiss_covariate_table, 
                                     sensitivity_anlys_poiss_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_poiss_covariate_table<-sensitivity_anlys_poiss_covariate_table[,-which(colnames(sensitivity_anlys_poiss_covariate_table) %in%
                                                                       c("Coefficient_Estimate", "Coefficient_Lower_Bound",
                                                                         "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_poiss_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_poiss_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_poiss_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_unintentional_od.csv")


```

## Attributable Deaths
````{r}
############################## Main Analysis: Number of Overdose Deaths Attributed to Intervention ###############################
#find the number of deaths attributable to the intervention
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_sensitivity_anlys_log<-sensitivity_anlys_poiss_data[which(sensitivity_anlys_poiss_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_sensitivity_anlys_log<-expit(-coef(sensitivity_anlys_poiss_model)["Intervention_Redefined"]*attr_deaths_anlys_sensitivity_anlys_log$Intervention_Redefined
                                    + logit(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths/attr_deaths_anlys_sensitivity_anlys_log$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_poiss_model) - 1.96*summary(sensitivity_anlys_poiss_model)$se
coef_ub<-coef(sensitivity_anlys_poiss_model) + 1.96*summary(sensitivity_anlys_poiss_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_sensitivity_anlys_log<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_sensitivity_anlys_log$Intervention_Redefined
                                       + logit(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths/attr_deaths_anlys_sensitivity_anlys_log$population))

prob_od_no_int_UB_sensitivity_anlys_log<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_sensitivity_anlys_log$Intervention_Redefined
                                       + logit(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths/attr_deaths_anlys_sensitivity_anlys_log$population))


#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths[time_point_index]
                          - prob_od_no_int_sensitivity_anlys_log[time_point_index]*attr_deaths_anlys_sensitivity_anlys_log$population[time_point_index])
  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_sensitivity_anlys_log[time_point_index]*attr_deaths_anlys_sensitivity_anlys_log$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_sensitivity_anlys_log[time_point_index]*attr_deaths_anlys_sensitivity_anlys_log$population[time_point_index])
  index<-index + 1
}

num_attr_od_sensitivity_anlys_log<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID)),
                                      "Time_Start" = sort(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_Start)),
                                      "Num_Attr_Deaths" = num_attr_od,
                                      "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                      "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths)
summary(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths)
num_attr_od_sensitivity_anlys_log$Time_Start<-as.Date(num_attr_od_sensitivity_anlys_log$Time_Start)

#compute the 95% CI for the total
sum(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths_LB)
sum(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths_UB)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_sensitivity_anlys_log<-num_attr_od_sensitivity_anlys_log %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_sensitivity_anlys_log$deaths)
summary(yearly_num_Attr_Deaths_sensitivity_anlys_log$death_lb)
summary(yearly_num_Attr_Deaths_sensitivity_anlys_log$death_ub)

```


# Sensitivity Analysis 9: Including Number of DIH Prosecutions as Covariate
## Analysis
```{r, results = "asis"}
########### Sensitivity Analysis 4: Two Year Intervention Effect ######################
#create a plot for each state to see how many prosecution media alerts there are per 6 month period
#read in the prosecution media alert data
prosecution_data<-read.csv("./Data/dih_prosecutions_9_6_21.csv")

#data cleaning
prosecution_data<-prosecution_data %>% 
  mutate(Date = as.Date(Date.charged, "%m/%d/%Y")) %>%
  mutate(State = ifelse(State.Filed == "pennsylvania", "Pennsylvania", State.Filed),
         State = ifelse(State.Filed == "Virginia ", "Virginia", State)) %>%
  filter(!is.na(Date), State.Filed != "No Info", State.Filed != "No info", State.Filed != "No Info ",
         State != "")

#clean up the data by looking at the link to the article
prosecution_data$Date[prosecution_data$Date == "2026-08-01"] <- as.Date("2016-02-15", "%Y-%m-%d")

#change the states into Character instead of factor
prosecution_data$State<-as.character(prosecution_data$State)
#see how many prosecution data points there are for each state
table(prosecution_data$State)

#there are some repeated cases depending on victim
prosecution_data_unique <- prosecution_data %>%
  group_by(State) %>%
  distinct(Accused.Name, Date, .keep_all = T)
table(prosecution_data_unique$State)

#change date charged into Date object
prosecution_data_unique$Date<-mdy(prosecution_data_unique$Date.charged)

#group the data into six month periods
prosecution_data_unique<-prosecution_data_unique %>% 
  mutate(six_month_pd = lubridate::floor_date(Date , "6 months" ))

#######ONLY IF GROUPS######
prosecution_grouped <- prosecution_data_unique %>% 
  #filter to dates after 2000 and dates before 2020
  filter(year(six_month_pd) >= 2000 & year(six_month_pd) <= 2019) %>%
  group_by(State, six_month_pd) %>% 
  #for each state, for each six month period, count the number of DIH prosecutions
  summarise(num_dih = n()) %>% 
  #label the groups according to zero, low, or high
  mutate(group = ifelse(num_dih == 0, "zero", ifelse(num_dih >= 5, "high", "low"))) %>%
  ungroup() %>%
  #have to add in a row for hawaii because its not in the prosecution dataset
  add_row(State = "Hawaii", six_month_pd = as.Date("2000-01-01"), num_dih = 0, group = "zero")

#we compute the final group for each state by seeing if it ever hits high or low
prosecution_grouped_final <- prosecution_grouped %>%  
  group_by(State) %>% 
  summarise(final_gp = ifelse(sum(group == "high") > 0, "high", ifelse(sum(group == "low")> 0, "low", "zero"))) 

ggplot(prosecution_grouped_final, aes(final_gp)) + 
  geom_bar() + 
  labs(title = "Number of States by DIH prosecution Category, with Low = [1,5]") + 
  geom_text(aes(label = ..count..), stat = "count", vjust = -.75)

#number of DIH prosecutions per six month for each state
# pdf("Figures/num_dih_per_six_month_pd_by_state_11_12_21.pdf")
ggplot(prosecution_grouped, aes(x = six_month_pd, y = num_dih)) + 
  geom_bar(stat = "identity") + 
  facet_wrap(~State) + 
  theme(axis.text.x = element_text(angle = 30, size = 5))
# dev.off()

# write.csv(prosecution_grouped, "./Data/num_dih_per_six_month_pd_by_state_11_12_21.csv")

#count the number of prosecution media alerts in each six month period
#we also get the first and last date of prosecution in time period
prosecution_data_by_six_month_pd <- prosecution_data %>%
  filter(year(six_month_pd)>1999 & year(six_month_pd)<2020) %>%
  group_by(State, six_month_pd) %>%
  summarise(first_date_in_pd = min(Date), last_date_in_pd = max(Date))

#create the data set used for this sensitivity analysis
#first, we merge the grouped prosecution data set with the main data set by state and time period
sensitivity_anlys_redefine_int_data<-merge(main_analysis_data,
                                           prosecution_data_by_six_month_pd, 
                                           by.x = c("State", "Time_Period_Start"),
                                           by.y = c("State", "six_month_pd"), all = TRUE)

#create a intervention 2 year effect variable by initializing it to be all 0
sensitivity_anlys_redefine_int_data<-sensitivity_anlys_redefine_int_data %>% 
  group_by(State) %>%
  mutate(int_2_yr_effect = 0)

#change the date into a date object
sensitivity_anlys_redefine_int_data$Time_Period_Start<-as.Date(sensitivity_anlys_redefine_int_data$Time_Period_Start)
sensitivity_anlys_redefine_int_data$Time_Period_End<-as.Date(sensitivity_anlys_redefine_int_data$Time_Period_End)

#we need to impute the newly defined intervention variable depending on the case
#by examining each row of the data set
for(state in unique(sensitivity_anlys_redefine_int_data$State)){
  #first, subset the data set into state_data which only contains the data for the state
  state_index<-which(sensitivity_anlys_redefine_int_data$State == state)
  state_data<-sensitivity_anlys_redefine_int_data[state_index,]

  #note that the first four rows of the 2 year effect intervention variable are the same as the
  #first four rows of the original intervention variable
  state_data$int_2_yr_effect[1:4]<-state_data$Intervention_Redefined[1:4]

  for(i in 5:nrow(state_data)){
    #next, we deal with the rows where there was at least one prosecution in the last 3 six month periods
    #These rows will be imputed with a 1
    if((!is.na(state_data$first_date_in_pd[i - 1]) |
        !is.na(state_data$first_date_in_pd[i - 2]) |
        !is.na(state_data$first_date_in_pd[i - 3]))){

      state_data$int_2_yr_effect[i]<-1

    }else{
      #next, we account for the rows with the fractions:
      # 1) an intervention occurs in row i without an intervention 2 years ago
      # 2) row i contains the lasting effects of an intervention that occurred 2 years ago
      # 3) row i contains effects from both a new intervention starting in row i and lasting
      # effects from 2 years ago

      #To compute the fraction, we add the number of days that are affected by an intervention
      #(from both the current prosecution and previous prosecution) and then divide by the total
      #number of days in the period:

      total_len_of_pd<-as.numeric(state_data$Time_Period_End[i] - state_data$Time_Period_Start[i])

      #If there is no prosecution two years ago, i.e. in period i-4, then the last_date is the first
      #date in period i. We subtract the last_date by the first date in the period, so we will get
      #a 0 for the number of days that are affected by a prosecution from period i-4. Otherwise,
      #the last_date is the last date of prosecution from period i-4, plus 2 years.
      len_of_past_effect <- ifelse(!is.na(state_data$first_date_in_pd[i - 4]),
                                   (state_data$last_date_in_pd[i - 4] + years(2)) - state_data$Time_Period_Start[i],
                                   0)

      #If there is no prosecution in the period i, then the start_date is the last date in the period i.
      #We subtract start_date from the last date in period i, so we will get a 0 for the number
      #of days that are affected by a prosecution in period i. Otherwise, the start_date is the
      #first date of a prosecution in period i.
      len_of_current_effect <- ifelse(!is.na(state_data$first_date_in_pd[i]),
                                      as.numeric(state_data$Time_Period_End[i] - state_data$first_date_in_pd[i]),
                                      0)

      state_data$int_2_yr_effect[i]<-(len_of_past_effect + len_of_current_effect)/total_len_of_pd
    }
  }

  #for the case where the int_2_yr_effect is greater than 1 (could result when we add the effects of
  #previous intervention and the current intervention), we just impute a 1 instead
  state_data$int_2_yr_effect[state_data$int_2_yr_effect>1]<-1

  #lastly, we store the int_2_yr_effect variable into the sensitivity analysis data set
  sensitivity_anlys_redefine_int_data$int_2_yr_effect[state_index]<-state_data$int_2_yr_effect
}

#view the data set just to make sure the imputation looks right
# View(sensitivity_anlys_redefine_int_data %>% select(State, Time_Period_Start, Time_Period_End,
#                                                     Intervention_Redefined, first_date_in_pd,
#                                                     last_date_in_pd,
#                                                     int_2_yr_effect))


sensitivity_anlys_redefine_int_data <- sensitivity_anlys_redefine_int_data %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention_2_yr_effect = sum(int_2_yr_effect))

#run the analysis on the sensitivity analysis data
sensitivity_anlys_redefine_int_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                            s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                            Naloxone_Pharmacy_Yes_Redefined +
                                            Naloxone_Pharmacy_No_Redefined +
                                            Medical_Marijuana_Redefined +
                                            Recreational_Marijuana_Redefined +
                                            GSL_Redefined +
                                            PDMP_Redefined +
                                            Medicaid_Expansion_Redefined +
                                            int_2_yr_effect + 
                                            num_states_w_intervention_2_yr_effect,
                                          data = sensitivity_anlys_redefine_int_data, family = "binomial")

stargazer(sensitivity_anlys_redefine_int_model, type = "html", dep.var.labels = "Unintentional Overdose Death")
```

## Plots
```{r}
plot(sensitivity_anlys_redefine_int_model)

```

## Compile Results
```{r}
############## Sensitivity Analysis 4: Make Data Frame of Results and 95% CI ##########
#store the coefficients into the table
sensitivity_anlys_redefine_int_full_table<-data.frame(coef(sensitivity_anlys_redefine_int_model))
#check to see how the table looks
head(sensitivity_anlys_redefine_int_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_redefine_int_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "int_2_yr_effect", 
              "num_states_w_intervention_2_yr_effect")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_redefine_int_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_redefine_int_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 6,
                                                                     stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i]))

    }else if(rownames(sensitivity_anlys_redefine_int_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                    substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 36,
                                                                           stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i])),
                                                                    sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_redefine_int_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate - 1.96*summary(sensitivity_anlys_redefine_int_model)$se
sensitivity_anlys_redefine_int_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate + 1.96*summary(sensitivity_anlys_redefine_int_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_redefine_int_full_table$Odds_Ratio<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Estimate)
sensitivity_anlys_redefine_int_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_redefine_int_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_redefine_int_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_redefine_int_full_table$Standard_Error<-summary(sensitivity_anlys_redefine_int_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_redefine_int_full_table$p_value<-c(summary(sensitivity_anlys_redefine_int_model)$p.pv,
                                                     rep(NA, length(coef(sensitivity_anlys_redefine_int_model)) -
                                                           length(summary(sensitivity_anlys_redefine_int_model)$p.pv)))

head(sensitivity_anlys_redefine_int_full_table)
tail(sensitivity_anlys_redefine_int_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_redefine_int_full_table) %in% covariates)
sensitivity_anlys_redefine_int_covariate_table<-(round(sensitivity_anlys_redefine_int_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_redefine_int_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                            "Naloxone_Pharmacy_No",
                                                            "Medical_Marijuana",
                                                            "Recreational_Marijuana",
                                                            "GSL", 
                                                            "PDMP", 
                                                            "Medicaid_Expansion",
                                                            "Two Year Intervention Effect", 
                                                            "Number of States w Intervention")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_redefine_int_covariate_table<-rbind(sensitivity_anlys_redefine_int_covariate_table, sensitivity_anlys_redefine_int_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_redefine_int_covariate_table<-sensitivity_anlys_redefine_int_covariate_table[,-which(colnames(sensitivity_anlys_redefine_int_covariate_table) %in%
                                                                                                         c("Coefficient_Estimate", "Coefficient_Lower_Bound", "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_redefine_int_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_redefine_int_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_redefine_int_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_redefine_int.csv")

```

## Attributable Deaths
```{r}
################ Sensitivity Analysis 4: Number of Attributable Deaths ################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_redefine_int<-sensitivity_anlys_redefine_int_data[which(sensitivity_anlys_redefine_int_data$int_2_yr_effect>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_redefine_int<-expit(-coef(sensitivity_anlys_redefine_int_model)["int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                   + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_redefine_int_model) - 1.96*summary(sensitivity_anlys_redefine_int_model)$se
coef_ub<-coef(sensitivity_anlys_redefine_int_model) + 1.96*summary(sensitivity_anlys_redefine_int_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_redefine_int<-expit(-coef_lb[names(coef_lb) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

prob_od_no_int_UB_redefine_int<-expit(-coef_ub[names(coef_ub) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_redefine_int_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_redefine_int$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                          - prob_od_no_int_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])


  index<-index + 1
}

num_attr_od_redefine_int<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID)),
                                     "Time_Start" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_Start)),
                                     "Num_Attr_Deaths" = num_attr_od,
                                     "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                     "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_redefine_int$Num_Attr_Deaths)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_redefine_int<-num_attr_od_redefine_int %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths),
            death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_redefine_int$deaths)

ggplot(yearly_num_Attr_Deaths_redefine_int, aes(x = year, y = deaths)) + geom_line() + geom_point()

```


# Sensitivity Analysis 10: Including Unemployment Rates and Age Groups

## Clean Data
```{r}
#create age groups using the population dataset
pop_by_age_2010<- read.csv("./Data/pop_by_age_state_2000_2010.csv")
head(pop_by_age_2010)
#base the age groups on abouk, powell, pacula
#age groups: 0 - 17, 18 - 34, 35 - 54, 65+
pop_by_age_2010_group <- pop_by_age_2010 %>% 
  #999 indicates total population
  filter(AGE < 999, AGE >= 18) %>%
  #create age groups
  mutate(age_groups = ifelse(AGE >= 18 & AGE <= 24, "18_24", 
                             ifelse(AGE >= 25 & AGE <= 34, "25_34",
                                    ifelse(AGE >= 35 & AGE <= 44, "35_44", 
                                           ifelse(AGE >= 45 & AGE <= 64, "45_64", "65_plus"))))) %>%
  #we filter according to how the DIH prosecutions were filtered, excluding cases with minors
  filter(NAME != "United States", SEX == 0) %>%
  group_by(NAME, age_groups) %>%
  summarise(pop_2000 = sum(ESTIMATESBASE2000),
            pop_2001 = sum(POPESTIMATE2001),
            pop_2002 = sum(POPESTIMATE2002),
            pop_2003 = sum(POPESTIMATE2003),
            pop_2004 = sum(POPESTIMATE2004),
            pop_2005 = sum(POPESTIMATE2005),
            pop_2006 = sum(POPESTIMATE2006),
            pop_2007 = sum(POPESTIMATE2007),
            pop_2008 = sum(POPESTIMATE2008),
            pop_2009 = sum(POPESTIMATE2009),
            pop_2010 = sum(CENSUS2010POP))

#also make distribution of each age group
#define proportion function
#calculate the proportion of the age group for each state, year
prop <- function(x){x/sum(x)}
pop_by_age_2010_group <- pop_by_age_2010_group %>%
  group_by(NAME) %>%
  mutate_if(is.numeric, prop)

pop_by_age_2019 <- read.csv("./Data/pop_by_age_state_2010_2019.csv")
head(pop_by_age_2019)
pop_by_age_2019_group <- pop_by_age_2019 %>% 
  #999 indicates total population
  filter(AGE < 999, AGE >= 18) %>%
  #create age groups
  mutate(age_groups = ifelse(AGE >= 18 & AGE <= 24, "18_24", 
                             ifelse(AGE >= 25 & AGE <= 34, "25_34",
                                    ifelse(AGE >= 35 & AGE <= 44, "35_44", 
                                           ifelse(AGE >= 45 & AGE <= 64, "45_64", "65_plus"))))) %>%
  filter(NAME != "United States", SEX == 0) %>%
  group_by(NAME, age_groups) %>%
  summarise(pop_2011 = sum(POPEST2011_CIV),
            pop_2012 = sum(POPEST2012_CIV),
            pop_2013 = sum(POPEST2013_CIV),
            pop_2014 = sum(POPEST2014_CIV),
            pop_2015 = sum(POPEST2015_CIV),
            pop_2016 = sum(POPEST2016_CIV),
            pop_2017 = sum(POPEST2017_CIV),
            pop_2018 = sum(POPEST2018_CIV),
            pop_2019 = sum(POPEST2019_CIV))

#calculate the proportion of the age group for each year, state
pop_by_age_2019_group <- pop_by_age_2019_group %>%
  group_by(NAME) %>%
  mutate_if(is.numeric, prop)

#combine the population and unemployment data to the main analysis dataset
population_prop_data <- merge(pop_by_age_2010_group, pop_by_age_2019_group, by = c("NAME", "age_groups"))

#change the population and unemployment dataset from wide to long
population_prop_data_long <- population_prop_data %>%
  pivot_longer(
    #change the cols that start with "pop"
    cols = starts_with("pop"),
    #put the column names into a col named "year"
    names_to = "year",
    #put the values into a col names "population_prop"
    values_to = "population_prop",
    #delete the "pop_" from the column names
    names_prefix = "pop_"
  )

#now we want to pivot the table to wider so that each age_group population share is a column
population_prop_data_final <- population_prop_data_long %>%
  pivot_wider(
    #each row identified by state and year
    id_cols = c("NAME", "year"),
    #take the column names from the age groups
    names_from = age_groups,
    #take the values from the population proportion column
    values_from = population_prop,
    #add "pop_prop_" to the col names
    names_prefix = "pop_prop_"
  )
```

```{r}
#skip the first three rows because that is the header
#the series ID corresponds to the different states in order, so have to rename the IDs -- checked this manually 11/29/21
#we manually retrieved the data from BLS to get seasonally adjusted unemployment rates for each state
unemployment_rate <- readxl::read_excel("./Data/bls_unemployment_rate_2000_2019.xlsx", skip = 3)
unemployment_rate <- unemployment_rate %>%
  mutate(State = unique(main_analysis_data$State))

#change dataset from wide to long
unemployment_rate_long <- unemployment_rate %>%
  pivot_longer(
    cols = is.numeric,
    #add column for the date
    names_to = "time",
    #add column where values are the unemployment rate
    values_to = "unemployment_rate"
  ) %>%
  #remove the columns which are not needed or are all NA
  dplyr::select(-`Series ID`,
         -`Nov\n2021` ,
         -`Dec\n2021`) %>%
  #change date to date object
  mutate(time = mdy(time)) %>%
  #filter to only keep date up to 2019
  filter(year(time) <= 2019)

#since we need 6 month groupings, we take the average per 6 month and state
unemployment_rate_summary <- unemployment_rate_long %>%
  #compute the six month period for each date
  mutate(six_month_pd = floor_date(time, unit = "6 months")) %>%
  #group by state and six month period
  group_by(State, six_month_pd) %>%
  #compute the mean unemployment rate
  summarise(mean_unemployment = mean(unemployment_rate/100))

```

```{r}
#finally merge the two datasets into the main analysis dataset
#first create a temporary main analysis data with a year column
main_analysis_data_w_year <- main_analysis_data %>%
  mutate(year = year(Time_Period_Start))
#merge the age group proportion data
sensitivity_anlys_age_gp_unemp <- merge(main_analysis_data_w_year, population_prop_data_final,
                            by.x = c("State", "year"), by.y = c("NAME", "year"))

#merge the unemployment rate
sensitivity_anlys_age_gp_unemp <- merge(sensitivity_anlys_age_gp_unemp, unemployment_rate_summary,
                            by.x = c("State", "Time_Period_Start"), by.y = c("State", "six_month_pd"))

#remove the year column
sensitivity_anlys_age_gp_unemp <- sensitivity_anlys_age_gp_unemp %>%
  dplyr::select(-year) %>%
  mutate(mean_unemployment = mean_unemployment)
```

## Quick EDA
```{r}
#do an EDA to check drug overdoses by age group and unemployment rate
ggplot(sensitivity_anlys_age_gp_unemp) + 
  geom_line(aes(y = imputed_deaths/population, x = pop_prop_18_24, color = "18_24")) + 
  geom_line(aes(y = imputed_deaths/population, x = pop_prop_25_34, color = "25_34")) + 
  geom_line(aes(y = imputed_deaths/population, x = pop_prop_35_44, color = "35_44")) + 
  geom_line(aes(y = imputed_deaths/population, x = pop_prop_45_64, color = "45_64")) + 
  geom_line(aes(y = imputed_deaths/population, x = pop_prop_65_plus, color = "65+")) + 
  facet_wrap(~State)

ggplot(sensitivity_anlys_age_gp_unemp) + 
  geom_line( aes(y = (imputed_deaths/population)*100, x = Time_Period_Start, color = "od_rate", group = 1)) + 
  geom_line( aes(y = mean_unemployment, x = Time_Period_Start, color = "unemp", group = 1)) + 
  facet_wrap(~State)
```


```{r, results = "asis"}
############################## Run Model with Spline Time Effects by Region ###############################
#model that we will be using for the main analysis
#cr is used for cubic regression spline -- we are smoothing time effects by region
#run the analysis for all the states
sensitivity_anlys_age_unemp_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                     s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                                     Naloxone_Pharmacy_Yes_Redefined +
                                     Naloxone_Pharmacy_No_Redefined +
                                     Medical_Marijuana_Redefined +
                                     Recreational_Marijuana_Redefined +
                                     GSL_Redefined +
                                     PDMP_Redefined +
                                     Medicaid_Expansion_Redefined +
                                     Intervention_Redefined + 
                                     num_states_w_intervention + 
                                     mean_unemployment + 
                                     pop_prop_25_34 + 
                                     pop_prop_35_44 + 
                                     pop_prop_45_64 + 
                                     pop_prop_65_plus,
                                     data = sensitivity_anlys_age_gp_unemp,
                                   family = "binomial")

# summary(sensitivity_anlys_age_unemp_model)

#summary output of the model
stargazer(sensitivity_anlys_age_unemp_model, type = "html", dep.var.labels = c("Unintentional Overdose Deaths"))
```

## Sandwich Estimator
```{r}
#here, we estimate the variance-covariance matrix through the sandwich estimator
#we only want to estimate the variances for the policies, intervention variable, and number of states with intervention

sensitivity_anlys_age_unemp_subset_cov <- sensitivity_anlys_age_gp_unemp %>%
  ungroup() %>%
  dplyr::select(
         Naloxone_Pharmacy_Yes_Redefined,
         Naloxone_Pharmacy_No_Redefined,
         Medical_Marijuana_Redefined,
         Recreational_Marijuana_Redefined,
         GSL_Redefined,
         PDMP_Redefined,
         Medicaid_Expansion_Redefined,
         Intervention_Redefined,
         num_states_w_intervention,
         mean_unemployment,
         pop_prop_25_34,
         pop_prop_35_44,
         pop_prop_45_64,
         pop_prop_65_plus)

#extract the probability of overdoses and observed proportion of OD
sensitivity_anlys_age_unemp_prob_od <- predict(sensitivity_anlys_age_unemp_model, 
                                               newdata = sensitivity_anlys_age_gp_unemp, type = "response")
sensitivity_anlys_age_unemp_prop_obs_od <- main_analysis_data$imputed_deaths/sensitivity_anlys_age_gp_unemp$population

#estimate the constant term: sum_{s,t} x_{s,t}*p_{s,t}*(1-p_{s,t})*x_{s,t}^T
sensitivity_anlys_age_unemp_constant_term <- matrix(0, nrow = ncol(sensitivity_anlys_age_unemp_subset_cov), 
                                                    ncol = ncol(sensitivity_anlys_age_unemp_subset_cov))
for(row in 1:nrow(sensitivity_anlys_age_unemp_subset_cov)){
  sensitivity_anlys_age_unemp_constant_term <- sensitivity_anlys_age_unemp_constant_term +
    t(sensitivity_anlys_age_unemp_subset_cov[row,])%*%sensitivity_anlys_age_unemp_prob_od[row]%*%
    (1-sensitivity_anlys_age_unemp_prob_od[row])%*%as.matrix(sensitivity_anlys_age_unemp_subset_cov[row,])
}

#estimate middle term: sum_{s,t} x_{s,t}*mean(prop_obs_od - prob_od)^2*x_{s,t}^T
sensitivity_anlys_age_unemp_mean_prop_minus_prob_sq <- mean((sensitivity_anlys_age_unemp_prop_obs_od -
                                                               sensitivity_anlys_age_unemp_prob_od)^2)
sensitivity_anlys_age_unemp_exp_squared <- matrix(0, nrow = ncol(sensitivity_anlys_age_unemp_subset_cov), 
                                                  ncol = ncol(sensitivity_anlys_age_unemp_subset_cov))
for(row in 1:nrow(sensitivity_anlys_age_unemp_subset_cov)){
  sensitivity_anlys_age_unemp_exp_squared <- sensitivity_anlys_age_unemp_exp_squared +
    t(sensitivity_anlys_age_unemp_subset_cov[row,])%*%sensitivity_anlys_age_unemp_mean_prop_minus_prob_sq%*%
    as.matrix(sensitivity_anlys_age_unemp_subset_cov[row,])
}

#variance-covariance = constant_term^{-1}*exp_squared*t(constant_term^{-1})
#here, solve computes the inverse of the matrix
sensitivity_anlys_age_unemp_var_cov <- solve(sensitivity_anlys_age_unemp_constant_term)%*%sensitivity_anlys_age_unemp_exp_squared%*%
  t(solve(sensitivity_anlys_age_unemp_constant_term))
#we obtain the standard deviations by taking the square root of the diagonal of the variance-covariance matrix.
sensitivity_anlys_age_unemp_sd_of_coefficients <- sqrt(diag(sensitivity_anlys_age_unemp_var_cov))

#find the 95% CI for the coefficients
sensitivity_anlys_age_unemp_coefficient_values <- tail(summary(sensitivity_anlys_age_unemp_model)$p.coeff,
                                                       length(diag(sensitivity_anlys_age_unemp_var_cov)))
sensitivity_anlys_age_unemp_lb_coef <- sensitivity_anlys_age_unemp_coefficient_values -
  1.96*(sensitivity_anlys_age_unemp_sd_of_coefficients)
sensitivity_anlys_age_unemp_ub_coef <- sensitivity_anlys_age_unemp_coefficient_values +
  1.96*(sensitivity_anlys_age_unemp_sd_of_coefficients)

data.frame(sensitivity_anlys_age_unemp_lb_coef, sensitivity_anlys_age_unemp_coefficient_values, sensitivity_anlys_age_unemp_ub_coef,
           exp_lb = exp(sensitivity_anlys_age_unemp_lb_coef), exp_coef = exp(sensitivity_anlys_age_unemp_coefficient_values),
           exp_ub = exp(sensitivity_anlys_age_unemp_ub_coef))
```



## Bootstrap Estimates
```{r}
#first create a function to run the analysis
fit_model <- function(boot_data, int_variables, other_predictors = NULL){
  #want to make a function that will work for the case when we use one intervention variable and two
  #to do this, we store the formula that will be put into the model
  #we paste the intervention variables, a vector of variable names, at the end. 
  #clean up the int_variables by putting a "+" in between the variable names
  cleaned_int_var <- paste(c(int_variables, other_predictors), collapse = "+")
  model_formula <- as.formula(paste("cbind(round(imputed_deaths), round(num_alive))~ State +
                           s(Time_Period_ID, bs = 'cr', by = as.factor(Region)) +
                           Naloxone_Pharmacy_Yes_Redefined +
                           Naloxone_Pharmacy_No_Redefined +
                           Medical_Marijuana_Redefined +
                           Recreational_Marijuana_Redefined +
                           GSL_Redefined +
                           PDMP_Redefined +
                           Medicaid_Expansion_Redefined +",
                           cleaned_int_var))
  #plug model_formula into the gam model
  boot_model <- gam(model_formula, data = boot_data, family = "binomial")
  #take the summary of the gam model
  summary_boot_model <- summary(boot_model)
  #extract the coefficients for the variables excluding intervention and state
  int_variables_coef <- summary_boot_model$p.coeff[grepl("Intercept", names(summary_boot_model$p.coeff)) == FALSE &
                                                     grepl("State", names(summary_boot_model$p.coeff)) == FALSE]
  #return the coefficients
  return(int_variables_coef)
}

bootstrap_run <- function(data_set, int_variables, other_predictors = NULL, boot_var = "State", n_iter = 1000, seed = 123, 
                          print_index = TRUE){
  boot_var_unique <- unique(data_set[,boot_var])
  stored_coef <- data.frame()
  # names(stored_coef) <- c("(Intercept)", sapply(unique(data_set$State), function(x){paste("State", x, sep = "")}), 
  #                         "Naloxone_Pharmacy_Yes_Redefined", "Naloxone_Pharmacy_No_Redefined", "Medical_Marijuana_Redefined",
  #                          "Recreational_Marijuana_Redefined", "GSL_Redefined", "PDMP_Redefined", "Medicaid_Expansion_Redefined",
  #                         int_variables, other_predictors)
  stored_attributable_deaths <- data.frame()
  
  #set seed
  set.seed(seed)
  #run n_iter times
  for(iter in 1:n_iter){
    if(print_index == TRUE & iter %% 10 == 0){print(iter)}
    #sample state with replacement
    sampled_var <- sample(boot_var_unique, length(boot_var_unique), replace = TRUE)
    #bootstrap dataset contains the sampled states
    boot_data_list <- lapply(sampled_var, function(x){data_set[data_set[,boot_var] == x,]})
    boot_data <- do.call(rbind, boot_data_list)

    #run model 
    fitted_model_coef <- fit_model(boot_data, int_variables, other_predictors)
    #store the coefficients
    stored_coef <- rbind(stored_coef, fitted_model_coef)
    names(stored_coef) <- names(fitted_model_coef)

    #estimate attributable deaths
    #first create vector to store the counts
    attr_deaths_boot <- rep(0, length(unique(year(data_set$Time_Period_Start))))
    names(attr_deaths_boot) <- unique(year(data_set$Time_Period_Start))

    #first subset data to the rows in which intervention is non-zero
    if(length(int_variables) == 2){
      #write the condition to filter the dataset to those with int > 0
      condition <- paste(paste(int_variables, collapse = "> 0 &"), "> 0")
    }else{
      condition <- paste(int_variables, "> 0")
    }
    attr_death_data <- data_set %>%
      filter(eval(parse(text = condition)))

    attr_death_data <- attr_death_data %>%
      #then we compute the probability of overdose death had intervention not occured
      mutate(p_OD_no_int = expit(logit(imputed_deaths/population) - 
                                   apply(sapply(int_variables, function(x){attr_death_data[,x]*fitted_model_coef[x]}), 
                                         1, sum)),
             #compute the number of OD had intervention not occurred
             n_OD_no_int = population*p_OD_no_int,
             #compute the number of attributable deaths
             n_attr = imputed_deaths - n_OD_no_int
             
      )
    #now group by the year to find the sum of OD deaths
    yrly_num_attr <- attr_death_data %>%
      group_by(year = year(Time_Period_Start)) %>%
      summarise(total_attr_deaths = sum(n_attr))
    
    #store into the attributable deaths
    attr_deaths_boot <- as.vector(yrly_num_attr$total_attr_deaths)
    stored_attributable_deaths <- rbind(stored_attributable_deaths, attr_deaths_boot)
  }
  
  names(stored_attributable_deaths) <- unique(year(data_set$Time_Period_Start))
  #rename the columns of the dataset so that we know which columns the coefficients correspond to
  return(list(stored_coef = stored_coef, stored_attributable_deaths = stored_attributable_deaths))
}

#run the bootstrap
# bootstrap_results <- bootstrap_run(sensitivity_anlys_age_gp_unemp, int_variables = "Intervention_Redefined", 
#                                   other_predictors = c("num_states_w_intervention", "mean_unemployment", 
#                                      "pop_prop_25_34", "pop_prop_35_44", "pop_prop_45_64", "pop_prop_65_plus"),
#                                   n_iter = 2000)

apply(bootstrap_results$stored_coef, 2, quantile, probs = c(.025, .5, .975))
apply(bootstrap_results$stored_coef, 2, mean)

# write.csv(bootstrap_results$stored_coef, "./Data/bootstrap_coef_age_gp_unemp_11_29_21.csv", row.names = FALSE)
# write.csv(bootstrap_results$stored_attributable_deaths, "./Data/bootstrap_attr_deaths_age_gp_unemp_11_29_21.csv", row.names = FALSE)

stored_coef <- read.csv("./Data/bootstrap_coef_age_gp_unemp_11_29_21.csv")
stored_attributable_deaths <- read.csv("./Data/bootstrap_attr_deaths_age_gp_unemp_11_29_21.csv")
bootstrap_results <- list(stored_coef = stored_coef, stored_attributable_deaths = stored_attributable_deaths)

#convert the dataset to long to plot
bootstrap_results_stored_coef_long <- bootstrap_results$stored_coef %>%
  pivot_longer(everything(), 
               names_to = "predictor",
               values_to = "coef") %>%
  group_by(predictor) %>%
  mutate(lb = quantile(coef, 0.025),
         ub = quantile(coef, 0.975), 
         mean = mean(coef))

#merge the real coefficients into the dataset
sensitivity_anlys_age_unemp_model_coef <- data.frame(predictor = names(summary(sensitivity_anlys_age_unemp_model)$p.coeff), 
                                                     coef_obs = summary(sensitivity_anlys_age_unemp_model)$p.coeff)
bootstrap_results_stored_coef_long <- merge(bootstrap_results_stored_coef_long, sensitivity_anlys_age_unemp_model_coef, 
                                            by = "predictor")

ggplot(bootstrap_results_stored_coef_long, aes(x = coef)) + 
  geom_histogram() + 
  facet_wrap(~predictor, scales = "free") + 
  geom_vline(aes(xintercept = lb, color = "95% CI")) + 
  geom_vline(aes(xintercept = ub, color = "95% CI")) + 
  geom_vline(aes(xintercept = mean, color = "mean")) + 
  geom_vline(aes(xintercept = coef_obs, color = "Observed")) + 
  theme(axis.text.x = element_text(hjust = 1, size = 6, family = "Times"),
        axis.text.y = element_text(size = 6, family = "Times"),
        axis.title = element_text(size = 10, face = "bold", family = "Times"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size=8),
        panel.background = element_rect("white"),
        legend.position = "bottom")

bootstrap_results_attr_deaths_long <- bootstrap_results$stored_attributable_deaths %>%
  pivot_longer(everything(),
               names_to = "year",
               values_to = "attr_death") %>%
  group_by(year) %>%
  summarise(mean_death = mean(attr_death),
            lb_death = quantile(attr_death, 0.025),
            ub_death = quantile(attr_death, 0.975)) %>%
  ungroup()

ggplot(bootstrap_results_attr_deaths_long) + 
  geom_line(aes(y = mean_death, x = year, color = "Estimate", group = 1)) +
  geom_point(aes(y = mean_death, x = year)) + 
  geom_line(aes(y = lb_death, x = year, color = "95% CI", group = 1)) + 
  geom_line(aes(y = ub_death, x = year, color = "95% CI", group = 1)) + 
  labs(y = "Number of Unintentional Overdose Deaths Attributable to DIH Prosecutions",
       x = "Year") + 
  theme(axis.text=element_text(family="Times",size=10),
        axis.title=element_text(family="Times", size=10, face="bold"),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.line = element_line(colour = "black"),
        axis.text.x = element_text(family="Times", size=10, angle = 30),
        panel.background = element_rect("white")) 
```


## Compile Results
```{r}
############################## Main Analysis: Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
sensitivity_anlys_poiss_full_table<-data.frame(coef(sensitivity_anlys_poiss_model))
#check to see how the table looks
head(sensitivity_anlys_poiss_full_table)
#rename the column to "Coefficient_Estimate"
colnames(sensitivity_anlys_poiss_full_table)<-c("Coefficient_Estimate")

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined", 
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_poiss_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_poiss_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_poiss_full_table)[i]<-substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 6,
                                                    stop = nchar(rownames(sensitivity_anlys_poiss_full_table)[i]))

    }else if(rownames(sensitivity_anlys_poiss_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_poiss_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_poiss_full_table)[i]<-paste("Smoothed Time for Region ",
                                                   substr(rownames(sensitivity_anlys_poiss_full_table)[i], start = 36,
                                                          stop = nchar(rownames(sensitivity_anlys_poiss_full_table)[i])),
                                                   sep = "")

    }
  }
}

#confidence intervals for the coefficients
sensitivity_anlys_poiss_full_table$Coefficient_Lower_Bound<-sensitivity_anlys_poiss_full_table$Coefficient_Estimate - 
  1.96*summary(sensitivity_anlys_poiss_model)$se
sensitivity_anlys_poiss_full_table$Coefficient_Upper_Bound<-sensitivity_anlys_poiss_full_table$Coefficient_Estimate + 
  1.96*summary(sensitivity_anlys_poiss_model)$se

#impute the estimates and confidence intervals in the odds ratio scale
sensitivity_anlys_poiss_full_table$Odds_Ratio<-exp(sensitivity_anlys_poiss_full_table$Coefficient_Estimate)
sensitivity_anlys_poiss_full_table$Odds_Ratio_LB<-exp(sensitivity_anlys_poiss_full_table$Coefficient_Lower_Bound)
sensitivity_anlys_poiss_full_table$Odds_Ratio_UB<-exp(sensitivity_anlys_poiss_full_table$Coefficient_Upper_Bound)

#store the standard error and p-value
sensitivity_anlys_poiss_full_table$Standard_Error<-summary(sensitivity_anlys_poiss_model)$se
#note that there is no p-value for the smoothed time effects, so we put a NA for those rows
sensitivity_anlys_poiss_full_table$p_value<-c(summary(sensitivity_anlys_poiss_model)$p.pv, 
                                    rep(NA, length(coef(sensitivity_anlys_poiss_model)) - 
                                          length(summary(sensitivity_anlys_poiss_model)$p.pv)))

head(sensitivity_anlys_poiss_full_table)
tail(sensitivity_anlys_poiss_full_table)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_poiss_full_table,5), "./Data/coefficients_GAM_9_6_21_full_data_uninentional_od.csv")

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_poiss_full_table) %in% covariates)
sensitivity_anlys_poiss_covariate_table<-(round(sensitivity_anlys_poiss_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_poiss_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                           "Naloxone_Pharmacy_No",
                                           "Medical_Marijuana",
                                           "Recreational_Marijuana",
                                           "GSL", 
                                           "PDMP", 
                                           "Medicaid_Expansion",
                                           "Intervention", 
                                           "Number of States with DIH Prosecutions")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_poiss_covariate_table<-rbind(sensitivity_anlys_poiss_covariate_table, 
                                     sensitivity_anlys_poiss_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_poiss_covariate_table<-sensitivity_anlys_poiss_covariate_table[,-which(colnames(sensitivity_anlys_poiss_covariate_table) %in%
                                                                       c("Coefficient_Estimate", "Coefficient_Lower_Bound",
                                                                         "Coefficient_Upper_Bound", "Standard_Error"))]

colnames(sensitivity_anlys_poiss_covariate_table)<-c("Risk_Ratio_Estimates", "RR_95_CI_LB", "RR_95_CI_UB", "p-value")
head(sensitivity_anlys_poiss_covariate_table, 10)

#save the table into a CSV
# write.csv(round(sensitivity_anlys_poiss_covariate_table, 3), "./Data/coefficients_covariates_9_6_21_full_data_unintentional_od.csv")


```

## Attributable Deaths
````{r}
############################## Main Analysis: Number of Overdose Deaths Attributed to Intervention ###############################
#find the number of deaths attributable to the intervention
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_sensitivity_anlys_log<-sensitivity_anlys_poiss_data[which(sensitivity_anlys_poiss_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_sensitivity_anlys_log<-expit(-coef(sensitivity_anlys_poiss_model)["Intervention_Redefined"]*attr_deaths_anlys_sensitivity_anlys_log$Intervention_Redefined
                                    + logit(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths/attr_deaths_anlys_sensitivity_anlys_log$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-coef(sensitivity_anlys_poiss_model) - 1.96*summary(sensitivity_anlys_poiss_model)$se
coef_ub<-coef(sensitivity_anlys_poiss_model) + 1.96*summary(sensitivity_anlys_poiss_model)$se

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_sensitivity_anlys_log<-expit(-coef_lb[names(coef_lb) == "Intervention_Redefined"]*attr_deaths_anlys_sensitivity_anlys_log$Intervention_Redefined
                                       + logit(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths/attr_deaths_anlys_sensitivity_anlys_log$population))

prob_od_no_int_UB_sensitivity_anlys_log<-expit(-coef_ub[names(coef_ub) == "Intervention_Redefined"]*attr_deaths_anlys_sensitivity_anlys_log$Intervention_Redefined
                                       + logit(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths/attr_deaths_anlys_sensitivity_anlys_log$population))


#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths[time_point_index]
                          - prob_od_no_int_sensitivity_anlys_log[time_point_index]*attr_deaths_anlys_sensitivity_anlys_log$population[time_point_index])
  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_sensitivity_anlys_log[time_point_index]*attr_deaths_anlys_sensitivity_anlys_log$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_sensitivity_anlys_log$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_sensitivity_anlys_log[time_point_index]*attr_deaths_anlys_sensitivity_anlys_log$population[time_point_index])
  index<-index + 1
}

num_attr_od_sensitivity_anlys_log<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_ID)),
                                      "Time_Start" = sort(unique(attr_deaths_anlys_sensitivity_anlys_log$Time_Period_Start)),
                                      "Num_Attr_Deaths" = num_attr_od,
                                      "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                      "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths)
summary(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths)
num_attr_od_sensitivity_anlys_log$Time_Start<-as.Date(num_attr_od_sensitivity_anlys_log$Time_Start)

#compute the 95% CI for the total
sum(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths_LB)
sum(num_attr_od_sensitivity_anlys_log$Num_Attr_Deaths_UB)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_sensitivity_anlys_log<-num_attr_od_sensitivity_anlys_log %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_sensitivity_anlys_log$deaths)
summary(yearly_num_Attr_Deaths_sensitivity_anlys_log$death_lb)
summary(yearly_num_Attr_Deaths_sensitivity_anlys_log$death_ub)

```



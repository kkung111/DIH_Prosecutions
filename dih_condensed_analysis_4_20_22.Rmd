---
title: "Analysis of DIH Prosecutions: 2000 to 2019"
author: "Kelly Kung"
date: "1/22/2022"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_knit$set(echo = TRUE, root.dir = "~/OneDrive - Boston University/Research-Lok")
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Set Up
## R Code 
```{r}
#packages we need for this code file
library(ggplot2)
library(mgcv)
library(lubridate)
library(zoo)
library(tidyverse)
library(dplyr)
library(DHARMa)
library(mgcViz)
library(extrafont)
library(arm)
loadfonts()
library(stargazer)
library(ellipse)
library(dotwhisker)
library(countreg)
```

```{r}
#define functions we will need for analysis
#expit function
expit<-function(x){
  return(exp(x)/(1 + exp(x)))
}

#logit function
logit<-function(x){
  return(log(x/(1 - x)))
}
```

## Data 
```{r}
#read in data
main_analysis_data<-read.csv("./Data/full_data_set_11_29_21_unintentional.csv")

################################## set up data set ################################
#add the intervention dates and time period data
main_analysis_data$Intervention_First_Date<-as.Date(main_analysis_data$Intervention_First_Date)
main_analysis_data$Time_Period_Start<-as.Date(main_analysis_data$Time_Period_Start)
names(main_analysis_data)[which(colnames(main_analysis_data) == "sum_deaths")] <- "imputed_deaths"

################################## set up the Regions ##############################
#set up the regions according to Census: https://www.census.gov/geographies/reference-maps/2010/geo/2010-census-regions-and-divisions-of-the-united-states.html
NE.name <- c("Connecticut","Maine","Massachusetts","New Hampshire",
             "Rhode Island","Vermont","New Jersey","New York",
             "Pennsylvania")

MW.name <- c("Indiana","Illinois","Michigan","Ohio","Wisconsin",
             "Iowa","Kansas","Minnesota","Missouri","Nebraska",
             "North Dakota","South Dakota")

S.name <- c("Delaware","District of Columbia","Florida","Georgia",
            "Maryland","North Carolina","South Carolina","Virginia",
            "West Virginia","Alabama","Kentucky","Mississippi",
            "Tennessee","Arkansas","Louisiana","Oklahoma","Texas")

W.name <- c("Arizona","Colorado","Idaho","New Mexico","Montana",
            "Utah","Nevada","Wyoming","Alaska","California",
            "Hawaii","Oregon","Washington")

region.list <- list(
  Northeast=NE.name,
  Midwest=MW.name,
  South=S.name,
  West=W.name)

#initialize vector with "West" and then impute the other regions for the states
main_analysis_data$Region<-rep("West", nrow(main_analysis_data))
for(state in unique(main_analysis_data$State)){
  if(state %in% region.list$Northeast){
    main_analysis_data$Region[main_analysis_data$State == state]<-"Northeast"
  }else if(state %in% region.list$Midwest){
    main_analysis_data$Region[main_analysis_data$State == state]<-"Midwest"
  }else if(state %in% region.list$South){
    main_analysis_data$Region[main_analysis_data$State == state]<-"South"
  }
}

```

# Exploratory Data Analysis
## Overdose Deaths
```{r}
############################## EDA: Plot the Outcome and Intervention Trends ###############################
#plot the time series of the number of deaths and probability of overdose death
od_data_recent <- read.csv("./Data/od_unintentional_yearly_18_and_up_11_28_21.txt", 
                           sep = "\t", stringsAsFactors = FALSE)
od_data_recent$Deaths <- as.numeric(od_data_recent$Deaths)
od_data_recent<-od_data_recent[!is.na(od_data_recent$Year),] #delete the rows that just contains data set description info
od_data_recent<- od_data_recent %>% 
  filter(Year > 1999 & Year < 2020) %>% 
  group_by(Year) %>%
  summarise(sum_deaths = sum(Deaths, na.rm = TRUE))

# pdf("./Figures/total_od_deaths_all_paper_5_7_22_2000_2019.pdf")
# ggplot(data = od_data_recent, mapping = aes(x = Year, y = sum_deaths)) +
#   geom_line() + 
#   geom_point() +
#   labs(x = "Year", y = "Yearly Number of Unintentional Drug Overdose Deaths in the 50 U.S. States") +
#   theme(panel.background = element_rect("white"), 
#         panel.border = element_blank(), 
#         panel.grid.major = element_blank(),
#         panel.grid.minor = element_blank(), 
#         axis.line = element_line(colour = "black"),
#         axis.title=element_text(family="Times", size=12, face="bold"),
#         axis.text=element_text(family="Times",size=12)) +
#   scale_x_continuous(breaks = seq(2000, 2020, by = 2)) +
#   ylim(c(0, 62000))

# pdf("./Figures/total_od_deaths_all_paper_5_7_22_2000_2019.pdf")
ggplot(data = od_data_recent, mapping = aes(x = Year, y = sum_deaths)) +
  geom_line() + 
  geom_point() +
  labs(x = "Year", y = "Yearly Number of Unintentional Drug Overdose Deaths in the 50 U.S. States") +
  theme(panel.background = element_rect("white"), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.line = element_line(colour = "black"),
        axis.title=element_text(size=12),
        axis.text=element_text(size=12)) +
  scale_x_continuous(breaks = seq(2000, 2020, by = 2)) +
  ylim(c(0, 62000))
# dev.off()

main_analysis_data_sum <- main_analysis_data %>% 
  group_by(year = year(Time_Period_Start)) %>%
  summarise(total_deaths = sum(imputed_deaths),
            sum_pop = sum(population)/2,
            total_prop = sum(imputed_deaths)/(sum(population)/2),
            total_prop_by_100000 = 100000*sum(imputed_deaths)/(sum(population)/2))
# %>%mutate(date = as.Date(as.yearmon(year)))

#compute the percentage difference between 2000 and 2019
death_2000 <- main_analysis_data_sum$total_deaths[main_analysis_data_sum$year == 2000]
death_2019 <- main_analysis_data_sum$total_deaths[main_analysis_data_sum$year == 2019]

((death_2019 - death_2000)/death_2000)*100

```

## Intervention: DIH Prosecutions
```{r}
#plot the number of states with an intervention for each time point
#first, create a data set to find the number of states with an intervention at each time point
#initialize the data set with the start date of the time period
num_states_with_intervention<-data.frame("Start_Date" =
                                  unique((main_analysis_data$Intervention_First_Date[!is.na(main_analysis_data$Intervention_First_Date)])))
numStates<-c()

#for each time period i, we first find the states where the first intervention date occurred before i
#then, we append it to numStates
for(i in unique((num_states_with_intervention$Start_Date))){
  states_w_int<-unique(main_analysis_data$State[(main_analysis_data$Intervention_First_Date)<=i])
  numStates<-append(numStates, length(states_w_int[!is.na(states_w_int)]))
}
num_states_with_intervention$numStates<-numStates
num_states_with_intervention$Start_Date <- as.Date(num_states_with_intervention$Start_Date)
num_states_with_intervention <- rbind(data.frame("Start_Date" = c(as.Date("2000-01-01"),
                                                                  as.Date("2019-12-31")),
                                                 "numStates" = c(0, max(num_states_with_intervention$numStates))),
                                      num_states_with_intervention)
num_states_with_intervention <- num_states_with_intervention %>% 
  arrange(Start_Date) %>%
  mutate(lag_numStates = lag(numStates))

num_states_with_intervention <- num_states_with_intervention %>%
  pivot_longer( c("lag_numStates", "numStates"), "numStates")

# pdf("Figures/num_states_with_intervention_5_7_22.pdf")
# ggplot(num_states_with_intervention, aes(x = Start_Date, y = value, group = 1)) +
#   geom_line() +
#   # geom_point(num_states_with_intervention[num_states_with_intervention$numStates == "numStates",],
#   #            mapping = aes(x = Start_Date, y = value, group = 1), size = 1) +
#   labs(x = "Year", y = "Cumulative Number of States to have DIH Prosecutions") +
#   theme(axis.text=element_text(family="Times",size=12),
#         axis.title=element_text(family="Times", size=12, face="bold"),
#         panel.border = element_blank(), 
#         panel.grid.major = element_blank(),
#         panel.grid.minor = element_blank(), 
#         axis.line = element_line(colour = "black"),
#         axis.text.x = element_text(family="Times", size=12),
#         panel.background = element_rect("white")) +
#   scale_x_date(date_labels="%Y", breaks = seq(as.Date("2000-01-01"), as.Date("2018-01-01"), by = "2 years"))

# pdf("Figures/num_states_with_intervention_5_7_22.pdf")
ggplot(num_states_with_intervention, aes(x = Start_Date, y = value, group = 1)) +
  geom_line() +
  # geom_point(num_states_with_intervention[num_states_with_intervention$numStates == "numStates",],
  #            mapping = aes(x = Start_Date, y = value, group = 1), size = 1) +
  labs(x = "Year", y = "Cumulative Number of States to have DIH Prosecutions") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=12),
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.line = element_line(colour = "black"),
        axis.text.x = element_text( size=12),
        panel.background = element_rect("white")) +
  scale_x_date(date_labels="%Y", breaks = seq(as.Date("2000-01-01"), as.Date("2018-01-01"), by = "2 years"))

# dev.off()
```

## Policy Dates
```{r}
#add the intervention variable as a measure of number of states with DIH prosecution
main_analysis_data <- main_analysis_data %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention = sum(Intervention_Redefined))

############################# Look at the policy dates #######################
policy_dates <- main_analysis_data %>% 
  group_by(State) %>%
  summarise(unique(format(Intervention_First_Date, "%Y-%m")),
            unique(format(as.Date(Naloxone_Pharmacy_Yes_First_Date), "%Y-%m")),
            unique(format(as.Date(Naloxone_Pharmacy_No_First_Date), "%Y-%m")),
            unique(format(as.Date(Medical_Marijuana_First_Date), "%Y-%m")),
            unique(format(as.Date(Recreational_Marijuana_First_Date), "%Y-%m")),
            unique(format(as.Date(PDMP_First_Date), "%Y-%m")),
            unique(format(as.Date(GSL_First_Date), "%Y-%m")),
            unique(format(as.Date(Medicaid_Expansion_First_Date), "%Y-%m")))
names(policy_dates) <- c("State", "DIH Prosecutions", "NAL: Pharmacists Yes",
                         "NAL: Pharmacists No", "MML", "RML", "PDMP", "GSL",
                         "Medicaid")
# write.csv(policy_dates, "./Data/policy_dates_11_29_21.csv")

```

## Create Plot of Number of DIH Prosecutions Per State
```{r}
#create a plot for each state to see how many prosecution media alerts there are per 6 month period
#read in the prosecution media alert data
prosecution_data<-read.csv("./Data/dih_prosecutions_9_6_21.csv")

#data cleaning
prosecution_data<-prosecution_data %>% 
  mutate(Date = as.Date(Date.charged, "%m/%d/%Y")) %>%
  mutate(State = ifelse(State.Filed == "pennsylvania", "Pennsylvania", State.Filed),
         State = ifelse(State.Filed == "Virginia ", "Virginia", State)) %>%
  mutate(deceased_age = ifelse(!is.na(as.numeric(Deceased.s.Age)), as.numeric(Deceased.s.Age), 9999)) %>%
  filter(!is.na(Date), 
         State.Filed != "No Info", 
         State.Filed != "No info", 
         State.Filed != "No Info ",
         deceased_age >= 18,
         State != "" ) %>%
  mutate(deceased_age = ifelse(deceased_age == 9999, NA, deceased_age))

#clean up the data by looking at the link to the article
prosecution_data$Date[prosecution_data$Date == "2026-08-01"] <- as.Date("2016-02-15", "%Y-%m-%d")

#change the states into Character instead of factor
prosecution_data$State<-as.character(prosecution_data$State)
#see how many prosecution data points there are for each state
table(prosecution_data$State)

#there are some repeated cases depending on victim so extract distinct cases
prosecution_data_unique <- prosecution_data %>%
  group_by(State) %>%
  distinct(Accused.Name, Date, .keep_all = T)
table(prosecution_data_unique$State)

#change date charged into Date object
prosecution_data_unique$Date<-mdy(prosecution_data_unique$Date.charged)

#group the data into six month periods
prosecution_data_unique<-prosecution_data_unique %>% 
  mutate(six_month_pd = lubridate::floor_date(Date , "6 months" ))

prosecution_grouped <- prosecution_data_unique %>% 
  #filter to dates after 2000 and dates before 2020
  filter(year(six_month_pd) >= 2000 & year(six_month_pd) <= 2019) %>%
  group_by(State, six_month_pd) %>% 
  #for each state, for each six month period, count the number of DIH prosecutions
  summarise(num_dih = n()) %>% 
  #ONLY IF GROUPS
  #label the groups according to zero, low, or high
  mutate(group = ifelse(num_dih == 0, "zero", ifelse(num_dih >= 5, "high", "low"))) %>%
  ungroup() %>%
  #have to add in a row for hawaii because its not in the prosecution dataset
  add_row(State = "Hawaii", six_month_pd = as.Date("2000-01-01"), num_dih = 0, group = "zero")

#look at table of ages of the victims
table(as.numeric(prosecution_data_unique$Deceased.s.Age))
table(as.numeric(prosecution_data_unique$Deceased.s.Age))/sum(!is.na(as.numeric(prosecution_data_unique$Deceased.s.Age)))

#view table of people in each age category, based on prosecution data
prosecution_data_unique <- prosecution_data_unique %>%
  mutate(Deceased.s.Age = as.numeric(Deceased.s.Age),
         age_groups = ifelse(Deceased.s.Age <= 17, "0_17", 
                             ifelse(Deceased.s.Age >= 18 & Deceased.s.Age <= 34, "18_34", 
                                    ifelse(Deceased.s.Age >= 35 & Deceased.s.Age <= 54, "35_54", 
                                           ifelse(Deceased.s.Age >= 55 & Deceased.s.Age <= 64, "55_64", "65_plus")))))

table(prosecution_data_unique$age_groups, exclude = "NA")

#we compute the final group for each state by seeing if it ever hits high or low
#if it hits a higher level, then it will remain defined in that higher level
prosecution_grouped_final <- prosecution_grouped %>%  
  group_by(State) %>% 
  summarise(final_gp = ifelse(sum(group == "high") > 0, "high", ifelse(sum(group == "low")> 0, "low", "zero"))) 

#plot of the number of states in each zero/low/high category
ggplot(prosecution_grouped_final, aes(final_gp)) + 
  geom_bar() + 
  labs(title = "Number of States by DIH prosecution Category, with Low = [1,5]") + 
  geom_text(aes(label = ..count..), stat = "count", vjust = -.75)

#number of DIH prosecutions per six month for each state
# pdf("Figures/num_dih_per_six_month_pd_by_state_11_29_21.pdf")
ggplot(prosecution_grouped, aes(x = six_month_pd, y = num_dih)) + 
  geom_bar(stat = "identity") + 
  facet_wrap(~State) + 
  labs(y = "Number of DIH Prosecutions Reported in the Media",
       x = "Date") + 
  theme(axis.text.x = element_text(hjust = 1, size = 6, family = "Times", angle = 30),
        axis.text.y = element_text(size = 6, family = "Times"),
        axis.title = element_text(size = 10, face = "bold", family = "Times"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size=8),
        panel.background = element_rect("white"),
        legend.position = "bottom")
# dev.off()

# write.csv(prosecution_grouped, "./Data/num_dih_per_six_month_pd_by_state_11_12_21.csv")


```

# Main Analysis: Effect of At Least One DIH Prosecution Report in Media on Unintentional Overdose Deaths

## Analysis
```{r, results = "asis"}
############################## Run Model with Spline Time Effects by Region ###############################
#model that we will be using for the main analysis
#cr is used for cubic regression spline -- we are smoothing time effects by region
#run the analysis for all the states
main_analysis_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                           s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                           Naloxone_Pharmacy_Yes_Redefined +
                           Naloxone_Pharmacy_No_Redefined +
                           Medical_Marijuana_Redefined +
                           Recreational_Marijuana_Redefined +
                           GSL_Redefined +
                           PDMP_Redefined +
                           Medicaid_Expansion_Redefined +
                           Intervention_Redefined + 
                           num_states_w_intervention,
                         data = main_analysis_data, family = "binomial")



#summary output of the model
stargazer(main_analysis_model, type = "html", dep.var.labels = c("Unintentional Overdose Deaths"))
```

## Sandwich Estimator
```{r}
#here, we estimate the variance-covariance matrix through the sandwich estimator
#we create a function so that we don't have to keep writing the code:
#cov_data is such that rows are state-time combinations and columns are the different policy measures
#coef_values need to be in order of the columns of cov_data
#z_value is the z-value that corresponds to the CI. We default to 95% CI so we default to 1.96

# compute_sd_and_CI <- function(cov_data, population, observed_od, predicted_prob_od, coef_values, z_value = 1.96,
#                               k,
#                               print_full_cov = FALSE){
#   #compute the predicted number of OD
#   pred_num_od <- population*predicted_prob_od
#   #compute the square of observed number of OD minus predicted number of OD
#   obs_minus_pred_num_od_sq <- (observed_od - pred_num_od)^2
# 
#   #estimate the denominator term: sum_{s,t} n_{s,t}z_{s,t}*p_{s,t}*(1-p_{s,t})*z_{s,t}^T
#   #initialize the matrix so that we can add the terms to it
#   denominator <- matrix(0, nrow = ncol(cov_data), ncol = ncol(cov_data))
#   #estimate middle term: sum_{s,t} x_{s,t}*mean(prop_obs_od - prob_od)^2*x_{s,t}^T
#   middle_term <- matrix(0, nrow = ncol(cov_data), ncol = ncol(cov_data))
#   for(row in 1:nrow(cov_data)){
#     #note here: we first take t(cov_data[row]) since in the matrix, we have the state-time indices in the rows
#     #and the covariates in the columns
#     denominator <- denominator +
#       # population[row]*t(cov_data[row,])%*%as.matrix(cov_data[row,])
#       #don't need as.matrix() for the first term because transpose function t() automatically changes it to a matrix type
#       population[row]*t(cov_data[row,])%*%predicted_prob_od[row]%*%(1-predicted_prob_od[row])%*%as.matrix(cov_data[row,])
# 
#     middle_term <- middle_term + t(cov_data[row,])%*%obs_minus_pred_num_od_sq[row]%*%as.matrix(cov_data[row,])
#   }
# 
#   #variance-covariance = denominator^{-1}*middle_term*t(denominator^{-1})
#   #here, solve computes the inverse of the matrix
#   solve_denom <- solve(denominator)
#   var_cov <- solve_denom%*%middle_term%*%t(solve_denom)*(nrow(cov_data)/(nrow(cov_data) - k))
#   #we obtain the standard deviations by taking the square root of the diagonal of the variance-covariance matrix.
#   sd_of_coefficients <- sqrt(diag(var_cov))
# 
#   #find the CI for the coefficients
#   lb_coef <- coef_values - z_value*(sd_of_coefficients)
#   ub_coef <- coef_values + z_value*(sd_of_coefficients)
# 
#   return_data_set <- data.frame(lb_coef, coef_values, ub_coef,
#              exp_lb = exp(lb_coef), exp_coef = exp(coef_values),
#              exp_ub = exp(ub_coef), sd_coef = sd_of_coefficients)
# 
#   if(print_full_cov){
#     return(list(return_data_set = return_data_set, var_cov = var_cov))
#   }else{
#     return(return_data_set)
#   }
# }


compute_sd_and_CI <- function(cov_data, observed_y, coef_values, z_value = 1.96,
                              k,
                              print_full_cov = FALSE){
  middle_term <- matrix(0, nrow = ncol(cov_data), ncol = ncol(cov_data))
  for(i in 1:nrow(cov_data)){
    middle_term <- middle_term + tcrossprod(as.matrix(cov_data[i,]))*
      as.numeric((observed_y[i] - (t(as.matrix(cov_data[i,]))%*%coef_values))^2)
  }
  # var_cov <- solve(crossprod(cov_data))%*%(t(cov_data)%*%tcrossprod(observed_y - cov_data%*%coef_values,
  #                                                                   observed_y -
  #                                                                     cov_data%*%coef_values)%*%cov_data)%*%solve(crossprod(cov_data))

  var_cov <- solve(crossprod(cov_data))%*%as.matrix(middle_term)%*%solve(crossprod(cov_data))*(nrow(cov_data)/(nrow(cov_data) - k))
  #we obtain the standard deviations by taking the square root of the diagonal of the variance-covariance matrix.
  sd_of_coefficients <- sqrt(diag(var_cov))

  #find the CI for the coefficients
  lb_coef <- coef_values - z_value*(sd_of_coefficients)
  ub_coef <- coef_values + z_value*(sd_of_coefficients)

  return_data_set <- data.frame(lb_coef, coef_values, ub_coef,
             exp_lb = exp(lb_coef), exp_coef = exp(coef_values),
             exp_ub = exp(ub_coef), sd_coef = sd_of_coefficients)


  if(print_full_cov){
    return(list(return_data_set = return_data_set, var_cov = var_cov))
  }else{
    return(return_data_set)
  }
}

#compute the full dataset including basis functions
full_df_w_basis_functions <- as.matrix(data.frame(predict(main_analysis_model, type = "lpmatrix")))

main_analysis_data$prop_dead <- main_analysis_data$imputed_deaths/main_analysis_data$population
#estimate the 95% CI and SD
coefficient_values <- coef(main_analysis_model)
#type = "response" to get the estimated probabilities
main_analysis_sd_and_ci <- compute_sd_and_CI(full_df_w_basis_functions, 
                                             logit(main_analysis_data$prop_dead),
                                             coefficient_values,
                                             k = ncol(full_df_w_basis_functions))
main_analysis_sd_and_ci
```

## Plots 
```{r}
#check diagnostics
gam.check(main_analysis_model)

main_analysis_model_object <- getViz(main_analysis_model)

midwest_plot <- plot(sm(main_analysis_model_object, 1)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + 
  theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for Midwest") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005", "2010", "2015"))  +
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

northeast_plot <- plot(sm(main_analysis_model_object,2)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + 
  theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for Northeast") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005", "2010", "2015"))+
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

south_plot <- plot(sm(main_analysis_model_object, 3)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + 
  theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for South") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005","2010", "2015"))+
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

west_plot <- plot(sm(main_analysis_model_object, 4)) +
  l_fitLine() +
  l_ciLine(mul = 5, linetype = 2)  + theme_classic() +
  labs(x = "Time Period", y = "Smoothed Time Effects for West") +
  scale_x_continuous(breaks=c(1,11,21,31), 
                     labels=c("2000", "2005", "2010", "2015"))+
  theme(text=element_text(family="Times",size=10),
        title = element_text(family="Times", size=10, face = "bold"),
        panel.background = element_rect("white")) +
  ylim(c(-1,1.2))

# pdf("./Figures/time_smoothed_effects_9_6_21.pdf")
gridPrint(midwest_plot, northeast_plot, south_plot, west_plot, ncol = 2)
# dev.off()

total_pop <- main_analysis_data %>% 
  group_by(year = year(Time_Period_Start), State) %>% 
  summarise(pop = unique(population)) %>%
  group_by(year) %>% 
  summarise(sum(pop))

main_analysis_data %>% 
  group_by(year(Time_Period_Start)) %>% 
  summarise(sum_deaths = sum(imputed_deaths)*100000) %>% 
  mutate(sum_deaths/total_pop$`sum(pop)`)

main_analysis_data %>%
  group_by(State, year(Time_Period_Start)) %>%
  summarise(death_rate = (sum(imputed_deaths)/unique(population))*100000) %>%
  group_by(State) %>%
  summarise(first_death_rate = death_rate[1],
            last_death_rate = death_rate[20]) %>%
  mutate(range_death_rate = last_death_rate - first_death_rate) %>% 
  filter(range_death_rate == min(range_death_rate) | range_death_rate == max(range_death_rate))
  

# #summarize the DIH dates
# main_analysis_data %>% 
#   group_by(Time_Period_Start) %>%
#   summarise(prop_w_intervention = mean(Intervention_Redefined > 0)) %>%
#   View()

#create a data frame to store the results and compute the confidence intervals
#initialize the columns
main_analysis_plot_table<-data.frame(State = main_analysis_data$State)
main_analysis_plot_table$Fitted<-rep(NA, nrow(main_analysis_plot_table))
main_analysis_plot_table$Observed<-rep(NA, nrow(main_analysis_plot_table))
main_analysis_plot_table$Time<-main_analysis_data$Time_Period_ID
main_analysis_plot_table$Time_Date<-main_analysis_data$Time_Period_Start
main_analysis_plot_table$Intervention_Date<-main_analysis_data$Intervention_First_Date

#we want to compare the fitted probability of overdose death and the observed values to see how the model does as a goodness of fit visual test
for(i in unique(main_analysis_plot_table$State)){
  #for each state, we first subset the main analysis data to only look at the data for that state
  index_of_state<-which(main_analysis_plot_table$State == i)
  #impute the fitted and observed probability of overdose death for the state
  main_analysis_plot_table$Fitted[index_of_state]<-fitted(main_analysis_model)[index_of_state]
  main_analysis_plot_table$Observed[index_of_state] <- (main_analysis_data$imputed_deaths[main_analysis_data$State == i]/main_analysis_data$population[main_analysis_data$State == i])
}

```

```{r, fig.height=8, fig.width=8}
#plot to compare the fitted values vs observed deaths
pdf("./Figures/GAM_fitted_vs_actual_by_Region_5_10_22_with_int_date_full_data.pdf")
# ggplot(data = main_analysis_plot_table, aes(x = Time_Date, y = Observed*100000, group = 1,
#                                             color = "Observed")) +
#   geom_line(aes(color = "Observed"))+ geom_point(aes(color = "Observed"), size = .5, alpha = .5) +
#   geom_line(data = main_analysis_plot_table, aes(x = Time_Date, y = Fitted*100000, group = 1,
#                                                  color = "Estimate")) +
#   geom_point(data = main_analysis_plot_table, aes(x = Time_Date, y = Fitted*100000,
#                                                   color = "Estimate"),
#              size = .5, alpha = .5) +
#   scale_color_manual(values = c("red", "blue")) + 
#   scale_linetype_manual(values = c("dashed", "solid")) + 
#   geom_vline(main_analysis_plot_table, mapping = aes(xintercept = Intervention_Date)) +
#   facet_wrap(facets = vars(State), scales = "free_y", ncol = 5) +
#   theme(axis.text.x = element_text(hjust = 1, size = 6, family = "Times"),
#         axis.text.y = element_text(size = 6, family = "Times"),
#         axis.title=element_text(size = 10,face = "bold", family = "Times"),
#         panel.grid.major = element_blank(),
#         panel.grid.minor = element_blank(),
#         strip.background = element_blank(),
#         strip.text = element_text(size=8),
#         panel.background = element_rect("white"),
#         legend.position = "bottom") +
#   labs(x = "Year", y = "Unintentional Drug Overdose Death Rates per 100,000 People",
#        color = "")

ggplot(data = main_analysis_plot_table) +
  geom_line(aes(color = "Observed", x = Time_Date, y = Observed*100000, group = 1,
                linetype = "Observed"))+ 
  # geom_point(aes(color = "Observed"), size = .5, alpha = .5) +
  geom_line(data = main_analysis_plot_table, aes(x = Time_Date, y = Fitted*100000, group = 1,
                                                 color = "Estimate", linetype = "Estimate")) +
  # geom_point(data = main_analysis_plot_table, aes(x = Time_Date, y = Fitted*100000,
  #                                                 color = "Estimate"),
  #            size = .5, alpha = .5) +
  scale_color_manual(values = c("red", "blue")) + 
  scale_linetype_manual(values = c("dashed", "solid")) + 
  geom_vline(main_analysis_plot_table, mapping = aes(xintercept = Intervention_Date)) +
  facet_wrap(facets = vars(State), scales = "free_y", ncol = 5) +
  theme(axis.text.x = element_text(hjust = 1, size = 6),
        axis.text.y = element_text(size = 7),
        axis.title=element_text(size = 12),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(size=8),
        panel.background = element_rect("white"),
        legend.position = "bottom") +
  labs(x = "Year", y = "Unintentional Drug Overdose Death Rates per 100,000 People",
       color = "", linetype = "")
dev.off()
```

```{r}
#diagnostic plots to check model
residTab <- data.frame(logit_fitted_vals = logit(fitted(main_analysis_model)),
                       resids = residuals(main_analysis_model))
# pdf("./Figures/deviance_resids_1_18_22.pdf")
ggplot(residTab, aes(x = logit_fitted_vals, y = resids)) +
  geom_point() +
  theme(text = element_text(size = 10, family = "Times"),
        title = element_text(size = 10, family = "Times", face = "bold"),
        panel.background = element_rect(fill = "white", color = "black")) +
  # theme_classic() +
  labs(x = "Logistic Function of Fitted Values", y = "Deviance Residuals")
# dev.off()

pred_vals <- predict(main_analysis_model)
resids <- resid(main_analysis_model, type = "response")
# pdf("./Figures/binned_resids_plot_9_6_21.pdf")
par(font.lab = 2)
par(family = "Times")
binnedplot(pred_vals, resids, main = "", 
           xlab = "Average Logistic Function of Fitted Values",
           ylab = "Average Residuals")
# dev.off()

```

## Compile Results
```{r}
############################## Main Analysis: Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
main_analysis_full_table<-main_analysis_sd_and_ci
#check to see how the table looks
head(main_analysis_full_table)

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined", 
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(main_analysis_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(main_analysis_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(main_analysis_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(main_analysis_full_table)[i]<-substr(rownames(main_analysis_full_table)[i], start = 6,
                                                    stop = nchar(rownames(main_analysis_full_table)[i]))

    }else if(rownames(main_analysis_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(main_analysis_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(main_analysis_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(main_analysis_full_table)[i]<-paste("Smoothed Time for Region ",
                                                   substr(rownames(main_analysis_full_table)[i], start = 36,
                                                          stop = nchar(rownames(main_analysis_full_table)[i])),
                                                   sep = "")

    }
  }
}

head(main_analysis_full_table)
tail(main_analysis_full_table)

#save the table into a CSV
# write.csv(round(main_analysis_full_table,5), "./Data/coefficients_GAM_1_18_22_full_data_uninentional_od.csv")

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(main_analysis_full_table) %in% covariates)
main_analysis_covariate_table<-(round(main_analysis_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(main_analysis_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                           "Naloxone_Pharmacy_No",
                                           "Medical_Marijuana",
                                           "Recreational_Marijuana",
                                           "GSL", 
                                           "PDMP", 
                                           "Medicaid_Expansion",
                                           "Intervention", 
                                           "Number of States with DIH Prosecutions")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
main_analysis_covariate_table<-rbind(main_analysis_covariate_table, 
                                     main_analysis_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
main_analysis_covariate_table<-main_analysis_covariate_table[,-which(colnames(main_analysis_covariate_table) %in%
                                                                       c("lb_coef", "coef_values",
                                                                         "ub_coef"))]

colnames(main_analysis_covariate_table)<-c("RR_95_CI_LB", "Risk_Ratio_Estimates", "RR_95_CI_UB", "standard_error")
head(main_analysis_covariate_table, 10)

#save the table into a CSV
write.csv(round(main_analysis_covariate_table, 3), "./Data/coefficients_covariates_4_20_22_full_data_unintentional_od.csv")


```

## Attributable Deaths
````{r}
############################## Main Analysis: Number of Overdose Deaths Attributed to Intervention ###############################
#find the number of deaths attributable to the intervention
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_main_analysis<-main_analysis_data[which(main_analysis_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_main_analysis<-expit(-coef(main_analysis_model)["Intervention_Redefined"]*
                                      attr_deaths_anlys_main_analysis$Intervention_Redefined
                                    + logit(attr_deaths_anlys_main_analysis$imputed_deaths/
                                              attr_deaths_anlys_main_analysis$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-main_analysis_full_table$lb_coef
coef_ub<-main_analysis_full_table$ub_coef
names(coef_lb) <- names(coef_ub) <- rownames(main_analysis_full_table)

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_main_analysis<-expit(-coef_lb[names(coef_lb) ==
                                                  "Intervention_Redefined"]*attr_deaths_anlys_main_analysis$Intervention_Redefined
                                       + logit(attr_deaths_anlys_main_analysis$imputed_deaths/attr_deaths_anlys_main_analysis$population))

prob_od_no_int_UB_main_analysis<-expit(-coef_ub[names(coef_ub) ==
                                                  "Intervention_Redefined"]*attr_deaths_anlys_main_analysis$Intervention_Redefined
                                       + logit(attr_deaths_anlys_main_analysis$imputed_deaths/attr_deaths_anlys_main_analysis$population))


#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(attr_deaths_anlys_main_analysis$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_main_analysis$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_main_analysis$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_main_analysis$imputed_deaths[time_point_index]
                          - prob_od_no_int_main_analysis[time_point_index]*
                            attr_deaths_anlys_main_analysis$population[time_point_index])
  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_main_analysis$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_main_analysis[time_point_index]*
                               attr_deaths_anlys_main_analysis$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_main_analysis$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_main_analysis[time_point_index]*
                               attr_deaths_anlys_main_analysis$population[time_point_index])
  index<-index + 1
}

num_attr_od_main_analysis<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_main_analysis$Time_Period_ID)),
                                      "Time_Start" = sort(unique(attr_deaths_anlys_main_analysis$Time_Period_Start)),
                                      "Num_Attr_Deaths" = num_attr_od,
                                      "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                      "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_main_analysis$Num_Attr_Deaths)
summary(num_attr_od_main_analysis$Num_Attr_Deaths)
num_attr_od_main_analysis$Time_Start<-as.Date(num_attr_od_main_analysis$Time_Start)

#compute the 95% CI for the total
sum(num_attr_od_main_analysis$Num_Attr_Deaths_LB)
sum(num_attr_od_main_analysis$Num_Attr_Deaths_UB)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_main_analysis<-num_attr_od_main_analysis %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_main_analysis$deaths)
summary(yearly_num_Attr_Deaths_main_analysis$death_lb)
summary(yearly_num_Attr_Deaths_main_analysis$death_ub)

```

# Sensitivity Analysis 1: Confounding by Indication
## Analysis

```{r, results = "asis"}
############################## Sensitivity Analysis 1: Confounding by Indication Analysis ###############################
#create a variable that is equal to 1 just before intervention
#initialize the column to all zeros
main_analysis_data$justBeforeIntervention<-0

#for each state, we first subset the data so it only contains the state's data
for(state in unique(main_analysis_data$State)){
  state_data<-main_analysis_data[main_analysis_data$State == state,]
  #then, we find the first time point where intervention occurred
  index_first_intervention<-which(state_data$Intervention_Redefined>0)[1]
  #impute a 1 for the time point right before when intervention first occurs
  main_analysis_data$justBeforeIntervention[main_analysis_data$State == state][index_first_intervention-1]<-1
}

#subset the data so that we are only looking at the periods before the intervention occurs for each state
sensitivity_anlys_conf_by_indication_data<-data.frame()
for(state in unique(main_analysis_data$State)){
  state_data<-main_analysis_data[main_analysis_data$State == state,]
  #we don't include these states because Georgia and Ohio have intervention in 2000
  #Hawaii is in this list because it doesn't have an intervention, so we will encounter an error
  #if the state is Hawaii, we'll go to the else if condition
  if(!(state %in% c("Hawaii", "Georgia", "Ohio"))){
    #look for the index where it is just before the intervention
    index_first_intervention<-which(state_data$justBeforeIntervention>0)
    #add the rows that occur before the intervention to the sensitivity analysis data
    sensitivity_anlys_conf_by_indication_data<-rbind(sensitivity_anlys_conf_by_indication_data, state_data[1:(index_first_intervention),])

  }else if(state == "Hawaii"){
    #Hawaii doesn't have an intervention, so we want to include all the rows of Hawaii
    sensitivity_anlys_conf_by_indication_data<-rbind(sensitivity_anlys_conf_by_indication_data, state_data)
  }
}

#run the analysis for the sensitivity analysis
sensitivity_anlys_conf_by_indication<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                            s(Time_Period_ID, bs = "cr", by = as.factor(Region), k = 7) +
                                            Naloxone_Pharmacy_Yes_Redefined +
                                            Naloxone_Pharmacy_No_Redefined +
                                            Medical_Marijuana_Redefined +
                                            Recreational_Marijuana_Redefined +
                                            GSL_Redefined +
                                            PDMP_Redefined +
                                            Medicaid_Expansion_Redefined +
                                            justBeforeIntervention + 
                                            num_states_w_intervention,
                                          data = sensitivity_anlys_conf_by_indication_data, family = "binomial")

stargazer(sensitivity_anlys_conf_by_indication, type = "html", dep.var.labels = "Unintentional Overdose Deaths")

```

## Sandwich Estimator
```{r}
sensitivity_anlys_conf_by_indication_data$prop_dead <- sensitivity_anlys_conf_by_indication_data$imputed_deaths/
  sensitivity_anlys_conf_by_indication_data$population
#compute the full dataset including basis functions
sensitivity_anlys_conf_by_indication_w_basis_functions <- as.matrix(predict(sensitivity_anlys_conf_by_indication, type = "lpmatrix"))

#estimate the 95% CI and SD
sensitivity_anlys_conf_by_indication_coefficient_values <- coef(sensitivity_anlys_conf_by_indication)
#type = "response" to get the estimated probabilities
sensitivity_anlys_conf_by_indication_pred_prob <- predict(sensitivity_anlys_conf_by_indication, 
                                                          newdata = sensitivity_anlys_conf_by_indication_data, type = "response")
sensitivity_anlys_conf_by_indication_sd_and_ci <- compute_sd_and_CI(sensitivity_anlys_conf_by_indication_w_basis_functions, 
                                             logit(sensitivity_anlys_conf_by_indication_data$prop_dead),
                                             sensitivity_anlys_conf_by_indication_coefficient_values,
                                             k = ncol(sensitivity_anlys_conf_by_indication_w_basis_functions))
round(sensitivity_anlys_conf_by_indication_sd_and_ci, 3)
```

## Compile Results
```{r}
############################## Sensitivity Analysis 1: Make Data Frame of Results and 95% CI ###############################
#store the coefficients of all the terms into a table
sensitivity_anlys_conf_by_indication_full_table<-sensitivity_anlys_conf_by_indication_sd_and_ci
head(sensitivity_anlys_conf_by_indication_full_table)

#store a vector of the covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "justBeforeIntervention",
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_conf_by_indication_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_conf_by_indication_full_table)[i] %in% covariates)){

    #we see if it is a state effect
    if(substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_conf_by_indication_full_table)[i]<-substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i],
                                                                           start = 6,
                                                                           stop =
                                                                    nchar(rownames(sensitivity_anlys_conf_by_indication_full_table)[i]))

    }else if(rownames(sensitivity_anlys_conf_by_indication_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_conf_by_indication_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i], start = 1, stop = 35) ==
             "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_conf_by_indication_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                      substr(rownames(sensitivity_anlys_conf_by_indication_full_table)[i],
                                                                             start = 36,
                                                                             stop = 
                                                                      nchar(rownames(sensitivity_anlys_conf_by_indication_full_table)[i])),
                                                                          sep = "")


    }
  }
}


head(sensitivity_anlys_conf_by_indication_full_table)
tail(sensitivity_anlys_conf_by_indication_full_table)

#export the sensitivity analysis confounding by indication full table of estimates as CSV
# write.csv(round(sensitivity_anlys_conf_by_indication_full_table,3), "./Data/coefficients_JustBeforeInd_9_6_21_full_data.csv")

#export out a table with just the covariates:
#find the rows in the full table which contain estimates for the covariates and extract them
covariate_Index<-which(rownames(sensitivity_anlys_conf_by_indication_full_table) %in% covariates)
sensitivity_anlys_conf_by_indication_covariate_table<-(round(sensitivity_anlys_conf_by_indication_full_table[covariate_Index,],5))

#rename these variables so they look nicer
rownames(sensitivity_anlys_conf_by_indication_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                                  "Naloxone_Pharmacy_No",
                                                                  "Medical_Marijuana",
                                                                  "Recreational_Marijuana",
                                                                  "GSL", 
                                                                  "PDMP",
                                                                  "Medicaid_Expansion",
                                                                  "Just Before Indicator", 
                                                                  "Number of States w DIH Prosecution")

#put the covariate rows on top and the rest of the data at the bottom
sensitivity_anlys_conf_by_indication_covariate_table<-rbind(sensitivity_anlys_conf_by_indication_covariate_table,
                                                            sensitivity_anlys_conf_by_indication_full_table[-covariate_Index,])

#extract the columns that gives the odds ratio estimates
sensitivity_anlys_conf_by_indication_covariate_table<-sensitivity_anlys_conf_by_indication_covariate_table[,
       -which(colnames(sensitivity_anlys_conf_by_indication_covariate_table) %in% 
                c("lb_coef",
                  "coef_values",
                  "ub_coef"))]
colnames(sensitivity_anlys_conf_by_indication_covariate_table)<-c("RR_95_CI_LB", "Risk_Ratio_Estimates","RR_95_CI_UB", "standard_error")
head(sensitivity_anlys_conf_by_indication_covariate_table, 10)

#export the covariate table into a CSV file
# write.csv(round(sensitivity_anlys_conf_by_indication_covariate_table,3), "./Data/covariates_just_before_intervention_9_6_21_full_data.csv")

```

# Sensitivity Analysis 2: Effect of DIH Prosecutions on Unintentional Overdose Deaths, Assuming Effect Lasts for Two Years
## Analysis
```{r, results = "asis"}
########### Sensitivity Analysis 2: Two Year Intervention Effect ######################
#create a plot for each state to see how many prosecution media alerts there are per 6 month period
#read in the prosecution media alert data
prosecution_data<-read.csv("./Data/dih_prosecutions_9_6_21.csv")

#data cleaning
prosecution_data<-prosecution_data %>% 
  mutate(Date = as.Date(Date.charged, "%m/%d/%Y")) %>%
  mutate(State = ifelse(State.Filed == "pennsylvania", "Pennsylvania", State.Filed),
         State = ifelse(State.Filed == "Virginia ", "Virginia", State)) %>%
  filter(!is.na(Date), State.Filed != "No Info", State.Filed != "No info", State.Filed != "No Info ",
         State != "")

#clean up the data by looking at the link to the article
prosecution_data$Date[prosecution_data$Date == "2026-08-01"] <- as.Date("2016-02-15", "%Y-%m-%d")

#change the states into Character instead of factor
prosecution_data$State<-as.character(prosecution_data$State)
#see how many prosecution data points there are for each state
table(prosecution_data$State)

#there are some repeated cases depending on victim
prosecution_data_unique <- prosecution_data %>%
  group_by(State) %>%
  distinct(Accused.Name, Date, .keep_all = T)
table(prosecution_data_unique$State)

#change date charged into Date object
prosecution_data_unique$Date<-mdy(prosecution_data_unique$Date.charged)

#group the data into six month periods
prosecution_data_unique<-prosecution_data_unique %>% 
  mutate(six_month_pd = lubridate::floor_date(Date , "6 months" ))

# #######ONLY IF GROUPS######
# prosecution_grouped <- prosecution_data_unique %>% 
#   #filter to dates after 2000 and dates before 2020
#   filter(year(six_month_pd) >= 2000 & year(six_month_pd) <= 2019) %>%
#   group_by(State, six_month_pd) %>% 
#   #for each state, for each six month period, count the number of DIH prosecutions
#   summarise(num_dih = n()) %>% 
#   #label the groups according to zero, low, or high
#   mutate(group = ifelse(num_dih == 0, "zero", ifelse(num_dih >= 5, "high", "low"))) %>%
#   ungroup() %>%
#   #have to add in a row for hawaii because its not in the prosecution dataset
#   add_row(State = "Hawaii", six_month_pd = as.Date("2000-01-01"), num_dih = 0, group = "zero")
# 
# #we compute the final group for each state by seeing if it ever hits high or low
# prosecution_grouped_final <- prosecution_grouped %>%  
#   group_by(State) %>% 
#   summarise(final_gp = ifelse(sum(group == "high") > 0, "high", ifelse(sum(group == "low")> 0, "low", "zero"))) 
# 
# ggplot(prosecution_grouped_final, aes(final_gp)) + 
#   geom_bar() + 
#   labs(title = "Number of States by DIH prosecution Category, with Low = [1,5]") + 
#   geom_text(aes(label = ..count..), stat = "count", vjust = -.75)
# 
# #number of DIH prosecutions per six month for each state
# # pdf("Figures/num_dih_per_six_month_pd_by_state_11_12_21.pdf")
# ggplot(prosecution_grouped, aes(x = six_month_pd, y = num_dih)) + 
#   geom_bar(stat = "identity") + 
#   facet_wrap(~State) + 
#   theme(axis.text.x = element_text(angle = 30, size = 5))
# # dev.off()
# 
# # write.csv(prosecution_grouped, "./Data/num_dih_per_six_month_pd_by_state_11_12_21.csv")
# 
#count the number of prosecution media alerts in each six month period
#we also get the first and last date of prosecution in time period
prosecution_data_by_six_month_pd <- prosecution_data_unique %>%
  filter(year(six_month_pd)>1999 & year(six_month_pd)<2020) %>%
  group_by(State, six_month_pd) %>%
  summarise(first_date_in_pd = min(Date), last_date_in_pd = max(Date))

#create the data set used for this sensitivity analysis
#first, we merge the grouped prosecution data set with the main data set by state and time period
sensitivity_anlys_redefine_int_data<-merge(main_analysis_data,
                                           prosecution_data_by_six_month_pd,
                                           by.x = c("State", "Time_Period_Start"),
                                           by.y = c("State", "six_month_pd"), all = TRUE)

#create a intervention 2 year effect variable by initializing it to be all 0
sensitivity_anlys_redefine_int_data<-sensitivity_anlys_redefine_int_data %>%
  group_by(State) %>%
  mutate(int_2_yr_effect = 0)

#change the date into a date object
sensitivity_anlys_redefine_int_data$Time_Period_Start<-as.Date(sensitivity_anlys_redefine_int_data$Time_Period_Start)
sensitivity_anlys_redefine_int_data$Time_Period_End<-as.Date(sensitivity_anlys_redefine_int_data$Time_Period_End)

#we need to impute the newly defined intervention variable depending on the case
#by examining each row of the data set
for(state in unique(sensitivity_anlys_redefine_int_data$State)){
  #first, subset the data set into state_data which only contains the data for the state
  state_index<-which(sensitivity_anlys_redefine_int_data$State == state)
  state_data<-sensitivity_anlys_redefine_int_data[state_index,]

  #note that the first four rows of the 2 year effect intervention variable are the same as the
  #first four rows of the original intervention variable
  state_data$int_2_yr_effect[1:4]<-state_data$Intervention_Redefined[1:4]

  for(i in 5:nrow(state_data)){
    #next, we deal with the rows where there was at least one prosecution in the last 3 six month periods
    #These rows will be imputed with a 1
    if((!is.na(state_data$first_date_in_pd[i - 1]) |
        !is.na(state_data$first_date_in_pd[i - 2]) |
        !is.na(state_data$first_date_in_pd[i - 3]))){

      state_data$int_2_yr_effect[i]<-1

    }else{
      #next, we account for the rows with the fractions:
      # 1) an intervention occurs in row i without an intervention 2 years ago
      # 2) row i contains the lasting effects of an intervention that occurred 2 years ago
      # 3) row i contains effects from both a new intervention starting in row i and lasting
      # effects from 2 years ago

      #To compute the fraction, we add the number of days that are affected by an intervention
      #(from both the current prosecution and previous prosecution) and then divide by the total
      #number of days in the period:

      total_len_of_pd<-as.numeric(state_data$Time_Period_End[i] - state_data$Time_Period_Start[i])

      #If there is no prosecution two years ago, i.e. in period i-4, then the last_date is the first
      #date in period i. We subtract the last_date by the first date in the period, so we will get
      #a 0 for the number of days that are affected by a prosecution from period i-4. Otherwise,
      #the last_date is the last date of prosecution from period i-4, plus 2 years.
      len_of_past_effect <- ifelse(!is.na(state_data$first_date_in_pd[i - 4]),
                                   (state_data$last_date_in_pd[i - 4] + years(2)) - state_data$Time_Period_Start[i],
                                   0)

      #If there is no prosecution in the period i, then the start_date is the last date in the period i.
      #We subtract start_date from the last date in period i, so we will get a 0 for the number
      #of days that are affected by a prosecution in period i. Otherwise, the start_date is the
      #first date of a prosecution in period i.
      len_of_current_effect <- ifelse(!is.na(state_data$first_date_in_pd[i]),
                                      as.numeric(state_data$Time_Period_End[i] - state_data$first_date_in_pd[i]),
                                      0)

      state_data$int_2_yr_effect[i]<-(len_of_past_effect + len_of_current_effect)/total_len_of_pd
    }
  }

  #for the case where the int_2_yr_effect is greater than 1 (could result when we add the effects of
  #previous intervention and the current intervention), we just impute a 1 instead
  state_data$int_2_yr_effect[state_data$int_2_yr_effect>1]<-1

  #lastly, we store the int_2_yr_effect variable into the sensitivity analysis data set
  sensitivity_anlys_redefine_int_data$int_2_yr_effect[state_index]<-state_data$int_2_yr_effect
}

#view the data set just to make sure the imputation looks right
# View(sensitivity_anlys_redefine_int_data %>% select(State, Time_Period_Start, Time_Period_End,
#                                                     Intervention_Redefined, first_date_in_pd,
#                                                     last_date_in_pd,
#                                                     int_2_yr_effect))


sensitivity_anlys_redefine_int_data <- sensitivity_anlys_redefine_int_data %>%
  group_by(Time_Period_Start) %>%
  mutate(num_states_w_intervention_2_yr_effect = sum(int_2_yr_effect))

#run the analysis on the sensitivity analysis data
sensitivity_anlys_redefine_int_model<-gam(cbind(round(imputed_deaths), round(num_alive))~ State +
                                            s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                            Naloxone_Pharmacy_Yes_Redefined +
                                            Naloxone_Pharmacy_No_Redefined +
                                            Medical_Marijuana_Redefined +
                                            Recreational_Marijuana_Redefined +
                                            GSL_Redefined +
                                            PDMP_Redefined +
                                            Medicaid_Expansion_Redefined +
                                            int_2_yr_effect +
                                            num_states_w_intervention_2_yr_effect,
                                          data = sensitivity_anlys_redefine_int_data, family = "binomial")

stargazer(sensitivity_anlys_redefine_int_model, type = "html", dep.var.labels = "Unintentional Overdose Death")
```

## Sandwich Estimator
```{r}
sensitivity_anlys_redefine_int_data$prop_dead <- sensitivity_anlys_redefine_int_data$imputed_deaths/
  sensitivity_anlys_redefine_int_data$population
#compute the full dataset including basis functions
sensitivity_anlys_redefine_int_w_basis_functions <- as.matrix(predict(sensitivity_anlys_redefine_int_model, type = "lpmatrix"))

#estimate the 95% CI and SD
sensitivity_anlys_redefine_int_coefficient_values <- coef(sensitivity_anlys_redefine_int_model)
#type = "response" to get the estimated probabilities
sensitivity_anlys_redefine_int_pred_prob <- predict(sensitivity_anlys_redefine_int_model, 
                                                    newdata = sensitivity_anlys_redefine_int_data, type = "response")
sensitivity_anlys_redefine_int_sd_and_ci <- compute_sd_and_CI(sensitivity_anlys_redefine_int_w_basis_functions, 
                                                              logit(sensitivity_anlys_redefine_int_data$prop_dead),
                                                              sensitivity_anlys_redefine_int_coefficient_values,
                                                              k = ncol(sensitivity_anlys_redefine_int_w_basis_functions))
sensitivity_anlys_redefine_int_sd_and_ci
```


## Plots
```{r}
plot(sensitivity_anlys_redefine_int_model, pages = 1)

```

## Compile Results
```{r}
############## Sensitivity Analysis 4: Make Data Frame of Results and 95% CI ##########
#store the coefficients into the table
sensitivity_anlys_redefine_int_full_table<-sensitivity_anlys_redefine_int_sd_and_ci

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "int_2_yr_effect", 
              "num_states_w_intervention_2_yr_effect")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_redefine_int_full_table))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_redefine_int_full_table)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 6,
                                                                     stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i]))

    }else if(rownames(sensitivity_anlys_redefine_int_full_table)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_redefine_int_full_table)[i]<-paste("Smoothed Time for Region ",
                                                                    substr(rownames(sensitivity_anlys_redefine_int_full_table)[i], start = 36,
                                                                           stop = nchar(rownames(sensitivity_anlys_redefine_int_full_table)[i])),
                                                                    sep = "")

    }
  }
}

head(sensitivity_anlys_redefine_int_full_table)
tail(sensitivity_anlys_redefine_int_full_table)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_redefine_int_full_table) %in% covariates)
sensitivity_anlys_redefine_int_covariate_table<-(round(sensitivity_anlys_redefine_int_full_table[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_redefine_int_covariate_table)<-c("Naloxone_Pharmacy_Yes", 
                                                            "Naloxone_Pharmacy_No",
                                                            "Medical_Marijuana",
                                                            "Recreational_Marijuana",
                                                            "GSL", 
                                                            "PDMP", 
                                                            "Medicaid_Expansion",
                                                            "Two Year Intervention Effect", 
                                                            "Number of States w Intervention")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_redefine_int_covariate_table<-rbind(sensitivity_anlys_redefine_int_covariate_table,
                                                      sensitivity_anlys_redefine_int_full_table[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_redefine_int_covariate_table<-sensitivity_anlys_redefine_int_covariate_table[,
                                                                                               -which(colnames(sensitivity_anlys_redefine_int_covariate_table) %in% 
                                                                                                        c("lb_coef",
                                                                                                          "coef_values",
                                                                                                          "ub_coef"))]

colnames(sensitivity_anlys_redefine_int_covariate_table)<-c("RR_95_CI_LB", "Risk_Ratio_Estimates",  "RR_95_CI_UB", "standard_error")
head(sensitivity_anlys_redefine_int_covariate_table, 10)

#save the table into a CSV
write.csv(round(sensitivity_anlys_redefine_int_covariate_table, 3), "./Data/coefficients_covariates_4_20_22_full_data_redefine_int.csv")

```

## Attributable Deaths
```{r}
################ Sensitivity Analysis 4: Number of Attributable Deaths ################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_redefine_int<-sensitivity_anlys_redefine_int_data[which(sensitivity_anlys_redefine_int_data$int_2_yr_effect>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_redefine_int<-expit(-coef(sensitivity_anlys_redefine_int_model)["int_2_yr_effect"]*
                                     attr_deaths_anlys_redefine_int$int_2_yr_effect
                                   + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-sensitivity_anlys_redefine_int_full_table$lb_coef
coef_ub<-sensitivity_anlys_redefine_int_full_table$ub_coef
names(coef_lb) <- names(coef_ub) <- rownames(sensitivity_anlys_redefine_int_full_table)

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_redefine_int<-expit(-coef_lb[names(coef_lb) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

prob_od_no_int_UB_redefine_int<-expit(-coef_ub[names(coef_ub) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int$int_2_yr_effect
                                      + logit(attr_deaths_anlys_redefine_int$imputed_deaths/attr_deaths_anlys_redefine_int$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_redefine_int_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_redefine_int$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                          - prob_od_no_int_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])
  num_attr_od_UB[index]<-sum(attr_deaths_anlys_redefine_int$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_redefine_int[time_point_index]*attr_deaths_anlys_redefine_int$population[time_point_index])


  index<-index + 1
}

num_attr_od_redefine_int<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_ID)),
                                     "Time_Start" = sort(unique(attr_deaths_anlys_redefine_int$Time_Period_Start)),
                                     "Num_Attr_Deaths" = num_attr_od,
                                     "Num_Attr_Deaths_LB" = num_attr_od_LB,
                                     "Num_Attr_Deaths_UB" = num_attr_od_UB)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_redefine_int$Num_Attr_Deaths)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_redefine_int<-num_attr_od_redefine_int %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths),
            death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_redefine_int$deaths)
summary(yearly_num_Attr_Deaths_redefine_int$death_lb)
summary(yearly_num_Attr_Deaths_redefine_int$death_ub)

sum(yearly_num_Attr_Deaths_redefine_int$deaths)
sum(yearly_num_Attr_Deaths_redefine_int$death_lb)
sum(yearly_num_Attr_Deaths_redefine_int$death_ub)

ggplot(yearly_num_Attr_Deaths_redefine_int, aes(x = year, y = deaths)) + 
  geom_line() + 
  geom_line(aes(y = death_lb, x = year), linetype = "dashed") + 
  geom_line(aes(y = death_ub, x = year), linetype = "dashed") + 
  geom_point()

```

```{r}
pdf("./Figures/attributable_deaths_main_analysis_and_secondary.pdf")
ggplot() + 
  geom_line(yearly_num_Attr_Deaths_redefine_int, mapping = aes(x = year, y = deaths, linetype = "Estimate",
                                                               color = "Two-Year Intervention Effect")) + 
  geom_point(yearly_num_Attr_Deaths_redefine_int, mapping = aes(y = deaths, x = year, shape = "Two-Year Intervention Effect",
                                                                color = "Two-Year Intervention Effect")) + 
  geom_line(yearly_num_Attr_Deaths_redefine_int, mapping = aes(y = death_lb, x = year, linetype = "95% CI",
                                                               color = "Two-Year Intervention Effect")) + 
  geom_line(yearly_num_Attr_Deaths_redefine_int, mapping = aes(y = death_ub, x = year, linetype = "95% CI",
                                                               color = "Two-Year Intervention Effect")) + 
  geom_line(yearly_num_Attr_Deaths_main_analysis, mapping = aes(x = year, y = deaths, linetype = "Estimate",
                                                                color = "Intervention Effect Lasts Until End of 2019")) + 
  geom_point(yearly_num_Attr_Deaths_main_analysis, 
             mapping = aes(y = deaths, x = year, shape = "Intervention Effect Lasts Until End of 2019",
                           color = "Intervention Effect Lasts Until End of 2019")) + 
  geom_line(yearly_num_Attr_Deaths_main_analysis, mapping = aes(y = death_lb, x = year, linetype = "95% CI",
                                                                color = "Intervention Effect Lasts Until End of 2019")) + 
  geom_line(yearly_num_Attr_Deaths_main_analysis, mapping = aes(y = death_ub, x = year, linetype = "95% CI",
                                                                color = "Intervention Effect Lasts Until End of 2019")) + 
  theme(panel.background = element_rect("white"), 
        panel.border = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        axis.line = element_line(colour = "black"),
        axis.text = element_text(size = 12))+
  labs(y = "Yearly Number of Unintentional Drug OD Attributable to DIH Prosecutions",
       x = "Year",
       linetype = "",
       shape = "",
       color = "")+
  theme(legend.position = "bottom") + 
  scale_linetype_manual(values = c("dashed", "solid")) + 
  guides(color=guide_legend(nrow=2),
         shape=guide_legend(nrow=2))
dev.off()
```


# Sensitivity Analysis 3: OLS
## Analysis
```{r, results = "asis"}
############################## Run Model with Spline Time Effects by Region ###############################
#model that we will be using for the main analysis
#cr is used for cubic regression spline -- we are smoothing time effects by region
#run the analysis for all the states
main_analysis_data$prop_dead <- main_analysis_data$imputed_deaths/main_analysis_data$population
main_analysis_model_ols<-gam(log(prop_dead)~ State +
                           s(Time_Period_ID, bs = "cr", by = as.factor(Region)) +
                           Naloxone_Pharmacy_Yes_Redefined +
                           Naloxone_Pharmacy_No_Redefined +
                           Medical_Marijuana_Redefined +
                           Recreational_Marijuana_Redefined +
                           GSL_Redefined +
                           PDMP_Redefined +
                           Medicaid_Expansion_Redefined +
                           Intervention_Redefined + 
                           num_states_w_intervention,
                         data = main_analysis_data)



#summary output of the model
stargazer(main_analysis_model_ols, type = "html", dep.var.labels = c("Unintentional Overdose Deaths"))
```

## Sandwich Estimator
```{r}
#compute the full dataset including basis functions
full_df_w_basis_functions_ols <- as.matrix(data.frame(predict(main_analysis_model_ols, type = "lpmatrix")))

#estimate the 95% CI and SD
coefficient_values_ols <- coef(main_analysis_model_ols)
#type = "response" to get the estimated probabilities
main_analysis_sd_and_ci_ols <- compute_sd_and_CI(full_df_w_basis_functions_ols, 
                                             log(main_analysis_data$prop_dead),
                                             coefficient_values_ols,
                                             k = ncol(full_df_w_basis_functions_ols))
main_analysis_sd_and_ci_ols
```

## Compile Results
```{r}
############################## Main Analysis: Make Data Frame of Results and 95% CI ###############################
#store the coefficients into the table
main_analysis_full_table_ols<-main_analysis_sd_and_ci_ols
#check to see how the table looks
head(main_analysis_full_table_ols)

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "Intervention_Redefined", 
              "num_states_w_intervention")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(main_analysis_full_table_ols))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(main_analysis_full_table_ols)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(main_analysis_full_table_ols)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(main_analysis_full_table_ols)[i]<-substr(rownames(main_analysis_full_table_ols)[i], start = 6,
                                                    stop = nchar(rownames(main_analysis_full_table_ols)[i]))

    }else if(rownames(main_analysis_full_table_ols)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(main_analysis_full_table_ols)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(main_analysis_full_table_ols)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(main_analysis_full_table_ols)[i]<-paste("Smoothed Time for Region ",
                                                   substr(rownames(main_analysis_full_table_ols)[i], start = 36,
                                                          stop = nchar(rownames(main_analysis_full_table_ols)[i])),
                                                   sep = "")

    }
  }
}

head(main_analysis_full_table_ols)
tail(main_analysis_full_table_ols)

#save the table into a CSV
# write.csv(round(main_analysis_full_table,5), "./Data/coefficients_GAM_1_18_22_full_data_uninentional_od.csv")

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_index<-which(rownames(main_analysis_full_table_ols) %in% covariates)
main_analysis_covariate_table_ols<-(round(main_analysis_full_table_ols[covariate_index,], 5))

#rename the variables so that it looks cleaner
rownames(main_analysis_covariate_table_ols)<-c("Naloxone_Pharmacy_Yes", 
                                           "Naloxone_Pharmacy_No",
                                           "Medical_Marijuana",
                                           "Recreational_Marijuana",
                                           "GSL", 
                                           "PDMP", 
                                           "Medicaid_Expansion",
                                           "Intervention", 
                                           "Number of States with DIH Prosecutions")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
main_analysis_covariate_table_ols<-rbind(main_analysis_covariate_table_ols, 
                                     main_analysis_full_table_ols[-covariate_index,])
#remove the columns that aren't in odds ratio scale
main_analysis_covariate_table_ols<-main_analysis_covariate_table_ols[,-which(colnames(main_analysis_covariate_table_ols) %in%
                                                                       c("lb_coef", "coef_values",
                                                                         "ub_coef"))]

colnames(main_analysis_covariate_table_ols)<-c("RR_95_CI_LB", "Risk_Ratio_Estimates", "RR_95_CI_UB", "standard_error")
head(main_analysis_covariate_table_ols, 10)

#save the table into a CSV
write.csv(round(main_analysis_covariate_table_ols, 3), "./Data/coefficients_covariates_5_5_22_full_data_unintentional_od_ols.csv")


```

## Attributable Deaths
````{r}
############################## Main Analysis: Number of Overdose Deaths Attributed to Intervention ###############################
#find the number of deaths attributable to the intervention
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_main_analysis_ols<-main_analysis_data[which(main_analysis_data$Intervention_Redefined>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_main_analysis_ols<-exp(-coef(main_analysis_model_ols)["Intervention_Redefined"]*
                                      attr_deaths_anlys_main_analysis_ols$Intervention_Redefined
                                    + log(attr_deaths_anlys_main_analysis_ols$imputed_deaths/
                                              attr_deaths_anlys_main_analysis_ols$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb_ols<-main_analysis_full_table_ols$lb_coef
coef_ub_ols<-main_analysis_full_table_ols$ub_coef
names(coef_lb_ols) <- names(coef_ub_ols) <- rownames(main_analysis_full_table_ols)

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_main_analysis_ols<-exp(-coef_lb_ols[names(coef_lb_ols) ==
                                                  "Intervention_Redefined"]*attr_deaths_anlys_main_analysis_ols$Intervention_Redefined
                                       +log(attr_deaths_anlys_main_analysis_ols$imputed_deaths/
                                                attr_deaths_anlys_main_analysis_ols$population))

prob_od_no_int_UB_main_analysis_ols<-exp(-coef_ub_ols[names(coef_ub_ols) ==
                                                  "Intervention_Redefined"]*attr_deaths_anlys_main_analysis_ols$Intervention_Redefined
                                       + log(attr_deaths_anlys_main_analysis_ols$imputed_deaths/
                                                 attr_deaths_anlys_main_analysis_ols$population))


#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB_ols<-num_attr_od_LB_ols<-num_attr_od_ols<-rep(NA, length(unique(attr_deaths_anlys_main_analysis_ols$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_main_analysis_ols$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index_ols<-which(attr_deaths_anlys_main_analysis_ols$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od_ols[index]<-sum(attr_deaths_anlys_main_analysis_ols$imputed_deaths[time_point_index_ols]
                          - prob_od_no_int_main_analysis_ols[time_point_index_ols]*
                            attr_deaths_anlys_main_analysis_ols$population[time_point_index_ols])
  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB_ols[index]<-sum(attr_deaths_anlys_main_analysis_ols$imputed_deaths[time_point_index_ols]
                             - prob_od_no_int_LB_main_analysis_ols[time_point_index_ols]*
                               attr_deaths_anlys_main_analysis_ols$population[time_point_index_ols])
  num_attr_od_UB_ols[index]<-sum(attr_deaths_anlys_main_analysis_ols$imputed_deaths[time_point_index_ols]
                             - prob_od_no_int_UB_main_analysis_ols[time_point_index_ols]*
                               attr_deaths_anlys_main_analysis_ols$population[time_point_index_ols])
  index<-index + 1
}

num_attr_od_main_analysis_ols<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_main_analysis_ols$Time_Period_ID)),
                                      "Time_Start" = sort(unique(attr_deaths_anlys_main_analysis_ols$Time_Period_Start)),
                                      "Num_Attr_Deaths" = num_attr_od_ols,
                                      "Num_Attr_Deaths_LB" = num_attr_od_LB_ols,
                                      "Num_Attr_Deaths_UB" = num_attr_od_UB_ols)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_main_analysis_ols$Num_Attr_Deaths)
summary(num_attr_od_main_analysis_ols$Num_Attr_Deaths)
num_attr_od_main_analysis_ols$Time_Start<-as.Date(num_attr_od_main_analysis_ols$Time_Start)

#compute the 95% CI for the total
sum(num_attr_od_main_analysis_ols$Num_Attr_Deaths_LB)
sum(num_attr_od_main_analysis_ols$Num_Attr_Deaths_UB)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_main_analysis_ols<-num_attr_od_main_analysis_ols %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths), 
            death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_main_analysis_ols$deaths)
summary(yearly_num_Attr_Deaths_main_analysis_ols$death_lb)
summary(yearly_num_Attr_Deaths_main_analysis_ols$death_ub)

```


# Sensitivity Analysis 2: Effect of DIH Prosecutions on Unintentional Overdose Deaths, Assuming Effect Lasts for Two Years OLS
## Analysis
```{r, results = "asis"}
sensitivity_anlys_redefine_int_data$prop_dead <- sensitivity_anlys_redefine_int_data$imputed_deaths/
  sensitivity_anlys_redefine_int_data$population
#run the analysis on the sensitivity analysis data
sensitivity_anlys_redefine_int_model_ols<-gam(log(prop_dead)~ State +
                                            s(Time_Period_ID, bs = "cr", by = as.factor(Region))  +
                                            Naloxone_Pharmacy_Yes_Redefined +
                                            Naloxone_Pharmacy_No_Redefined +
                                            Medical_Marijuana_Redefined +
                                            Recreational_Marijuana_Redefined +
                                            GSL_Redefined +
                                            PDMP_Redefined +
                                            Medicaid_Expansion_Redefined +
                                            int_2_yr_effect +
                                            num_states_w_intervention_2_yr_effect,
                                          data = sensitivity_anlys_redefine_int_data)

stargazer(sensitivity_anlys_redefine_int_model_ols, type = "html", dep.var.labels = "Unintentional Overdose Death")
```

## Sandwich Estimator
```{r}

#compute the full dataset including basis functions
sensitivity_anlys_redefine_int_w_basis_functions_ols <- as.matrix(predict(sensitivity_anlys_redefine_int_model_ols, type = "lpmatrix"))

#estimate the 95% CI and SD
sensitivity_anlys_redefine_int_coefficient_values_ols <- coef(sensitivity_anlys_redefine_int_model_ols)
#type = "response" to get the estimated probabilities
sensitivity_anlys_redefine_int_pred_prob_ols <- predict(sensitivity_anlys_redefine_int_model_ols, 
                                                    newdata = sensitivity_anlys_redefine_int_data, type = "response")
sensitivity_anlys_redefine_int_sd_and_ci_ols <- compute_sd_and_CI(sensitivity_anlys_redefine_int_w_basis_functions_ols, 
                                                              log(sensitivity_anlys_redefine_int_data$prop_dead),
                                                              sensitivity_anlys_redefine_int_coefficient_values_ols,
                                                              k = ncol(sensitivity_anlys_redefine_int_w_basis_functions_ols))
sensitivity_anlys_redefine_int_sd_and_ci_ols
```


## Compile Results
```{r}
############## Sensitivity Analysis 4: Make Data Frame of Results and 95% CI ##########
#store the coefficients into the table
sensitivity_anlys_redefine_int_full_table_ols<-sensitivity_anlys_redefine_int_sd_and_ci_ols

#vector of covariates
covariates<-c("Naloxone_Pharmacy_Yes_Redefined", 
              "Naloxone_Pharmacy_No_Redefined",
              "Medical_Marijuana_Redefined",
              "Recreational_Marijuana_Redefined",
              "GSL_Redefined", 
              "PDMP_Redefined",
              "Medicaid_Expansion_Redefined", 
              "int_2_yr_effect", 
              "num_states_w_intervention_2_yr_effect")

#rename the variable names of the regression output so that they look nicer:
#currently there are 3 types of coefficients: state effects, the covariates, and smoothed time effects
#for each row in the main analysis table
for(i in 1:length(rownames(sensitivity_anlys_redefine_int_full_table_ols))){

  #if the coefficient is not in the covariates vector
  if(!(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i] %in% covariates)){

    #we see if it's a state effect
    if(substr(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i], start = 1, stop = 5) == "State"){

      #if so, here, the names look like: StateMassachusetts or StateGeorgia, so take out the "State" part
      #and just rename these rows to just the state name
      rownames(sensitivity_anlys_redefine_int_full_table_ols)[i]<-substr(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i], 
                                                                         start = 6,
                                                                     stop =
                                                                       nchar(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i]))

    }else if(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i] == "(Intercept)"){

      #otherwise, if the current name is Intercept, we rename it so that we know that Alabama is the baseline
      rownames(sensitivity_anlys_redefine_int_full_table_ols)[i]<-"Intercept/Alabama"

    }else if(substr(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i], start = 1, stop = 35) == "s(Time_Period_ID):as.factor(Region)"){

      #otherwise, it's the smoothed time effects which look like: s(Time_Period_ID):as.factor(Region)West
      #or s(Time_Period_ID):as.factor(Region)South, so we want to get rid of "s(Time_Period_ID):as.factor(Region)"
      #and change it to "Smoothed Time for Region"
      rownames(sensitivity_anlys_redefine_int_full_table_ols)[i]<-paste("Smoothed Time for Region ",
                                                                    substr(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i],
                                                                           start = 36,
                                                                           stop =
                                                                            nchar(rownames(sensitivity_anlys_redefine_int_full_table_ols)[i])),
                                                                    sep = "")

    }
  }
}

head(sensitivity_anlys_redefine_int_full_table_ols)
tail(sensitivity_anlys_redefine_int_full_table_ols)

#export a table with just the covariates
#first, find the rows that contains the covariates
covariate_Index<-which(rownames(sensitivity_anlys_redefine_int_full_table_ols) %in% covariates)
sensitivity_anlys_redefine_int_covariate_table_ols<-(round(sensitivity_anlys_redefine_int_full_table_ols[covariate_Index,], 5))

#rename the variables so that it looks cleaner
rownames(sensitivity_anlys_redefine_int_covariate_table_ols)<-c("Naloxone_Pharmacy_Yes", 
                                                            "Naloxone_Pharmacy_No",
                                                            "Medical_Marijuana",
                                                            "Recreational_Marijuana",
                                                            "GSL", 
                                                            "PDMP", 
                                                            "Medicaid_Expansion",
                                                            "Two Year Intervention Effect", 
                                                            "Number of States w Intervention")

#now, reorganize the data so that the covariates are on top and the rest of the variable sare below
sensitivity_anlys_redefine_int_covariate_table_ols<-rbind(sensitivity_anlys_redefine_int_covariate_table_ols,
                                                      sensitivity_anlys_redefine_int_full_table_ols[-covariate_Index,])
#remove the columns that aren't in odds ratio scale
sensitivity_anlys_redefine_int_covariate_table_ols<-sensitivity_anlys_redefine_int_covariate_table_ols[,
                                                                                               -which(colnames(sensitivity_anlys_redefine_int_covariate_table_ols) %in% 
                                                                                                        c("lb_coef",
                                                                                                          "coef_values",
                                                                                                          "ub_coef"))]

colnames(sensitivity_anlys_redefine_int_covariate_table_ols)<-c("RR_95_CI_LB", "Risk_Ratio_Estimates",  "RR_95_CI_UB", "standard_error")
head(sensitivity_anlys_redefine_int_covariate_table_ols, 10)

#save the table into a CSV
write.csv(round(sensitivity_anlys_redefine_int_covariate_table_ols, 3),
          "./Data/coefficients_covariates_5_6_22_full_data_redefine_int_ols.csv")

```

## Attributable Deaths
```{r}
################ Sensitivity Analysis 4: Number of Attributable Deaths ################
#first, we subset the data so that we only focus on the time points for which at least one state had the intervention
attr_deaths_anlys_redefine_int_ols<-sensitivity_anlys_redefine_int_data[which(sensitivity_anlys_redefine_int_data$int_2_yr_effect>0),]

#compute the probability of overdose had intervention not occurred
prob_od_no_int_redefine_int_ols<-exp(-coef(sensitivity_anlys_redefine_int_model_ols)["int_2_yr_effect"]*
                                     attr_deaths_anlys_redefine_int_ols$int_2_yr_effect
                                   + log(attr_deaths_anlys_redefine_int_ols$imputed_deaths/
                                             attr_deaths_anlys_redefine_int_ols$population))

#compute the lower and upper bounds of 95% CI of probability of overdose had intervention not occurred
#here, we compute the lower and upper bounds of the 95% CI of all the coefficients using the standard error from the model
coef_lb<-sensitivity_anlys_redefine_int_full_table_ols$lb_coef
coef_ub<-sensitivity_anlys_redefine_int_full_table_ols$ub_coef
names(coef_lb) <- names(coef_ub) <- rownames(sensitivity_anlys_redefine_int_full_table_ols)

#we then calculate the upper and lower bounds of the probability of overdose death had intervention not occurred by using
#the lower and upper bounds of the coefficient of the intervention variable
prob_od_no_int_LB_redefine_int_ols<-exp(-coef_lb[names(coef_lb) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int_ols$int_2_yr_effect
                                      + log(attr_deaths_anlys_redefine_int_ols$imputed_deaths/
                                                attr_deaths_anlys_redefine_int_ols$population))

prob_od_no_int_UB_redefine_int_ols<-exp(-coef_ub[names(coef_ub) == "int_2_yr_effect"]*attr_deaths_anlys_redefine_int_ols$int_2_yr_effect
                                      + log(attr_deaths_anlys_redefine_int_ols$imputed_deaths/
                                                attr_deaths_anlys_redefine_int_ols$population))

#estimate the number of deaths attributable to the intervention
#first, initialize the vectors to store the numbers
num_attr_od_UB<-num_attr_od_LB<-num_attr_od<-rep(NA, length(unique(sensitivity_anlys_redefine_int_data$Time_Period_ID)))


#for each time period, we first find the indices of rows containing data from that time point
#then, we find the total number of deaths that attributable to the intervention

index<-1 #keep track of where to store the values in the vector

for(time in sort(unique(attr_deaths_anlys_redefine_int_ols$Time_Period_ID))){
  #find the indices of rows where the time point = time
  time_point_index<-which(attr_deaths_anlys_redefine_int_ols$Time_Period_ID == time)

  #find the number of deaths attributable to intervention = observed number of deaths with intervention - estimated number of deaths had intervention not occurred
  num_attr_od_ols[index]<-sum(attr_deaths_anlys_redefine_int_ols$imputed_deaths[time_point_index]
                          - prob_od_no_int_redefine_int_ols[time_point_index]*
                            attr_deaths_anlys_redefine_int_ols$population[time_point_index])

  #find the lower and upper bounds of the estimated number of deaths attributable to the intervention
  num_attr_od_LB_ols[index]<-sum(attr_deaths_anlys_redefine_int_ols$imputed_deaths[time_point_index]
                             - prob_od_no_int_LB_redefine_int_ols[time_point_index]*
                               attr_deaths_anlys_redefine_int_ols$population[time_point_index])
  num_attr_od_UB_ols[index]<-sum(attr_deaths_anlys_redefine_int_ols$imputed_deaths[time_point_index]
                             - prob_od_no_int_UB_redefine_int_ols[time_point_index]*
                               attr_deaths_anlys_redefine_int_ols$population[time_point_index])


  index<-index + 1
}

num_attr_od_redefine_int_ols<-data.frame("Time_Period_ID" = sort(unique(attr_deaths_anlys_redefine_int_ols$Time_Period_ID)),
                                     "Time_Start" = sort(unique(attr_deaths_anlys_redefine_int_ols$Time_Period_Start)),
                                     "Num_Attr_Deaths" = num_attr_od_ols,
                                     "Num_Attr_Deaths_LB" = num_attr_od_LB_ols,
                                     "Num_Attr_Deaths_UB" = num_attr_od_UB_ols)

#sum up the total number of excess deaths attributable to the intervention
sum(num_attr_od_redefine_int_ols$Num_Attr_Deaths)

#sum up the number of excess deaths per year
yearly_num_Attr_Deaths_redefine_int_ols<-num_attr_od_redefine_int_ols %>%
  group_by("year" = year(Time_Start)) %>%
  summarise("deaths" = sum(Num_Attr_Deaths),
            death_lb = sum(Num_Attr_Deaths_LB),
            death_ub = sum(Num_Attr_Deaths_UB))

summary(yearly_num_Attr_Deaths_redefine_int_ols$deaths)
summary(yearly_num_Attr_Deaths_redefine_int_ols$death_lb)
summary(yearly_num_Attr_Deaths_redefine_int_ols$death_ub)

ggplot(yearly_num_Attr_Deaths_redefine_int_ols, aes(x = year, y = deaths)) + 
  geom_line() + 
  geom_line(aes(y = death_lb, x = year), linetype = "dashed") + 
  geom_line(aes(y = death_ub, x = year), linetype = "dashed") + 
  geom_point()

```


